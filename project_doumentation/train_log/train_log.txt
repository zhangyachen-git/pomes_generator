Epoch: 14, batch: 0, training loss: 1.930131
Epoch: 14, batch: 1, training loss: 2.077824
Epoch: 14, batch: 2, training loss: 1.279802
Epoch: 14, batch: 3, training loss: 1.864902
Epoch: 14, batch: 4, training loss: 0.915340
Epoch: 14, batch: 5, training loss: 1.023350
Epoch: 14, batch: 6, training loss: 0.882979
Epoch: 14, batch: 7, training loss: 1.251360
Epoch: 14, batch: 8, training loss: 1.360496
Epoch: 14, batch: 9, training loss: 1.520576
Epoch: 14, batch: 10, training loss: 1.791427
Epoch: 14, batch: 11, training loss: 2.149421
Epoch: 14, batch: 12, training loss: 1.868161
Epoch: 14, batch: 13, training loss: 2.232265
Epoch: 14, batch: 14, training loss: 2.018370
Epoch: 14, batch: 15, training loss: 2.444708
Epoch: 14, batch: 16, training loss: 2.144704
Epoch: 14, batch: 17, training loss: 2.172683
Epoch: 14, batch: 18, training loss: 1.954907
Epoch: 14, batch: 19, training loss: 1.531316
Epoch: 14, batch: 20, training loss: 2.165844
Epoch: 14, batch: 21, training loss: 1.391247
Epoch: 14, batch: 22, training loss: 2.007111
Epoch: 14, batch: 23, training loss: 1.970367
Epoch: 14, batch: 24, training loss: 2.020684
Epoch: 14, batch: 25, training loss: 2.220515
Epoch: 14, batch: 26, training loss: 2.289808
Epoch: 14, batch: 27, training loss: 2.403732
Epoch: 14, batch: 28, training loss: 2.623917
Epoch: 14, batch: 29, training loss: 2.406757
Epoch: 14, batch: 30, training loss: 2.492640
Epoch: 14, batch: 31, training loss: 2.059985
Epoch: 14, batch: 32, training loss: 1.985653
Epoch: 14, batch: 33, training loss: 2.488719
Epoch: 14, batch: 34, training loss: 2.399257
Epoch: 14, batch: 35, training loss: 2.293513
Epoch: 14, batch: 36, training loss: 2.230926
Epoch: 14, batch: 37, training loss: 2.152888
Epoch: 14, batch: 38, training loss: 2.327703
Epoch: 14, batch: 39, training loss: 2.716771
Epoch: 14, batch: 40, training loss: 1.827235
Epoch: 14, batch: 41, training loss: 2.206690
Epoch: 14, batch: 42, training loss: 2.319341
Epoch: 14, batch: 43, training loss: 1.897871
Epoch: 14, batch: 44, training loss: 2.513644
Epoch: 14, batch: 45, training loss: 2.505348
Epoch: 14, batch: 46, training loss: 2.412877
Epoch: 14, batch: 47, training loss: 2.346514
Epoch: 14, batch: 48, training loss: 2.075790
Epoch: 14, batch: 49, training loss: 2.286806
Epoch: 15, batch: 0, training loss: 2.051405
Epoch: 15, batch: 1, training loss: 2.183816
Epoch: 15, batch: 2, training loss: 1.403343
Epoch: 15, batch: 3, training loss: 2.012464
Epoch: 15, batch: 4, training loss: 1.039914
Epoch: 15, batch: 5, training loss: 1.155290
Epoch: 15, batch: 6, training loss: 0.979289
Epoch: 15, batch: 7, training loss: 1.155183
Epoch: 15, batch: 8, training loss: 1.268082
Epoch: 15, batch: 9, training loss: 1.371928
Epoch: 15, batch: 10, training loss: 1.699769
Epoch: 15, batch: 11, training loss: 2.082436
Epoch: 15, batch: 12, training loss: 1.817829
Epoch: 15, batch: 13, training loss: 2.171305
Epoch: 15, batch: 14, training loss: 1.952185
Epoch: 15, batch: 15, training loss: 2.325888
Epoch: 15, batch: 0, training loss: 1.890933
Epoch: 15, batch: 1, training loss: 2.134175
Epoch: 15, batch: 2, training loss: 1.334744
Epoch: 15, batch: 3, training loss: 1.761684
Epoch: 15, batch: 4, training loss: 0.964348
Epoch: 15, batch: 5, training loss: 1.065288
Epoch: 15, batch: 6, training loss: 0.955025
Epoch: 15, batch: 7, training loss: 1.054289
Epoch: 15, batch: 8, training loss: 1.165413
Epoch: 15, batch: 9, training loss: 1.185139
Epoch: 15, batch: 10, training loss: 1.476408
Epoch: 15, batch: 11, training loss: 1.738029
Epoch: 15, batch: 12, training loss: 1.532758
Epoch: 15, batch: 13, training loss: 1.816587
Epoch: 15, batch: 14, training loss: 1.660080
Epoch: 15, batch: 15, training loss: 2.005362
Epoch: 15, batch: 16, training loss: 1.748610
Epoch: 15, batch: 17, training loss: 2.241828
Epoch: 15, batch: 18, training loss: 2.018523
Epoch: 15, batch: 19, training loss: 1.575131
Epoch: 15, batch: 20, training loss: 2.239729
Epoch: 15, batch: 21, training loss: 1.457229
Epoch: 15, batch: 22, training loss: 2.075417
Epoch: 15, batch: 23, training loss: 1.980791
Epoch: 15, batch: 24, training loss: 2.022622
Epoch: 15, batch: 25, training loss: 2.222070
Epoch: 15, batch: 26, training loss: 2.291483
Epoch: 15, batch: 27, training loss: 2.419385
Epoch: 15, batch: 28, training loss: 2.653960
Epoch: 15, batch: 29, training loss: 2.389665
Epoch: 15, batch: 30, training loss: 2.483885
Epoch: 15, batch: 31, training loss: 2.066406
Epoch: 15, batch: 32, training loss: 1.982290
Epoch: 15, batch: 33, training loss: 2.488202
Epoch: 15, batch: 34, training loss: 2.386501
Epoch: 15, batch: 35, training loss: 2.300886
Epoch: 15, batch: 36, training loss: 2.205719
Epoch: 15, batch: 37, training loss: 2.126772
Epoch: 15, batch: 38, training loss: 2.328068
Epoch: 15, batch: 39, training loss: 2.718905
Epoch: 15, batch: 40, training loss: 1.829246
Epoch: 15, batch: 41, training loss: 2.198739
Epoch: 15, batch: 42, training loss: 2.320807
Epoch: 15, batch: 43, training loss: 1.904310
Epoch: 15, batch: 44, training loss: 2.522961
Epoch: 15, batch: 45, training loss: 2.528927
Epoch: 15, batch: 46, training loss: 2.441421
Epoch: 15, batch: 47, training loss: 2.352025
Epoch: 15, batch: 48, training loss: 2.078856
Epoch: 15, batch: 49, training loss: 2.314572
Epoch: 16, batch: 0, training loss: 1.983723
Epoch: 16, batch: 1, training loss: 2.100699
Epoch: 16, batch: 2, training loss: 1.334768
Epoch: 16, batch: 3, training loss: 1.899061
Epoch: 16, batch: 4, training loss: 0.974871
Epoch: 16, batch: 5, training loss: 1.119197
Epoch: 16, batch: 6, training loss: 0.936812
Epoch: 16, batch: 7, training loss: 1.039822
Epoch: 16, batch: 8, training loss: 1.125362
Epoch: 16, batch: 9, training loss: 1.124788
Epoch: 16, batch: 10, training loss: 1.527926
Epoch: 16, batch: 11, training loss: 1.881766
Epoch: 16, batch: 12, training loss: 1.640880
Epoch: 16, batch: 13, training loss: 1.944788
Epoch: 16, batch: 14, training loss: 1.776206
Epoch: 16, batch: 15, training loss: 2.073809
Epoch: 16, batch: 16, training loss: 1.848022
Epoch: 16, batch: 17, training loss: 2.066523
Epoch: 16, batch: 18, training loss: 1.877275
Epoch: 16, batch: 19, training loss: 1.466354
Epoch: 16, batch: 20, training loss: 2.089871
Epoch: 16, batch: 21, training loss: 1.312361
Epoch: 16, batch: 22, training loss: 1.912906
Epoch: 16, batch: 23, training loss: 1.838491
Epoch: 16, batch: 24, training loss: 1.913959
Epoch: 16, batch: 25, training loss: 2.092155
Epoch: 16, batch: 26, training loss: 2.161741
Epoch: 16, batch: 27, training loss: 2.303790
Epoch: 16, batch: 28, training loss: 2.525896
Epoch: 16, batch: 29, training loss: 2.250515
Epoch: 16, batch: 30, training loss: 2.362046
Epoch: 16, batch: 31, training loss: 1.940666
Epoch: 16, batch: 32, training loss: 1.883218
Epoch: 16, batch: 33, training loss: 2.321044
Epoch: 16, batch: 34, training loss: 2.188464
Epoch: 16, batch: 35, training loss: 2.166876
Epoch: 16, batch: 36, training loss: 2.090012
Epoch: 16, batch: 37, training loss: 1.985637
Epoch: 16, batch: 38, training loss: 2.167114
Epoch: 16, batch: 39, training loss: 2.517699
Epoch: 16, batch: 40, training loss: 1.716407
Epoch: 16, batch: 41, training loss: 2.082780
Epoch: 16, batch: 42, training loss: 2.182016
Epoch: 16, batch: 43, training loss: 1.767290
Epoch: 16, batch: 44, training loss: 2.377632
Epoch: 16, batch: 45, training loss: 2.362015
Epoch: 16, batch: 46, training loss: 2.292270
Epoch: 16, batch: 47, training loss: 2.219987
Epoch: 16, batch: 48, training loss: 1.947332
Epoch: 16, batch: 49, training loss: 2.166093
Epoch: 17, batch: 0, training loss: 1.912987
Epoch: 17, batch: 1, training loss: 2.026730
Epoch: 17, batch: 2, training loss: 1.275222
Epoch: 17, batch: 3, training loss: 1.837810
Epoch: 17, batch: 4, training loss: 0.921279
Epoch: 17, batch: 5, training loss: 1.036794
Epoch: 17, batch: 6, training loss: 0.870421
Epoch: 17, batch: 7, training loss: 0.950307
Epoch: 17, batch: 8, training loss: 1.064123
Epoch: 17, batch: 9, training loss: 1.048821
Epoch: 17, batch: 10, training loss: 1.481370
Epoch: 17, batch: 11, training loss: 1.845204
Epoch: 17, batch: 12, training loss: 1.613378
Epoch: 17, batch: 13, training loss: 1.892619
Epoch: 17, batch: 14, training loss: 1.740571
Epoch: 17, batch: 15, training loss: 2.041015
Epoch: 17, batch: 16, training loss: 1.801799
Epoch: 17, batch: 17, training loss: 1.975321
Epoch: 17, batch: 18, training loss: 1.798161
Epoch: 17, batch: 19, training loss: 1.397737
Epoch: 17, batch: 20, training loss: 1.974329
Epoch: 17, batch: 21, training loss: 1.254482
Epoch: 17, batch: 22, training loss: 1.808993
Epoch: 17, batch: 23, training loss: 1.773881
Epoch: 17, batch: 24, training loss: 1.837446
Epoch: 17, batch: 25, training loss: 2.008359
Epoch: 17, batch: 26, training loss: 2.056294
Epoch: 17, batch: 27, training loss: 2.195932
Epoch: 17, batch: 28, training loss: 2.402683
Epoch: 17, batch: 29, training loss: 2.184064
Epoch: 17, batch: 30, training loss: 2.285511
Epoch: 17, batch: 31, training loss: 1.896335
Epoch: 17, batch: 32, training loss: 1.825675
Epoch: 17, batch: 33, training loss: 2.211503
Epoch: 17, batch: 34, training loss: 2.078424
Epoch: 17, batch: 0, training loss: 2.069613
Epoch: 17, batch: 1, training loss: 2.165545
Epoch: 17, batch: 2, training loss: 1.343424
Epoch: 17, batch: 3, training loss: 1.927463
Epoch: 17, batch: 4, training loss: 0.950410
Epoch: 17, batch: 5, training loss: 1.014312
Epoch: 17, batch: 6, training loss: 0.888246
Epoch: 17, batch: 7, training loss: 0.982118
Epoch: 17, batch: 8, training loss: 1.087490
Epoch: 17, batch: 9, training loss: 1.073642
Epoch: 17, batch: 10, training loss: 1.482171
Epoch: 17, batch: 11, training loss: 1.850603
Epoch: 17, batch: 12, training loss: 1.624517
Epoch: 17, batch: 13, training loss: 1.896050
Epoch: 17, batch: 14, training loss: 1.718587
Epoch: 17, batch: 15, training loss: 2.016744
Epoch: 17, batch: 16, training loss: 1.769124
Epoch: 17, batch: 17, training loss: 1.902625
Epoch: 17, batch: 18, training loss: 1.748903
Epoch: 17, batch: 19, training loss: 1.350815
Epoch: 17, batch: 20, training loss: 1.957655
Epoch: 17, batch: 21, training loss: 1.309925
Epoch: 17, batch: 22, training loss: 1.797442
Epoch: 17, batch: 23, training loss: 1.711957
Epoch: 17, batch: 24, training loss: 1.768231
Epoch: 17, batch: 25, training loss: 1.930270
Epoch: 17, batch: 26, training loss: 1.985831
Epoch: 17, batch: 27, training loss: 2.162506
Epoch: 17, batch: 28, training loss: 2.351652
Epoch: 17, batch: 29, training loss: 2.105717
Epoch: 17, batch: 30, training loss: 2.168184
Epoch: 17, batch: 31, training loss: 1.788011
Epoch: 17, batch: 32, training loss: 1.715637
Epoch: 17, batch: 33, training loss: 2.039842
Epoch: 17, batch: 34, training loss: 1.943261
Epoch: 17, batch: 35, training loss: 1.988356
Epoch: 17, batch: 36, training loss: 2.296262
Epoch: 17, batch: 37, training loss: 2.215427
Epoch: 17, batch: 38, training loss: 2.426422
Epoch: 17, batch: 39, training loss: 2.797765
Epoch: 17, batch: 40, training loss: 1.871616
Epoch: 17, batch: 41, training loss: 2.295044
Epoch: 17, batch: 42, training loss: 2.392973
Epoch: 17, batch: 43, training loss: 1.876435
Epoch: 17, batch: 44, training loss: 2.570386
Epoch: 17, batch: 45, training loss: 2.525140
Epoch: 17, batch: 46, training loss: 2.474375
Epoch: 17, batch: 47, training loss: 2.388098
Epoch: 17, batch: 48, training loss: 2.089264
Epoch: 17, batch: 49, training loss: 2.325322
Epoch: 18, batch: 0, training loss: 1.930526
Epoch: 18, batch: 1, training loss: 2.035763
Epoch: 18, batch: 2, training loss: 1.292978
Epoch: 18, batch: 3, training loss: 1.867105
Epoch: 18, batch: 4, training loss: 0.942704
Epoch: 18, batch: 5, training loss: 1.062755
Epoch: 18, batch: 6, training loss: 0.927639
Epoch: 18, batch: 7, training loss: 0.966148
Epoch: 18, batch: 8, training loss: 1.051685
Epoch: 18, batch: 9, training loss: 1.075276
Epoch: 18, batch: 10, training loss: 1.449157
Epoch: 18, batch: 11, training loss: 1.806360
Epoch: 18, batch: 12, training loss: 1.605062
Epoch: 18, batch: 13, training loss: 1.842549
Epoch: 18, batch: 14, training loss: 1.705883
Epoch: 18, batch: 15, training loss: 1.971641
Epoch: 18, batch: 16, training loss: 1.732744
Epoch: 18, batch: 17, training loss: 1.871157
Epoch: 18, batch: 18, training loss: 1.710876
Epoch: 18, batch: 19, training loss: 1.330765
Epoch: 18, batch: 20, training loss: 1.885311
Epoch: 18, batch: 21, training loss: 1.189977
Epoch: 18, batch: 22, training loss: 1.703013
Epoch: 18, batch: 23, training loss: 1.653655
Epoch: 18, batch: 24, training loss: 1.717303
Epoch: 18, batch: 25, training loss: 1.848918
Epoch: 18, batch: 26, training loss: 1.913543
Epoch: 18, batch: 27, training loss: 2.056343
Epoch: 18, batch: 28, training loss: 2.236372
Epoch: 18, batch: 29, training loss: 2.036012
Epoch: 18, batch: 30, training loss: 2.121272
Epoch: 18, batch: 31, training loss: 1.760563
Epoch: 18, batch: 32, training loss: 1.698567
Epoch: 18, batch: 33, training loss: 2.017903
Epoch: 18, batch: 34, training loss: 1.879062
Epoch: 18, batch: 35, training loss: 1.943599
Epoch: 18, batch: 36, training loss: 2.016609
Epoch: 18, batch: 37, training loss: 1.949568
Epoch: 18, batch: 38, training loss: 2.142380
Epoch: 18, batch: 39, training loss: 2.494856
Epoch: 18, batch: 40, training loss: 1.660918
Epoch: 18, batch: 41, training loss: 2.062225
Epoch: 18, batch: 42, training loss: 2.165961
Epoch: 18, batch: 43, training loss: 1.719016
Epoch: 18, batch: 44, training loss: 2.327289
Epoch: 18, batch: 45, training loss: 2.310373
Epoch: 18, batch: 46, training loss: 2.270544
Epoch: 18, batch: 47, training loss: 2.194557
Epoch: 18, batch: 48, training loss: 1.929179
Epoch: 18, batch: 49, training loss: 2.137289
Epoch: 19, batch: 0, training loss: 1.832363
Epoch: 19, batch: 1, training loss: 1.942209
Epoch: 19, batch: 2, training loss: 1.205749
Epoch: 19, batch: 3, training loss: 1.766357
Epoch: 19, batch: 4, training loss: 0.852433
Epoch: 19, batch: 5, training loss: 0.967559
Epoch: 19, batch: 6, training loss: 0.823177
Epoch: 19, batch: 7, training loss: 0.883763
Epoch: 19, batch: 8, training loss: 1.001675
Epoch: 19, batch: 9, training loss: 1.003188
Epoch: 19, batch: 10, training loss: 1.398792
Epoch: 19, batch: 11, training loss: 1.782298
Epoch: 19, batch: 12, training loss: 1.561493
Epoch: 19, batch: 13, training loss: 1.792250
Epoch: 19, batch: 14, training loss: 1.655367
Epoch: 19, batch: 15, training loss: 1.929699
Epoch: 19, batch: 16, training loss: 1.689912
Epoch: 19, batch: 17, training loss: 1.799778
Epoch: 19, batch: 18, training loss: 1.639120
Epoch: 19, batch: 19, training loss: 1.303984
Epoch: 19, batch: 20, training loss: 1.843862
Epoch: 19, batch: 21, training loss: 1.189901
Epoch: 19, batch: 22, training loss: 1.683465
Epoch: 19, batch: 23, training loss: 1.592829
Epoch: 19, batch: 24, training loss: 1.668307
Epoch: 19, batch: 25, training loss: 1.779336
Epoch: 19, batch: 26, training loss: 1.852697
Epoch: 19, batch: 27, training loss: 2.011090
Epoch: 19, batch: 28, training loss: 2.200742
Epoch: 19, batch: 29, training loss: 1.987039
Epoch: 19, batch: 30, training loss: 2.044300
Epoch: 19, batch: 31, training loss: 1.680580
Epoch: 19, batch: 32, training loss: 1.646770
Epoch: 19, batch: 33, training loss: 1.900710
Epoch: 19, batch: 34, training loss: 1.762271
Epoch: 19, batch: 35, training loss: 1.876746
Epoch: 19, batch: 36, training loss: 1.930459
Epoch: 19, batch: 37, training loss: 1.841137
Epoch: 19, batch: 38, training loss: 2.053855
Epoch: 19, batch: 39, training loss: 2.328748
Epoch: 19, batch: 40, training loss: 1.563636
Epoch: 19, batch: 41, training loss: 1.971184
Epoch: 19, batch: 42, training loss: 2.069466
Epoch: 19, batch: 43, training loss: 1.620544
Epoch: 19, batch: 44, training loss: 2.206928
Epoch: 19, batch: 45, training loss: 2.194793
Epoch: 19, batch: 46, training loss: 2.149307
Epoch: 19, batch: 47, training loss: 2.056763
Epoch: 19, batch: 48, training loss: 1.816281
Epoch: 19, batch: 49, training loss: 2.034348
Epoch: 20, batch: 0, training loss: 1.750662
Epoch: 20, batch: 1, training loss: 1.853042
Epoch: 20, batch: 2, training loss: 1.142919
Epoch: 20, batch: 3, training loss: 1.707157
Epoch: 20, batch: 4, training loss: 0.797381
Epoch: 20, batch: 5, training loss: 0.911657
Epoch: 20, batch: 6, training loss: 0.776847
Epoch: 20, batch: 7, training loss: 0.847742
Epoch: 20, batch: 8, training loss: 0.934237
Epoch: 20, batch: 9, training loss: 0.919159
Epoch: 20, batch: 10, training loss: 1.327207
Epoch: 20, batch: 11, training loss: 1.716625
Epoch: 20, batch: 12, training loss: 1.494865
Epoch: 20, batch: 13, training loss: 1.731339
Epoch: 20, batch: 14, training loss: 1.626943
Epoch: 20, batch: 15, training loss: 1.892232
Epoch: 20, batch: 16, training loss: 1.642390
Epoch: 20, batch: 17, training loss: 1.757151
Epoch: 20, batch: 18, training loss: 1.612363
Epoch: 20, batch: 19, training loss: 1.259843
Epoch: 20, batch: 20, training loss: 1.782908
Epoch: 20, batch: 21, training loss: 1.121564
Epoch: 20, batch: 22, training loss: 1.618796
Epoch: 20, batch: 23, training loss: 1.553882
Epoch: 20, batch: 24, training loss: 1.622165
Epoch: 20, batch: 25, training loss: 1.766763
Epoch: 20, batch: 26, training loss: 1.809794
Epoch: 20, batch: 27, training loss: 1.997965
Epoch: 20, batch: 28, training loss: 2.127529
Epoch: 20, batch: 29, training loss: 1.921581
Epoch: 20, batch: 30, training loss: 1.999931
Epoch: 20, batch: 31, training loss: 1.661436
Epoch: 20, batch: 32, training loss: 1.619122
Epoch: 20, batch: 33, training loss: 1.839710
Epoch: 20, batch: 34, training loss: 1.731667
Epoch: 20, batch: 35, training loss: 1.837321
Epoch: 20, batch: 36, training loss: 1.857732
Epoch: 20, batch: 37, training loss: 1.773144
Epoch: 20, batch: 38, training loss: 1.978445
Epoch: 20, batch: 39, training loss: 2.267322
Epoch: 20, batch: 40, training loss: 1.524709
Epoch: 20, batch: 41, training loss: 1.903883
Epoch: 20, batch: 42, training loss: 2.012218
Epoch: 20, batch: 43, training loss: 1.575626
Epoch: 20, batch: 44, training loss: 2.111120
Epoch: 20, batch: 45, training loss: 2.115620
Epoch: 20, batch: 46, training loss: 2.079602
Epoch: 20, batch: 47, training loss: 2.001589
Epoch: 20, batch: 48, training loss: 1.771793
Epoch: 20, batch: 49, training loss: 1.967432
Epoch: 21, batch: 0, training loss: 1.706844
Epoch: 21, batch: 1, training loss: 1.806080
Epoch: 21, batch: 2, training loss: 1.134745
Epoch: 21, batch: 3, training loss: 1.655092
Epoch: 21, batch: 4, training loss: 0.800195
Epoch: 21, batch: 5, training loss: 0.906533
Epoch: 21, batch: 6, training loss: 0.778581
Epoch: 21, batch: 7, training loss: 0.833950
Epoch: 21, batch: 8, training loss: 0.936847
Epoch: 21, batch: 9, training loss: 0.908711
Epoch: 21, batch: 10, training loss: 1.308511
Epoch: 21, batch: 11, training loss: 1.687286
Epoch: 21, batch: 12, training loss: 1.467070
Epoch: 21, batch: 13, training loss: 1.707471
Epoch: 21, batch: 14, training loss: 1.596390
Epoch: 21, batch: 15, training loss: 1.869285
Epoch: 21, batch: 16, training loss: 1.652595
Epoch: 21, batch: 17, training loss: 1.744294
Epoch: 21, batch: 18, training loss: 1.594143
Epoch: 21, batch: 19, training loss: 1.246457
Epoch: 21, batch: 20, training loss: 1.758391
Epoch: 21, batch: 21, training loss: 1.104552
Epoch: 21, batch: 22, training loss: 1.550158
Epoch: 21, batch: 23, training loss: 1.516910
Epoch: 21, batch: 24, training loss: 1.593488
Epoch: 21, batch: 25, training loss: 1.735199
Epoch: 21, batch: 26, training loss: 1.797640
Epoch: 21, batch: 27, training loss: 1.956854
Epoch: 21, batch: 28, training loss: 2.088966
Epoch: 21, batch: 29, training loss: 1.911419
Epoch: 21, batch: 30, training loss: 1.997353
Epoch: 21, batch: 31, training loss: 1.643441
Epoch: 21, batch: 32, training loss: 1.606470
Epoch: 21, batch: 33, training loss: 1.792061
Epoch: 21, batch: 34, training loss: 1.701368
Epoch: 21, batch: 35, training loss: 1.830892
Epoch: 21, batch: 36, training loss: 1.828604
Epoch: 21, batch: 37, training loss: 1.720281
Epoch: 21, batch: 38, training loss: 1.934333
Epoch: 21, batch: 39, training loss: 2.213178
Epoch: 21, batch: 40, training loss: 1.496301
Epoch: 21, batch: 41, training loss: 1.867995
Epoch: 21, batch: 42, training loss: 1.969789
Epoch: 21, batch: 43, training loss: 1.552395
Epoch: 21, batch: 44, training loss: 2.070216
Epoch: 21, batch: 45, training loss: 2.115093
Epoch: 21, batch: 46, training loss: 2.023645
Epoch: 21, batch: 47, training loss: 1.937058
Epoch: 21, batch: 48, training loss: 1.721983
Epoch: 21, batch: 49, training loss: 1.888324
Epoch: 22, batch: 0, training loss: 1.691769
Epoch: 22, batch: 1, training loss: 1.788940
Epoch: 22, batch: 2, training loss: 1.146065
Epoch: 22, batch: 3, training loss: 1.661929
Epoch: 22, batch: 4, training loss: 0.789562
Epoch: 22, batch: 5, training loss: 0.888931
Epoch: 22, batch: 6, training loss: 0.757252
Epoch: 22, batch: 7, training loss: 0.831700
Epoch: 22, batch: 8, training loss: 0.917881
Epoch: 22, batch: 9, training loss: 0.932366
Epoch: 22, batch: 10, training loss: 1.296284
Epoch: 22, batch: 11, training loss: 1.685223
Epoch: 22, batch: 12, training loss: 1.458676
Epoch: 22, batch: 13, training loss: 1.674026
Epoch: 22, batch: 14, training loss: 1.561445
Epoch: 22, batch: 15, training loss: 1.855095
Epoch: 22, batch: 16, training loss: 1.624019
Epoch: 22, batch: 17, training loss: 1.738807
Epoch: 22, batch: 18, training loss: 1.573876
Epoch: 22, batch: 19, training loss: 1.244445
Epoch: 22, batch: 20, training loss: 1.768288
Epoch: 22, batch: 21, training loss: 1.089437
Epoch: 22, batch: 22, training loss: 1.566340
Epoch: 22, batch: 23, training loss: 1.505010
Epoch: 22, batch: 24, training loss: 1.601754
Epoch: 22, batch: 25, training loss: 1.710414
Epoch: 22, batch: 26, training loss: 1.769000
Epoch: 22, batch: 27, training loss: 1.928089
Epoch: 22, batch: 28, training loss: 2.086791
Epoch: 22, batch: 29, training loss: 1.895685
Epoch: 22, batch: 30, training loss: 1.972575
Epoch: 22, batch: 31, training loss: 1.638942
Epoch: 22, batch: 32, training loss: 1.592822
Epoch: 22, batch: 33, training loss: 1.778310
Epoch: 22, batch: 34, training loss: 1.674958
Epoch: 22, batch: 35, training loss: 1.784544
Epoch: 22, batch: 36, training loss: 1.795877
Epoch: 22, batch: 37, training loss: 1.699954
Epoch: 22, batch: 38, training loss: 1.898892
Epoch: 22, batch: 39, training loss: 2.191807
Epoch: 22, batch: 40, training loss: 1.481360
Epoch: 22, batch: 41, training loss: 1.858362
Epoch: 22, batch: 42, training loss: 1.949330
Epoch: 22, batch: 43, training loss: 1.535154
Epoch: 22, batch: 44, training loss: 2.020890
Epoch: 22, batch: 45, training loss: 2.087606
Epoch: 22, batch: 46, training loss: 2.009168
Epoch: 22, batch: 47, training loss: 1.899845
Epoch: 22, batch: 48, training loss: 1.685255
Epoch: 22, batch: 49, training loss: 1.881642
Epoch: 23, batch: 0, training loss: 1.667262
Epoch: 23, batch: 1, training loss: 1.779920
Epoch: 23, batch: 2, training loss: 1.147804
Epoch: 23, batch: 3, training loss: 1.657929
Epoch: 23, batch: 4, training loss: 0.794872
Epoch: 23, batch: 5, training loss: 0.912514
Epoch: 23, batch: 6, training loss: 0.745256
Epoch: 23, batch: 7, training loss: 0.844463
Epoch: 23, batch: 8, training loss: 0.922265
Epoch: 23, batch: 9, training loss: 0.913669
Epoch: 23, batch: 10, training loss: 1.322850
Epoch: 23, batch: 11, training loss: 1.670832
Epoch: 23, batch: 12, training loss: 1.456670
Epoch: 23, batch: 13, training loss: 1.674973
Epoch: 23, batch: 14, training loss: 1.560184
Epoch: 23, batch: 15, training loss: 1.863301
Epoch: 23, batch: 16, training loss: 1.624089
Epoch: 23, batch: 17, training loss: 1.715855
Epoch: 23, batch: 18, training loss: 1.563011
Epoch: 23, batch: 19, training loss: 1.248720
Epoch: 23, batch: 20, training loss: 1.748025
Epoch: 23, batch: 21, training loss: 1.053954
Epoch: 23, batch: 22, training loss: 1.582427
Epoch: 23, batch: 23, training loss: 1.525275
Epoch: 23, batch: 24, training loss: 1.589697
Epoch: 23, batch: 25, training loss: 1.701631
Epoch: 23, batch: 26, training loss: 1.762447
Epoch: 23, batch: 27, training loss: 1.908271
Epoch: 23, batch: 28, training loss: 2.074082
Epoch: 23, batch: 29, training loss: 1.864066
Epoch: 23, batch: 30, training loss: 1.967776
Epoch: 23, batch: 31, training loss: 1.620436
Epoch: 23, batch: 32, training loss: 1.601845
Epoch: 23, batch: 33, training loss: 1.820508
Epoch: 23, batch: 34, training loss: 1.685267
Epoch: 23, batch: 35, training loss: 1.803928
Epoch: 23, batch: 36, training loss: 1.770949
Epoch: 23, batch: 37, training loss: 1.667132
Epoch: 23, batch: 38, training loss: 1.874606
Epoch: 23, batch: 39, training loss: 2.183259
Epoch: 23, batch: 40, training loss: 1.481607
Epoch: 23, batch: 41, training loss: 1.869647
Epoch: 23, batch: 42, training loss: 1.928814
Epoch: 23, batch: 43, training loss: 1.536927
Epoch: 23, batch: 44, training loss: 2.010258
Epoch: 23, batch: 45, training loss: 2.047142
Epoch: 23, batch: 46, training loss: 1.983088
Epoch: 23, batch: 47, training loss: 1.869579
Epoch: 23, batch: 48, training loss: 1.680421
Epoch: 23, batch: 49, training loss: 1.842165
Epoch: 24, batch: 0, training loss: 1.661363
Epoch: 24, batch: 1, training loss: 1.761748
Epoch: 24, batch: 2, training loss: 1.157757
Epoch: 24, batch: 3, training loss: 1.620742
Epoch: 24, batch: 4, training loss: 0.823706
Epoch: 24, batch: 5, training loss: 0.924091
Epoch: 24, batch: 6, training loss: 0.782191
Epoch: 24, batch: 7, training loss: 0.832046
Epoch: 24, batch: 8, training loss: 0.916529
Epoch: 24, batch: 9, training loss: 0.944619
Epoch: 24, batch: 10, training loss: 1.308476
Epoch: 24, batch: 11, training loss: 1.679533
Epoch: 24, batch: 12, training loss: 1.454818
Epoch: 24, batch: 13, training loss: 1.701646
Epoch: 24, batch: 14, training loss: 1.574696
Epoch: 24, batch: 15, training loss: 1.845502
Epoch: 24, batch: 16, training loss: 1.616760
Epoch: 24, batch: 17, training loss: 1.716822
Epoch: 24, batch: 18, training loss: 1.552330
Epoch: 24, batch: 19, training loss: 1.222559
Epoch: 24, batch: 20, training loss: 1.729912
Epoch: 24, batch: 21, training loss: 1.085060
Epoch: 24, batch: 22, training loss: 1.580513
Epoch: 24, batch: 23, training loss: 1.504742
Epoch: 24, batch: 24, training loss: 1.605620
Epoch: 24, batch: 25, training loss: 1.681751
Epoch: 24, batch: 26, training loss: 1.760828
Epoch: 24, batch: 27, training loss: 1.907065
Epoch: 24, batch: 28, training loss: 2.033338
Epoch: 24, batch: 29, training loss: 1.841515
Epoch: 24, batch: 30, training loss: 1.934981
Epoch: 24, batch: 31, training loss: 1.604547
Epoch: 24, batch: 32, training loss: 1.572764
Epoch: 24, batch: 33, training loss: 1.778989
Epoch: 24, batch: 34, training loss: 1.669944
Epoch: 24, batch: 35, training loss: 1.791168
Epoch: 24, batch: 36, training loss: 1.757495
Epoch: 24, batch: 37, training loss: 1.643024
Epoch: 24, batch: 38, training loss: 1.844492
Epoch: 24, batch: 39, training loss: 2.160468
Epoch: 24, batch: 40, training loss: 1.441211
Epoch: 24, batch: 41, training loss: 1.820010
Epoch: 24, batch: 42, training loss: 1.899755
Epoch: 24, batch: 43, training loss: 1.511747
Epoch: 24, batch: 44, training loss: 2.000068
Epoch: 24, batch: 45, training loss: 2.048889
Epoch: 24, batch: 46, training loss: 1.983019
Epoch: 24, batch: 47, training loss: 1.847243
Epoch: 24, batch: 48, training loss: 1.665838
Epoch: 24, batch: 49, training loss: 1.829770
Epoch: 25, batch: 0, training loss: 1.668370
Epoch: 25, batch: 1, training loss: 1.724537
Epoch: 25, batch: 2, training loss: 1.132559
Epoch: 25, batch: 3, training loss: 1.604055
Epoch: 25, batch: 4, training loss: 0.814925
Epoch: 25, batch: 5, training loss: 0.902337
Epoch: 25, batch: 6, training loss: 0.747830
Epoch: 25, batch: 7, training loss: 0.840358
Epoch: 25, batch: 8, training loss: 0.921412
Epoch: 25, batch: 9, training loss: 0.939874
Epoch: 25, batch: 10, training loss: 1.311276
Epoch: 25, batch: 11, training loss: 1.661327
Epoch: 25, batch: 12, training loss: 1.460377
Epoch: 25, batch: 13, training loss: 1.702408
Epoch: 25, batch: 14, training loss: 1.564850
Epoch: 25, batch: 15, training loss: 1.845283
Epoch: 25, batch: 16, training loss: 1.598171
Epoch: 25, batch: 17, training loss: 1.706354
Epoch: 25, batch: 18, training loss: 1.547021
Epoch: 25, batch: 19, training loss: 1.223613
Epoch: 25, batch: 20, training loss: 1.686778
Epoch: 25, batch: 21, training loss: 1.151333
Epoch: 25, batch: 22, training loss: 1.569970
Epoch: 25, batch: 23, training loss: 1.489660
Epoch: 25, batch: 24, training loss: 1.612208
Epoch: 25, batch: 25, training loss: 1.676043
Epoch: 25, batch: 26, training loss: 1.745163
Epoch: 25, batch: 27, training loss: 1.878519
Epoch: 25, batch: 28, training loss: 2.002012
Epoch: 25, batch: 29, training loss: 1.806000
Epoch: 25, batch: 30, training loss: 1.908904
Epoch: 25, batch: 31, training loss: 1.605189
Epoch: 25, batch: 32, training loss: 1.566500
Epoch: 25, batch: 33, training loss: 1.800650
Epoch: 25, batch: 34, training loss: 1.629061
Epoch: 25, batch: 35, training loss: 1.753908
Epoch: 25, batch: 36, training loss: 1.759543
Epoch: 25, batch: 37, training loss: 1.663711
Epoch: 25, batch: 38, training loss: 1.841516
Epoch: 25, batch: 39, training loss: 2.128491
Epoch: 25, batch: 40, training loss: 1.419727
Epoch: 25, batch: 41, training loss: 1.799803
Epoch: 25, batch: 42, training loss: 1.862575
Epoch: 25, batch: 43, training loss: 1.493467
Epoch: 25, batch: 44, training loss: 1.952350
Epoch: 25, batch: 45, training loss: 1.992821
Epoch: 25, batch: 46, training loss: 1.952565
Epoch: 25, batch: 47, training loss: 1.835252
Epoch: 25, batch: 48, training loss: 1.656278
Epoch: 25, batch: 49, training loss: 1.835592
Epoch: 26, batch: 0, training loss: 1.644546
Epoch: 26, batch: 1, training loss: 1.718774
Epoch: 26, batch: 2, training loss: 1.104891
Epoch: 26, batch: 3, training loss: 1.592406
Epoch: 26, batch: 4, training loss: 0.801701
Epoch: 26, batch: 5, training loss: 0.886845
Epoch: 26, batch: 6, training loss: 0.788166
Epoch: 26, batch: 7, training loss: 0.799697
Epoch: 26, batch: 8, training loss: 0.924126
Epoch: 26, batch: 9, training loss: 0.926231
Epoch: 26, batch: 10, training loss: 1.281741
Epoch: 26, batch: 11, training loss: 1.667755
Epoch: 26, batch: 12, training loss: 1.443641
Epoch: 26, batch: 13, training loss: 1.691235
Epoch: 26, batch: 14, training loss: 1.549018
Epoch: 26, batch: 15, training loss: 1.848815
Epoch: 26, batch: 16, training loss: 1.595852
Epoch: 26, batch: 17, training loss: 1.687888
Epoch: 26, batch: 18, training loss: 1.531137
Epoch: 26, batch: 19, training loss: 1.199277
Epoch: 26, batch: 20, training loss: 1.686582
Epoch: 26, batch: 21, training loss: 1.123754
Epoch: 26, batch: 22, training loss: 1.575070
Epoch: 26, batch: 23, training loss: 1.466694
Epoch: 26, batch: 24, training loss: 1.587366
Epoch: 26, batch: 25, training loss: 1.670554
Epoch: 26, batch: 26, training loss: 1.710842
Epoch: 26, batch: 27, training loss: 1.874312
Epoch: 26, batch: 28, training loss: 1.980241
Epoch: 26, batch: 29, training loss: 1.783889
Epoch: 26, batch: 30, training loss: 1.880584
Epoch: 26, batch: 31, training loss: 1.547529
Epoch: 26, batch: 32, training loss: 1.553482
Epoch: 26, batch: 33, training loss: 1.745783
Epoch: 26, batch: 34, training loss: 1.604833
Epoch: 26, batch: 35, training loss: 1.724277
Epoch: 26, batch: 36, training loss: 1.696792
Epoch: 26, batch: 37, training loss: 1.621785
Epoch: 26, batch: 38, training loss: 1.796340
Epoch: 26, batch: 39, training loss: 2.077434
Epoch: 26, batch: 40, training loss: 1.397223
Epoch: 26, batch: 41, training loss: 1.759761
Epoch: 26, batch: 42, training loss: 1.823671
Epoch: 26, batch: 43, training loss: 1.480948
Epoch: 26, batch: 44, training loss: 1.920629
Epoch: 26, batch: 45, training loss: 1.962354
Epoch: 26, batch: 46, training loss: 1.899659
Epoch: 26, batch: 47, training loss: 1.781273
Epoch: 26, batch: 48, training loss: 1.596883
Epoch: 26, batch: 49, training loss: 1.756287
Epoch: 27, batch: 0, training loss: 1.644925
Epoch: 27, batch: 1, training loss: 1.677685
Epoch: 27, batch: 2, training loss: 1.114777
Epoch: 27, batch: 3, training loss: 1.567311
Epoch: 27, batch: 4, training loss: 0.777215
Epoch: 27, batch: 5, training loss: 0.875302
Epoch: 27, batch: 6, training loss: 0.747443
Epoch: 27, batch: 7, training loss: 0.791216
Epoch: 27, batch: 8, training loss: 0.906305
Epoch: 27, batch: 9, training loss: 0.903442
Epoch: 27, batch: 10, training loss: 1.285649
Epoch: 27, batch: 11, training loss: 1.638827
Epoch: 27, batch: 12, training loss: 1.432097
Epoch: 27, batch: 13, training loss: 1.650511
Epoch: 27, batch: 14, training loss: 1.536147
Epoch: 27, batch: 15, training loss: 1.797055
Epoch: 27, batch: 16, training loss: 1.559070
Epoch: 27, batch: 17, training loss: 1.664546
Epoch: 27, batch: 18, training loss: 1.489271
Epoch: 27, batch: 19, training loss: 1.178636
Epoch: 27, batch: 20, training loss: 1.652375
Epoch: 27, batch: 21, training loss: 1.108064
Epoch: 27, batch: 22, training loss: 1.539077
Epoch: 27, batch: 23, training loss: 1.467305
Epoch: 27, batch: 24, training loss: 1.565479
Epoch: 27, batch: 25, training loss: 1.653297
Epoch: 27, batch: 26, training loss: 1.710089
Epoch: 27, batch: 27, training loss: 1.826831
Epoch: 27, batch: 28, training loss: 1.952707
Epoch: 27, batch: 29, training loss: 1.759309
Epoch: 27, batch: 30, training loss: 1.846325
Epoch: 27, batch: 31, training loss: 1.544673
Epoch: 27, batch: 32, training loss: 1.534001
Epoch: 27, batch: 33, training loss: 1.711972
Epoch: 27, batch: 34, training loss: 1.587015
Epoch: 27, batch: 35, training loss: 1.711227
Epoch: 27, batch: 36, training loss: 1.676187
Epoch: 27, batch: 37, training loss: 1.596306
Epoch: 27, batch: 38, training loss: 1.759750
Epoch: 27, batch: 39, training loss: 2.027558
Epoch: 27, batch: 40, training loss: 1.366168
Epoch: 27, batch: 41, training loss: 1.708634
Epoch: 27, batch: 42, training loss: 1.805145
Epoch: 27, batch: 43, training loss: 1.431462
Epoch: 27, batch: 44, training loss: 1.890345
Epoch: 27, batch: 45, training loss: 1.925002
Epoch: 27, batch: 46, training loss: 1.873384
Epoch: 27, batch: 47, training loss: 1.745567
Epoch: 27, batch: 48, training loss: 1.575763
Epoch: 27, batch: 49, training loss: 1.759181
Epoch: 28, batch: 0, training loss: 1.602856
Epoch: 28, batch: 1, training loss: 1.649768
Epoch: 28, batch: 2, training loss: 1.072611
Epoch: 28, batch: 3, training loss: 1.532628
Epoch: 28, batch: 4, training loss: 0.745136
Epoch: 28, batch: 5, training loss: 0.850677
Epoch: 28, batch: 6, training loss: 0.755011
Epoch: 28, batch: 7, training loss: 0.793543
Epoch: 28, batch: 8, training loss: 0.909692
Epoch: 28, batch: 9, training loss: 0.857495
Epoch: 28, batch: 10, training loss: 1.251832
Epoch: 28, batch: 11, training loss: 1.626675
Epoch: 28, batch: 12, training loss: 1.415466
Epoch: 28, batch: 13, training loss: 1.637245
Epoch: 28, batch: 14, training loss: 1.527544
Epoch: 28, batch: 15, training loss: 1.793332
Epoch: 28, batch: 16, training loss: 1.527523
Epoch: 28, batch: 17, training loss: 1.651803
Epoch: 28, batch: 18, training loss: 1.485009
Epoch: 28, batch: 19, training loss: 1.170427
Epoch: 28, batch: 20, training loss: 1.621771
Epoch: 28, batch: 21, training loss: 1.028099
Epoch: 28, batch: 22, training loss: 1.476104
Epoch: 28, batch: 23, training loss: 1.440651
Epoch: 28, batch: 24, training loss: 1.546820
Epoch: 28, batch: 25, training loss: 1.617101
Epoch: 28, batch: 26, training loss: 1.677660
Epoch: 28, batch: 27, training loss: 1.818496
Epoch: 28, batch: 28, training loss: 1.934335
Epoch: 28, batch: 29, training loss: 1.718542
Epoch: 28, batch: 30, training loss: 1.832340
Epoch: 28, batch: 31, training loss: 1.499123
Epoch: 28, batch: 32, training loss: 1.501155
Epoch: 28, batch: 33, training loss: 1.659200
Epoch: 28, batch: 34, training loss: 1.551516
Epoch: 28, batch: 35, training loss: 1.667038
Epoch: 28, batch: 36, training loss: 1.636233
Epoch: 28, batch: 37, training loss: 1.543883
Epoch: 28, batch: 38, training loss: 1.745351
Epoch: 28, batch: 39, training loss: 1.992464
Epoch: 28, batch: 40, training loss: 1.333016
Epoch: 28, batch: 41, training loss: 1.685547
Epoch: 28, batch: 42, training loss: 1.751508
Epoch: 28, batch: 43, training loss: 1.423692
Epoch: 28, batch: 44, training loss: 1.821923
Epoch: 28, batch: 45, training loss: 1.885669
Epoch: 28, batch: 46, training loss: 1.840286
Epoch: 28, batch: 47, training loss: 1.702934
Epoch: 28, batch: 48, training loss: 1.550535
Epoch: 28, batch: 49, training loss: 1.721651
Epoch: 29, batch: 0, training loss: 1.585575
Epoch: 29, batch: 1, training loss: 1.622497
Epoch: 29, batch: 2, training loss: 1.060476
Epoch: 29, batch: 3, training loss: 1.508162
Epoch: 29, batch: 4, training loss: 0.721659
Epoch: 29, batch: 5, training loss: 0.821719
Epoch: 29, batch: 6, training loss: 0.711412
Epoch: 29, batch: 7, training loss: 0.778525
Epoch: 29, batch: 8, training loss: 0.891360
Epoch: 29, batch: 9, training loss: 0.860245
Epoch: 29, batch: 10, training loss: 1.255866
Epoch: 29, batch: 11, training loss: 1.607184
Epoch: 29, batch: 12, training loss: 1.402899
Epoch: 29, batch: 13, training loss: 1.607498
Epoch: 29, batch: 14, training loss: 1.506431
Epoch: 29, batch: 15, training loss: 1.764116
Epoch: 29, batch: 16, training loss: 1.526423
Epoch: 29, batch: 17, training loss: 1.643300
Epoch: 29, batch: 18, training loss: 1.473452
Epoch: 29, batch: 19, training loss: 1.144565
Epoch: 29, batch: 20, training loss: 1.565143
Epoch: 29, batch: 21, training loss: 0.977799
Epoch: 29, batch: 22, training loss: 1.429785
Epoch: 29, batch: 23, training loss: 1.397328
Epoch: 29, batch: 24, training loss: 1.525188
Epoch: 29, batch: 25, training loss: 1.577380
Epoch: 29, batch: 26, training loss: 1.630474
Epoch: 29, batch: 27, training loss: 1.780565
Epoch: 29, batch: 28, training loss: 1.912258
Epoch: 29, batch: 29, training loss: 1.687443
Epoch: 29, batch: 30, training loss: 1.784321
Epoch: 29, batch: 31, training loss: 1.476390
Epoch: 29, batch: 32, training loss: 1.471876
Epoch: 29, batch: 33, training loss: 1.608901
Epoch: 29, batch: 34, training loss: 1.510991
Epoch: 29, batch: 35, training loss: 1.649183
Epoch: 29, batch: 36, training loss: 1.626471
Epoch: 29, batch: 37, training loss: 1.525489
Epoch: 29, batch: 38, training loss: 1.730979
Epoch: 29, batch: 39, training loss: 1.959895
Epoch: 29, batch: 40, training loss: 1.310124
Epoch: 29, batch: 41, training loss: 1.641390
Epoch: 29, batch: 42, training loss: 1.719817
Epoch: 29, batch: 43, training loss: 1.377745
Epoch: 29, batch: 44, training loss: 1.789607
Epoch: 29, batch: 45, training loss: 1.834733
Epoch: 29, batch: 46, training loss: 1.798415
Epoch: 29, batch: 47, training loss: 1.686145
Epoch: 29, batch: 48, training loss: 1.481145
Epoch: 29, batch: 49, training loss: 1.685449
Epoch: 30, batch: 0, training loss: 1.571528
Epoch: 30, batch: 1, training loss: 1.617254
Epoch: 30, batch: 2, training loss: 1.040035
Epoch: 30, batch: 3, training loss: 1.461293
Epoch: 30, batch: 4, training loss: 0.702231
Epoch: 30, batch: 5, training loss: 0.793469
Epoch: 30, batch: 6, training loss: 0.684641
Epoch: 30, batch: 7, training loss: 0.743624
Epoch: 30, batch: 8, training loss: 0.840327
Epoch: 30, batch: 9, training loss: 0.804828
Epoch: 30, batch: 10, training loss: 1.205523
Epoch: 30, batch: 11, training loss: 1.577853
Epoch: 30, batch: 12, training loss: 1.357030
Epoch: 30, batch: 13, training loss: 1.578902
Epoch: 30, batch: 14, training loss: 1.469338
Epoch: 30, batch: 15, training loss: 1.741086
Epoch: 30, batch: 16, training loss: 1.517491
Epoch: 30, batch: 17, training loss: 1.619421
Epoch: 30, batch: 18, training loss: 1.448968
Epoch: 30, batch: 19, training loss: 1.121761
Epoch: 30, batch: 20, training loss: 1.549215
Epoch: 30, batch: 21, training loss: 0.955242
Epoch: 30, batch: 22, training loss: 1.398968
Epoch: 30, batch: 23, training loss: 1.368014
Epoch: 30, batch: 24, training loss: 1.454222
Epoch: 30, batch: 25, training loss: 1.563350
Epoch: 30, batch: 26, training loss: 1.585351
Epoch: 30, batch: 27, training loss: 1.747157
Epoch: 30, batch: 28, training loss: 1.855168
Epoch: 30, batch: 29, training loss: 1.682368
Epoch: 30, batch: 30, training loss: 1.774588
Epoch: 30, batch: 31, training loss: 1.458868
Epoch: 30, batch: 32, training loss: 1.435783
Epoch: 30, batch: 33, training loss: 1.554193
Epoch: 30, batch: 34, training loss: 1.469689
Epoch: 30, batch: 35, training loss: 1.610190
Epoch: 30, batch: 36, training loss: 1.575602
Epoch: 30, batch: 37, training loss: 1.474748
Epoch: 30, batch: 38, training loss: 1.684170
Epoch: 30, batch: 39, training loss: 1.932164
Epoch: 30, batch: 40, training loss: 1.280583
Epoch: 30, batch: 41, training loss: 1.626442
Epoch: 30, batch: 42, training loss: 1.702591
Epoch: 30, batch: 43, training loss: 1.327795
Epoch: 30, batch: 44, training loss: 1.766568
Epoch: 30, batch: 45, training loss: 1.819098
Epoch: 30, batch: 46, training loss: 1.752649
Epoch: 30, batch: 47, training loss: 1.645110
Epoch: 30, batch: 48, training loss: 1.474683
Epoch: 30, batch: 49, training loss: 1.657195
Epoch: 31, batch: 0, training loss: 1.520445
Epoch: 31, batch: 1, training loss: 1.566106
Epoch: 31, batch: 2, training loss: 1.010939
Epoch: 31, batch: 3, training loss: 1.447964
Epoch: 31, batch: 4, training loss: 0.687583
Epoch: 31, batch: 5, training loss: 0.789191
Epoch: 31, batch: 6, training loss: 0.680839
Epoch: 31, batch: 7, training loss: 0.716805
Epoch: 31, batch: 8, training loss: 0.832861
Epoch: 31, batch: 9, training loss: 0.815870
Epoch: 31, batch: 10, training loss: 1.190724
Epoch: 31, batch: 11, training loss: 1.524498
Epoch: 31, batch: 12, training loss: 1.322402
Epoch: 31, batch: 13, training loss: 1.544343
Epoch: 31, batch: 14, training loss: 1.451229
Epoch: 31, batch: 15, training loss: 1.707523
Epoch: 31, batch: 16, training loss: 1.488751
Epoch: 31, batch: 17, training loss: 1.592611
Epoch: 31, batch: 18, training loss: 1.442509
Epoch: 31, batch: 19, training loss: 1.125971
Epoch: 31, batch: 20, training loss: 1.559362
Epoch: 31, batch: 21, training loss: 0.930416
Epoch: 31, batch: 22, training loss: 1.350386
Epoch: 31, batch: 23, training loss: 1.342598
Epoch: 31, batch: 24, training loss: 1.452013
Epoch: 31, batch: 25, training loss: 1.524821
Epoch: 31, batch: 26, training loss: 1.585546
Epoch: 31, batch: 27, training loss: 1.699845
Epoch: 31, batch: 28, training loss: 1.838529
Epoch: 31, batch: 29, training loss: 1.640328
Epoch: 31, batch: 30, training loss: 1.738278
Epoch: 31, batch: 31, training loss: 1.425583
Epoch: 31, batch: 32, training loss: 1.428233
Epoch: 31, batch: 33, training loss: 1.525598
Epoch: 31, batch: 34, training loss: 1.444466
Epoch: 31, batch: 35, training loss: 1.590286
Epoch: 31, batch: 36, training loss: 1.560968
Epoch: 31, batch: 37, training loss: 1.478672
Epoch: 31, batch: 38, training loss: 1.673099
Epoch: 31, batch: 39, training loss: 1.909435
Epoch: 31, batch: 40, training loss: 1.274413
Epoch: 31, batch: 41, training loss: 1.584414
Epoch: 31, batch: 42, training loss: 1.685056
Epoch: 31, batch: 43, training loss: 1.348283
Epoch: 31, batch: 44, training loss: 1.733584
Epoch: 31, batch: 45, training loss: 1.768922
Epoch: 31, batch: 46, training loss: 1.740782
Epoch: 31, batch: 47, training loss: 1.607092
Epoch: 31, batch: 48, training loss: 1.434130
Epoch: 31, batch: 49, training loss: 1.622133
Epoch: 32, batch: 0, training loss: 1.496122
Epoch: 32, batch: 1, training loss: 1.526743
Epoch: 32, batch: 2, training loss: 0.984276
Epoch: 32, batch: 3, training loss: 1.424930
Epoch: 32, batch: 4, training loss: 0.694302
Epoch: 32, batch: 5, training loss: 0.774969
Epoch: 32, batch: 6, training loss: 0.669046
Epoch: 32, batch: 7, training loss: 0.694001
Epoch: 32, batch: 8, training loss: 0.816961
Epoch: 32, batch: 9, training loss: 0.799017
Epoch: 32, batch: 10, training loss: 1.159432
Epoch: 32, batch: 11, training loss: 1.505015
Epoch: 32, batch: 12, training loss: 1.305478
Epoch: 32, batch: 13, training loss: 1.508667
Epoch: 32, batch: 14, training loss: 1.427187
Epoch: 32, batch: 15, training loss: 1.680689
Epoch: 32, batch: 16, training loss: 1.510247
Epoch: 32, batch: 17, training loss: 1.571718
Epoch: 32, batch: 18, training loss: 1.405720
Epoch: 32, batch: 19, training loss: 1.113161
Epoch: 32, batch: 20, training loss: 1.524097
Epoch: 32, batch: 21, training loss: 0.920350
Epoch: 32, batch: 22, training loss: 1.350802
Epoch: 32, batch: 23, training loss: 1.318456
Epoch: 32, batch: 24, training loss: 1.398497
Epoch: 32, batch: 25, training loss: 1.486110
Epoch: 32, batch: 26, training loss: 1.539782
Epoch: 32, batch: 27, training loss: 1.688626
Epoch: 32, batch: 28, training loss: 1.803784
Epoch: 32, batch: 29, training loss: 1.638309
Epoch: 32, batch: 30, training loss: 1.726248
Epoch: 32, batch: 31, training loss: 1.433047
Epoch: 32, batch: 32, training loss: 1.413855
Epoch: 32, batch: 33, training loss: 1.494495
Epoch: 32, batch: 34, training loss: 1.382667
Epoch: 32, batch: 35, training loss: 1.517700
Epoch: 32, batch: 36, training loss: 1.534621
Epoch: 32, batch: 37, training loss: 1.432299
Epoch: 32, batch: 38, training loss: 1.637522
Epoch: 32, batch: 39, training loss: 1.844841
Epoch: 32, batch: 40, training loss: 1.259208
Epoch: 32, batch: 41, training loss: 1.596979
Epoch: 32, batch: 42, training loss: 1.665203
Epoch: 32, batch: 43, training loss: 1.338371
Epoch: 32, batch: 44, training loss: 1.697708
Epoch: 32, batch: 45, training loss: 1.758236
Epoch: 32, batch: 46, training loss: 1.700553
Epoch: 32, batch: 47, training loss: 1.596286
Epoch: 32, batch: 48, training loss: 1.408360
Epoch: 32, batch: 49, training loss: 1.568569
Epoch: 33, batch: 0, training loss: 1.441242
Epoch: 33, batch: 1, training loss: 1.512466
Epoch: 33, batch: 2, training loss: 0.984535
Epoch: 33, batch: 3, training loss: 1.403336
Epoch: 33, batch: 4, training loss: 0.649362
Epoch: 33, batch: 5, training loss: 0.756964
Epoch: 33, batch: 6, training loss: 0.633015
Epoch: 33, batch: 7, training loss: 0.692949
Epoch: 33, batch: 8, training loss: 0.794236
Epoch: 33, batch: 9, training loss: 0.755225
Epoch: 33, batch: 10, training loss: 1.129717
Epoch: 33, batch: 11, training loss: 1.458923
Epoch: 33, batch: 12, training loss: 1.289305
Epoch: 33, batch: 13, training loss: 1.482345
Epoch: 33, batch: 14, training loss: 1.386557
Epoch: 33, batch: 15, training loss: 1.649172
Epoch: 33, batch: 16, training loss: 1.450274
Epoch: 33, batch: 17, training loss: 1.551457
Epoch: 33, batch: 18, training loss: 1.381915
Epoch: 33, batch: 19, training loss: 1.096007
Epoch: 33, batch: 20, training loss: 1.521531
Epoch: 33, batch: 21, training loss: 0.907372
Epoch: 33, batch: 22, training loss: 1.340222
Epoch: 33, batch: 23, training loss: 1.283216
Epoch: 33, batch: 24, training loss: 1.386998
Epoch: 33, batch: 25, training loss: 1.460348
Epoch: 33, batch: 26, training loss: 1.489535
Epoch: 33, batch: 27, training loss: 1.655975
Epoch: 33, batch: 28, training loss: 1.749638
Epoch: 33, batch: 29, training loss: 1.573959
Epoch: 33, batch: 30, training loss: 1.667163
Epoch: 33, batch: 31, training loss: 1.388616
Epoch: 33, batch: 32, training loss: 1.377821
Epoch: 33, batch: 33, training loss: 1.463035
Epoch: 33, batch: 34, training loss: 1.362643
Epoch: 33, batch: 35, training loss: 1.539527
Epoch: 33, batch: 36, training loss: 1.494892
Epoch: 33, batch: 37, training loss: 1.414624
Epoch: 33, batch: 38, training loss: 1.593432
Epoch: 33, batch: 39, training loss: 1.811354
Epoch: 33, batch: 40, training loss: 1.220572
Epoch: 33, batch: 41, training loss: 1.534334
Epoch: 33, batch: 42, training loss: 1.628297
Epoch: 33, batch: 43, training loss: 1.299405
Epoch: 33, batch: 44, training loss: 1.663775
Epoch: 33, batch: 45, training loss: 1.728531
Epoch: 33, batch: 46, training loss: 1.667141
Epoch: 33, batch: 47, training loss: 1.551373
Epoch: 33, batch: 48, training loss: 1.383466
Epoch: 33, batch: 49, training loss: 1.524977
Epoch: 34, batch: 0, training loss: 1.415332
Epoch: 34, batch: 1, training loss: 1.444179
Epoch: 34, batch: 2, training loss: 0.958334
Epoch: 34, batch: 3, training loss: 1.354845
Epoch: 34, batch: 4, training loss: 0.647504
Epoch: 34, batch: 5, training loss: 0.732797
Epoch: 34, batch: 6, training loss: 0.640907
Epoch: 34, batch: 7, training loss: 0.691715
Epoch: 34, batch: 8, training loss: 0.781195
Epoch: 34, batch: 9, training loss: 0.742459
Epoch: 34, batch: 10, training loss: 1.099589
Epoch: 34, batch: 11, training loss: 1.439168
Epoch: 34, batch: 12, training loss: 1.269683
Epoch: 34, batch: 13, training loss: 1.445019
Epoch: 34, batch: 14, training loss: 1.367827
Epoch: 34, batch: 15, training loss: 1.626971
Epoch: 34, batch: 16, training loss: 1.425219
Epoch: 34, batch: 17, training loss: 1.524468
Epoch: 34, batch: 18, training loss: 1.377032
Epoch: 34, batch: 19, training loss: 1.088847
Epoch: 34, batch: 20, training loss: 1.488742
Epoch: 34, batch: 21, training loss: 0.897251
Epoch: 34, batch: 22, training loss: 1.288179
Epoch: 34, batch: 23, training loss: 1.264415
Epoch: 34, batch: 24, training loss: 1.353032
Epoch: 34, batch: 25, training loss: 1.429348
Epoch: 34, batch: 26, training loss: 1.459251
Epoch: 34, batch: 27, training loss: 1.621718
Epoch: 34, batch: 28, training loss: 1.736194
Epoch: 34, batch: 29, training loss: 1.538723
Epoch: 34, batch: 30, training loss: 1.630667
Epoch: 34, batch: 31, training loss: 1.353380
Epoch: 34, batch: 32, training loss: 1.342752
Epoch: 34, batch: 33, training loss: 1.483593
Epoch: 34, batch: 34, training loss: 1.354656
Epoch: 34, batch: 35, training loss: 1.512221
Epoch: 34, batch: 36, training loss: 1.484618
Epoch: 34, batch: 37, training loss: 1.417204
Epoch: 34, batch: 38, training loss: 1.577127
Epoch: 34, batch: 39, training loss: 1.784830
Epoch: 34, batch: 40, training loss: 1.222767
Epoch: 34, batch: 41, training loss: 1.545325
Epoch: 34, batch: 42, training loss: 1.613937
Epoch: 34, batch: 43, training loss: 1.281343
Epoch: 34, batch: 44, training loss: 1.652224
Epoch: 34, batch: 45, training loss: 1.714466
Epoch: 34, batch: 46, training loss: 1.648447
Epoch: 34, batch: 47, training loss: 1.544519
Epoch: 34, batch: 48, training loss: 1.372123
Epoch: 34, batch: 49, training loss: 1.521439
Epoch: 35, batch: 0, training loss: 1.411430
Epoch: 35, batch: 1, training loss: 1.421635
Epoch: 35, batch: 2, training loss: 0.942877
Epoch: 35, batch: 3, training loss: 1.358641
Epoch: 35, batch: 4, training loss: 0.633725
Epoch: 35, batch: 5, training loss: 0.749246
Epoch: 35, batch: 6, training loss: 0.606955
Epoch: 35, batch: 7, training loss: 0.674780
Epoch: 35, batch: 8, training loss: 0.754471
Epoch: 35, batch: 9, training loss: 0.733901
Epoch: 35, batch: 10, training loss: 1.092198
Epoch: 35, batch: 11, training loss: 1.426380
Epoch: 35, batch: 12, training loss: 1.252971
Epoch: 35, batch: 13, training loss: 1.451147
Epoch: 35, batch: 14, training loss: 1.359145
Epoch: 35, batch: 15, training loss: 1.587238
Epoch: 35, batch: 16, training loss: 1.402740
Epoch: 35, batch: 17, training loss: 1.507237
Epoch: 35, batch: 18, training loss: 1.350252
Epoch: 35, batch: 19, training loss: 1.079478
Epoch: 35, batch: 20, training loss: 1.473228
Epoch: 35, batch: 21, training loss: 0.880958
Epoch: 35, batch: 22, training loss: 1.298927
Epoch: 35, batch: 23, training loss: 1.244994
Epoch: 35, batch: 24, training loss: 1.337348
Epoch: 35, batch: 25, training loss: 1.417798
Epoch: 35, batch: 26, training loss: 1.435600
Epoch: 35, batch: 27, training loss: 1.584516
Epoch: 35, batch: 28, training loss: 1.696616
Epoch: 35, batch: 29, training loss: 1.521639
Epoch: 35, batch: 30, training loss: 1.595150
Epoch: 35, batch: 31, training loss: 1.327903
Epoch: 35, batch: 32, training loss: 1.336057
Epoch: 35, batch: 33, training loss: 1.408226
Epoch: 35, batch: 34, training loss: 1.322942
Epoch: 35, batch: 35, training loss: 1.481006
Epoch: 35, batch: 36, training loss: 1.474814
Epoch: 35, batch: 37, training loss: 1.392601
Epoch: 35, batch: 38, training loss: 1.558714
Epoch: 35, batch: 39, training loss: 1.788507
Epoch: 35, batch: 40, training loss: 1.198658
Epoch: 35, batch: 41, training loss: 1.512403
Epoch: 35, batch: 42, training loss: 1.594179
Epoch: 35, batch: 43, training loss: 1.255262
Epoch: 35, batch: 44, training loss: 1.633633
Epoch: 35, batch: 45, training loss: 1.711497
Epoch: 35, batch: 46, training loss: 1.660207
Epoch: 35, batch: 47, training loss: 1.562144
Epoch: 35, batch: 48, training loss: 1.380570
Epoch: 35, batch: 49, training loss: 1.516045
Epoch: 36, batch: 0, training loss: 1.407251
Epoch: 36, batch: 1, training loss: 1.424172
Epoch: 36, batch: 2, training loss: 0.932960
Epoch: 36, batch: 3, training loss: 1.346224
Epoch: 36, batch: 4, training loss: 0.637262
Epoch: 36, batch: 5, training loss: 0.717024
Epoch: 36, batch: 6, training loss: 0.624741
Epoch: 36, batch: 7, training loss: 0.652186
Epoch: 36, batch: 8, training loss: 0.743492
Epoch: 36, batch: 9, training loss: 0.734958
Epoch: 36, batch: 10, training loss: 1.084869
Epoch: 36, batch: 11, training loss: 1.411051
Epoch: 36, batch: 12, training loss: 1.230137
Epoch: 36, batch: 13, training loss: 1.423524
Epoch: 36, batch: 14, training loss: 1.338298
Epoch: 36, batch: 15, training loss: 1.578337
Epoch: 36, batch: 16, training loss: 1.389668
Epoch: 36, batch: 17, training loss: 1.488999
Epoch: 36, batch: 18, training loss: 1.350331
Epoch: 36, batch: 19, training loss: 1.065681
Epoch: 36, batch: 20, training loss: 1.465642
Epoch: 36, batch: 21, training loss: 0.869042
Epoch: 36, batch: 22, training loss: 1.271222
Epoch: 36, batch: 23, training loss: 1.247553
Epoch: 36, batch: 24, training loss: 1.319258
Epoch: 36, batch: 25, training loss: 1.413298
Epoch: 36, batch: 26, training loss: 1.419227
Epoch: 36, batch: 27, training loss: 1.575628
Epoch: 36, batch: 28, training loss: 1.671403
Epoch: 36, batch: 29, training loss: 1.495124
Epoch: 36, batch: 30, training loss: 1.572426
Epoch: 36, batch: 31, training loss: 1.316067
Epoch: 36, batch: 32, training loss: 1.320243
Epoch: 36, batch: 33, training loss: 1.372116
Epoch: 36, batch: 34, training loss: 1.278511
Epoch: 36, batch: 35, training loss: 1.484958
Epoch: 36, batch: 36, training loss: 1.444112
Epoch: 36, batch: 37, training loss: 1.364387
Epoch: 36, batch: 38, training loss: 1.534667
Epoch: 36, batch: 39, training loss: 1.756626
Epoch: 36, batch: 40, training loss: 1.173121
Epoch: 36, batch: 41, training loss: 1.480887
Epoch: 36, batch: 42, training loss: 1.578731
Epoch: 36, batch: 43, training loss: 1.239304
Epoch: 36, batch: 44, training loss: 1.586563
Epoch: 36, batch: 45, training loss: 1.665457
Epoch: 36, batch: 46, training loss: 1.601065
Epoch: 36, batch: 47, training loss: 1.510956
Epoch: 36, batch: 48, training loss: 1.352123
Epoch: 36, batch: 49, training loss: 1.495420
Epoch: 37, batch: 0, training loss: 1.413931
Epoch: 37, batch: 1, training loss: 1.415813
Epoch: 37, batch: 2, training loss: 0.923657
Epoch: 37, batch: 3, training loss: 1.299263
Epoch: 37, batch: 4, training loss: 0.606212
Epoch: 37, batch: 5, training loss: 0.695522
Epoch: 37, batch: 6, training loss: 0.605341
Epoch: 37, batch: 7, training loss: 0.626989
Epoch: 37, batch: 8, training loss: 0.742771
Epoch: 37, batch: 9, training loss: 0.759886
Epoch: 37, batch: 10, training loss: 1.066463
Epoch: 37, batch: 11, training loss: 1.389153
Epoch: 37, batch: 12, training loss: 1.215425
Epoch: 37, batch: 13, training loss: 1.432098
Epoch: 37, batch: 14, training loss: 1.330896
Epoch: 37, batch: 15, training loss: 1.538066
Epoch: 37, batch: 16, training loss: 1.381772
Epoch: 37, batch: 17, training loss: 1.461810
Epoch: 37, batch: 18, training loss: 1.326775
Epoch: 37, batch: 19, training loss: 1.048360
Epoch: 37, batch: 20, training loss: 1.421784
Epoch: 37, batch: 21, training loss: 0.840536
Epoch: 37, batch: 22, training loss: 1.239978
Epoch: 37, batch: 23, training loss: 1.213101
Epoch: 37, batch: 24, training loss: 1.305221
Epoch: 37, batch: 25, training loss: 1.371581
Epoch: 37, batch: 26, training loss: 1.434001
Epoch: 37, batch: 27, training loss: 1.549572
Epoch: 37, batch: 28, training loss: 1.658437
Epoch: 37, batch: 29, training loss: 1.479699
Epoch: 37, batch: 30, training loss: 1.556708
Epoch: 37, batch: 31, training loss: 1.294286
Epoch: 37, batch: 32, training loss: 1.299523
Epoch: 37, batch: 33, training loss: 1.347051
Epoch: 37, batch: 34, training loss: 1.262466
Epoch: 37, batch: 35, training loss: 1.431596
Epoch: 37, batch: 36, training loss: 1.428582
Epoch: 37, batch: 37, training loss: 1.328120
Epoch: 37, batch: 38, training loss: 1.497461
Epoch: 37, batch: 39, training loss: 1.705783
Epoch: 37, batch: 40, training loss: 1.150184
Epoch: 37, batch: 41, training loss: 1.474263
Epoch: 37, batch: 42, training loss: 1.550169
Epoch: 37, batch: 43, training loss: 1.224475
Epoch: 37, batch: 44, training loss: 1.579572
Epoch: 37, batch: 45, training loss: 1.636430
Epoch: 37, batch: 46, training loss: 1.587231
Epoch: 37, batch: 47, training loss: 1.468590
Epoch: 37, batch: 48, training loss: 1.295274
Epoch: 37, batch: 49, training loss: 1.458589
Epoch: 38, batch: 0, training loss: 1.347569
Epoch: 38, batch: 1, training loss: 1.403117
Epoch: 38, batch: 2, training loss: 0.898631
Epoch: 38, batch: 3, training loss: 1.263205
Epoch: 38, batch: 4, training loss: 0.610661
Epoch: 38, batch: 5, training loss: 0.679574
Epoch: 38, batch: 6, training loss: 0.574137
Epoch: 38, batch: 7, training loss: 0.634819
Epoch: 38, batch: 8, training loss: 0.719039
Epoch: 38, batch: 9, training loss: 0.700248
Epoch: 38, batch: 10, training loss: 1.032014
Epoch: 38, batch: 11, training loss: 1.367757
Epoch: 38, batch: 12, training loss: 1.186672
Epoch: 38, batch: 13, training loss: 1.389098
Epoch: 38, batch: 14, training loss: 1.299872
Epoch: 38, batch: 15, training loss: 1.534288
Epoch: 38, batch: 16, training loss: 1.349245
Epoch: 38, batch: 17, training loss: 1.450511
Epoch: 38, batch: 18, training loss: 1.315079
Epoch: 38, batch: 19, training loss: 1.042698
Epoch: 38, batch: 20, training loss: 1.383700
Epoch: 38, batch: 21, training loss: 0.818770
Epoch: 38, batch: 22, training loss: 1.222267
Epoch: 38, batch: 23, training loss: 1.182080
Epoch: 38, batch: 24, training loss: 1.290962
Epoch: 38, batch: 25, training loss: 1.369818
Epoch: 38, batch: 26, training loss: 1.418111
Epoch: 38, batch: 27, training loss: 1.544978
Epoch: 38, batch: 28, training loss: 1.654804
Epoch: 38, batch: 29, training loss: 1.464659
Epoch: 38, batch: 30, training loss: 1.545337
Epoch: 38, batch: 31, training loss: 1.282262
Epoch: 38, batch: 32, training loss: 1.290091
Epoch: 38, batch: 33, training loss: 1.347577
Epoch: 38, batch: 34, training loss: 1.239435
Epoch: 38, batch: 35, training loss: 1.439531
Epoch: 38, batch: 36, training loss: 1.411425
Epoch: 38, batch: 37, training loss: 1.344853
Epoch: 38, batch: 38, training loss: 1.502267
Epoch: 38, batch: 39, training loss: 1.689306
Epoch: 38, batch: 40, training loss: 1.138993
Epoch: 38, batch: 41, training loss: 1.457379
Epoch: 38, batch: 42, training loss: 1.539502
Epoch: 38, batch: 43, training loss: 1.211983
Epoch: 38, batch: 44, training loss: 1.575002
Epoch: 38, batch: 45, training loss: 1.611538
Epoch: 38, batch: 46, training loss: 1.576857
Epoch: 38, batch: 47, training loss: 1.467237
Epoch: 38, batch: 48, training loss: 1.311439
Epoch: 38, batch: 49, training loss: 1.447484
Epoch: 39, batch: 0, training loss: 1.339887
Epoch: 39, batch: 1, training loss: 1.387111
Epoch: 39, batch: 2, training loss: 0.896714
Epoch: 39, batch: 3, training loss: 1.257391
Epoch: 39, batch: 4, training loss: 0.609628
Epoch: 39, batch: 5, training loss: 0.667295
Epoch: 39, batch: 6, training loss: 0.598004
Epoch: 39, batch: 7, training loss: 0.626091
Epoch: 39, batch: 8, training loss: 0.715920
Epoch: 39, batch: 9, training loss: 0.723024
Epoch: 39, batch: 10, training loss: 1.004410
Epoch: 39, batch: 11, training loss: 1.342644
Epoch: 39, batch: 12, training loss: 1.184324
Epoch: 39, batch: 13, training loss: 1.383982
Epoch: 39, batch: 14, training loss: 1.289342
Epoch: 39, batch: 15, training loss: 1.527553
Epoch: 39, batch: 16, training loss: 1.349778
Epoch: 39, batch: 17, training loss: 1.435853
Epoch: 39, batch: 18, training loss: 1.301692
Epoch: 39, batch: 19, training loss: 1.017788
Epoch: 39, batch: 20, training loss: 1.407983
Epoch: 39, batch: 21, training loss: 0.829341
Epoch: 39, batch: 22, training loss: 1.220910
Epoch: 39, batch: 23, training loss: 1.190632
Epoch: 39, batch: 24, training loss: 1.273489
Epoch: 39, batch: 25, training loss: 1.332148
Epoch: 39, batch: 26, training loss: 1.407842
Epoch: 39, batch: 27, training loss: 1.534909
Epoch: 39, batch: 28, training loss: 1.629678
Epoch: 39, batch: 29, training loss: 1.467799
Epoch: 39, batch: 30, training loss: 1.545792
Epoch: 39, batch: 31, training loss: 1.300085
Epoch: 39, batch: 32, training loss: 1.260981
Epoch: 39, batch: 33, training loss: 1.301484
Epoch: 39, batch: 34, training loss: 1.229668
Epoch: 39, batch: 35, training loss: 1.407578
Epoch: 39, batch: 36, training loss: 1.391888
Epoch: 39, batch: 37, training loss: 1.307793
Epoch: 39, batch: 38, training loss: 1.503587
Epoch: 39, batch: 39, training loss: 1.704488
Epoch: 39, batch: 40, training loss: 1.157918
Epoch: 39, batch: 41, training loss: 1.470729
Epoch: 39, batch: 42, training loss: 1.522657
Epoch: 39, batch: 43, training loss: 1.199626
Epoch: 39, batch: 44, training loss: 1.554862
Epoch: 39, batch: 45, training loss: 1.606773
Epoch: 39, batch: 46, training loss: 1.562345
Epoch: 39, batch: 47, training loss: 1.442864
Epoch: 39, batch: 48, training loss: 1.285703
Epoch: 39, batch: 49, training loss: 1.459956
Epoch: 40, batch: 0, training loss: 1.352788
Epoch: 40, batch: 1, training loss: 1.378037
Epoch: 40, batch: 2, training loss: 0.915093
Epoch: 40, batch: 3, training loss: 1.257717
Epoch: 40, batch: 4, training loss: 0.597221
Epoch: 40, batch: 5, training loss: 0.686069
Epoch: 40, batch: 6, training loss: 0.596683
Epoch: 40, batch: 7, training loss: 0.606271
Epoch: 40, batch: 8, training loss: 0.717120
Epoch: 40, batch: 9, training loss: 0.699849
Epoch: 40, batch: 10, training loss: 1.012929
Epoch: 40, batch: 11, training loss: 1.342631
Epoch: 40, batch: 12, training loss: 1.162932
Epoch: 40, batch: 13, training loss: 1.354764
Epoch: 40, batch: 14, training loss: 1.282472
Epoch: 40, batch: 15, training loss: 1.537293
Epoch: 40, batch: 16, training loss: 1.315771
Epoch: 40, batch: 17, training loss: 1.424630
Epoch: 40, batch: 18, training loss: 1.300838
Epoch: 40, batch: 19, training loss: 1.005376
Epoch: 40, batch: 20, training loss: 1.385817
Epoch: 40, batch: 21, training loss: 0.785181
Epoch: 40, batch: 22, training loss: 1.198070
Epoch: 40, batch: 23, training loss: 1.187607
Epoch: 40, batch: 24, training loss: 1.234409
Epoch: 40, batch: 25, training loss: 1.327868
Epoch: 40, batch: 26, training loss: 1.372052
Epoch: 40, batch: 27, training loss: 1.526252
Epoch: 40, batch: 28, training loss: 1.635243
Epoch: 40, batch: 29, training loss: 1.455585
Epoch: 40, batch: 30, training loss: 1.531987
Epoch: 40, batch: 31, training loss: 1.277555
Epoch: 40, batch: 32, training loss: 1.266630
Epoch: 40, batch: 33, training loss: 1.327693
Epoch: 40, batch: 34, training loss: 1.210091
Epoch: 40, batch: 35, training loss: 1.399035
Epoch: 40, batch: 36, training loss: 1.349034
Epoch: 40, batch: 37, training loss: 1.278142
Epoch: 40, batch: 38, training loss: 1.437649
Epoch: 40, batch: 39, training loss: 1.667013
Epoch: 40, batch: 40, training loss: 1.125630
Epoch: 40, batch: 41, training loss: 1.434365
Epoch: 40, batch: 42, training loss: 1.525403
Epoch: 40, batch: 43, training loss: 1.206179
Epoch: 40, batch: 44, training loss: 1.540799
Epoch: 40, batch: 45, training loss: 1.612931
Epoch: 40, batch: 46, training loss: 1.561313
Epoch: 40, batch: 47, training loss: 1.421899
Epoch: 40, batch: 48, training loss: 1.273049
Epoch: 40, batch: 49, training loss: 1.414951
Epoch: 41, batch: 0, training loss: 1.309707
Epoch: 41, batch: 1, training loss: 1.386558
Epoch: 41, batch: 2, training loss: 0.898004
Epoch: 41, batch: 3, training loss: 1.279689
Epoch: 41, batch: 4, training loss: 0.611128
Epoch: 41, batch: 5, training loss: 0.675878
Epoch: 41, batch: 6, training loss: 0.598844
Epoch: 41, batch: 7, training loss: 0.601830
Epoch: 41, batch: 8, training loss: 0.694729
Epoch: 41, batch: 9, training loss: 0.686161
Epoch: 41, batch: 10, training loss: 1.039360
Epoch: 41, batch: 11, training loss: 1.352032
Epoch: 41, batch: 12, training loss: 1.173853
Epoch: 41, batch: 13, training loss: 1.358703
Epoch: 41, batch: 14, training loss: 1.300375
Epoch: 41, batch: 15, training loss: 1.509968
Epoch: 41, batch: 16, training loss: 1.317318
Epoch: 41, batch: 17, training loss: 1.422310
Epoch: 41, batch: 18, training loss: 1.263101
Epoch: 41, batch: 19, training loss: 1.023414
Epoch: 41, batch: 20, training loss: 1.365734
Epoch: 41, batch: 21, training loss: 0.788265
Epoch: 41, batch: 22, training loss: 1.192641
Epoch: 41, batch: 23, training loss: 1.170896
Epoch: 41, batch: 24, training loss: 1.244922
Epoch: 41, batch: 25, training loss: 1.317050
Epoch: 41, batch: 26, training loss: 1.355285
Epoch: 41, batch: 27, training loss: 1.520279
Epoch: 41, batch: 28, training loss: 1.607729
Epoch: 41, batch: 29, training loss: 1.440801
Epoch: 41, batch: 30, training loss: 1.493896
Epoch: 41, batch: 31, training loss: 1.253173
Epoch: 41, batch: 32, training loss: 1.266971
Epoch: 41, batch: 33, training loss: 1.295848
Epoch: 41, batch: 34, training loss: 1.185046
Epoch: 41, batch: 35, training loss: 1.359465
Epoch: 41, batch: 36, training loss: 1.320451
Epoch: 41, batch: 37, training loss: 1.257735
Epoch: 41, batch: 38, training loss: 1.448256
Epoch: 41, batch: 39, training loss: 1.630818
Epoch: 41, batch: 40, training loss: 1.077249
Epoch: 41, batch: 41, training loss: 1.413971
Epoch: 41, batch: 42, training loss: 1.514731
Epoch: 41, batch: 43, training loss: 1.179274
Epoch: 41, batch: 44, training loss: 1.534981
Epoch: 41, batch: 45, training loss: 1.583861
Epoch: 41, batch: 46, training loss: 1.520736
Epoch: 41, batch: 47, training loss: 1.400313
Epoch: 41, batch: 48, training loss: 1.252069
Epoch: 41, batch: 49, training loss: 1.393850
Epoch: 42, batch: 0, training loss: 1.314309
Epoch: 42, batch: 1, training loss: 1.349819
Epoch: 42, batch: 2, training loss: 0.875209
Epoch: 42, batch: 3, training loss: 1.233568
Epoch: 42, batch: 4, training loss: 0.601463
Epoch: 42, batch: 5, training loss: 0.678365
Epoch: 42, batch: 6, training loss: 0.620579
Epoch: 42, batch: 7, training loss: 0.619961
Epoch: 42, batch: 8, training loss: 0.695353
Epoch: 42, batch: 9, training loss: 0.695852
Epoch: 42, batch: 10, training loss: 1.029251
Epoch: 42, batch: 11, training loss: 1.329669
Epoch: 42, batch: 12, training loss: 1.174082
Epoch: 42, batch: 13, training loss: 1.361613
Epoch: 42, batch: 14, training loss: 1.277897
Epoch: 42, batch: 15, training loss: 1.479736
Epoch: 42, batch: 16, training loss: 1.302037
Epoch: 42, batch: 17, training loss: 1.411656
Epoch: 42, batch: 18, training loss: 1.262365
Epoch: 42, batch: 19, training loss: 1.006871
Epoch: 42, batch: 20, training loss: 1.350297
Epoch: 42, batch: 21, training loss: 0.804234
Epoch: 42, batch: 22, training loss: 1.175681
Epoch: 42, batch: 23, training loss: 1.183533
Epoch: 42, batch: 24, training loss: 1.221919
Epoch: 42, batch: 25, training loss: 1.295933
Epoch: 42, batch: 26, training loss: 1.350280
Epoch: 42, batch: 27, training loss: 1.489468
Epoch: 42, batch: 28, training loss: 1.597010
Epoch: 42, batch: 29, training loss: 1.412697
Epoch: 42, batch: 30, training loss: 1.508012
Epoch: 42, batch: 31, training loss: 1.258778
Epoch: 42, batch: 32, training loss: 1.254279
Epoch: 42, batch: 33, training loss: 1.291256
Epoch: 42, batch: 34, training loss: 1.190728
Epoch: 42, batch: 35, training loss: 1.332100
Epoch: 42, batch: 36, training loss: 1.323090
Epoch: 42, batch: 37, training loss: 1.255128
Epoch: 42, batch: 38, training loss: 1.428434
Epoch: 42, batch: 39, training loss: 1.601477
Epoch: 42, batch: 40, training loss: 1.096001
Epoch: 42, batch: 41, training loss: 1.400245
Epoch: 42, batch: 42, training loss: 1.485552
Epoch: 42, batch: 43, training loss: 1.172643
Epoch: 42, batch: 44, training loss: 1.507422
Epoch: 42, batch: 45, training loss: 1.555347
Epoch: 42, batch: 46, training loss: 1.543557
Epoch: 42, batch: 47, training loss: 1.371953
Epoch: 42, batch: 48, training loss: 1.242140
Epoch: 42, batch: 49, training loss: 1.405623
Epoch: 43, batch: 0, training loss: 1.304223
Epoch: 43, batch: 1, training loss: 1.322896
Epoch: 43, batch: 2, training loss: 0.904546
Epoch: 43, batch: 3, training loss: 1.207158
Epoch: 43, batch: 4, training loss: 0.618155
Epoch: 43, batch: 5, training loss: 0.676844
Epoch: 43, batch: 6, training loss: 0.602541
Epoch: 43, batch: 7, training loss: 0.600388
Epoch: 43, batch: 8, training loss: 0.661458
Epoch: 43, batch: 9, training loss: 0.693861
Epoch: 43, batch: 10, training loss: 1.002656
Epoch: 43, batch: 11, training loss: 1.317079
Epoch: 43, batch: 12, training loss: 1.163430
Epoch: 43, batch: 13, training loss: 1.312707
Epoch: 43, batch: 14, training loss: 1.277648
Epoch: 43, batch: 15, training loss: 1.488389
Epoch: 43, batch: 16, training loss: 1.279695
Epoch: 43, batch: 17, training loss: 1.383713
Epoch: 43, batch: 18, training loss: 1.268797
Epoch: 43, batch: 19, training loss: 1.014584
Epoch: 43, batch: 20, training loss: 1.350456
Epoch: 43, batch: 21, training loss: 0.783912
Epoch: 43, batch: 22, training loss: 1.155792
Epoch: 43, batch: 23, training loss: 1.139062
Epoch: 43, batch: 24, training loss: 1.246514
Epoch: 43, batch: 25, training loss: 1.323020
Epoch: 43, batch: 26, training loss: 1.326462
Epoch: 43, batch: 27, training loss: 1.503390
Epoch: 43, batch: 28, training loss: 1.574110
Epoch: 43, batch: 29, training loss: 1.405358
Epoch: 43, batch: 30, training loss: 1.491834
Epoch: 43, batch: 31, training loss: 1.259059
Epoch: 43, batch: 32, training loss: 1.235765
Epoch: 43, batch: 33, training loss: 1.263192
Epoch: 43, batch: 34, training loss: 1.181452
Epoch: 43, batch: 35, training loss: 1.354908
Epoch: 43, batch: 36, training loss: 1.317569
Epoch: 43, batch: 37, training loss: 1.255173
Epoch: 43, batch: 38, training loss: 1.415847
Epoch: 43, batch: 39, training loss: 1.595279
Epoch: 43, batch: 40, training loss: 1.093894
Epoch: 43, batch: 41, training loss: 1.403994
Epoch: 43, batch: 42, training loss: 1.481923
Epoch: 43, batch: 43, training loss: 1.169344
Epoch: 43, batch: 44, training loss: 1.520969
Epoch: 43, batch: 45, training loss: 1.571165
Epoch: 43, batch: 46, training loss: 1.522643
Epoch: 43, batch: 47, training loss: 1.404780
Epoch: 43, batch: 48, training loss: 1.248087
Epoch: 43, batch: 49, training loss: 1.386002
Epoch: 44, batch: 0, training loss: 1.292652
Epoch: 44, batch: 1, training loss: 1.342781
Epoch: 44, batch: 2, training loss: 0.883148
Epoch: 44, batch: 3, training loss: 1.218095
Epoch: 44, batch: 4, training loss: 0.605667
Epoch: 44, batch: 5, training loss: 0.692628
Epoch: 44, batch: 6, training loss: 0.569584
Epoch: 44, batch: 7, training loss: 0.615779
Epoch: 44, batch: 8, training loss: 0.683950
Epoch: 44, batch: 9, training loss: 0.711769
Epoch: 44, batch: 10, training loss: 0.991735
Epoch: 44, batch: 11, training loss: 1.315758
Epoch: 44, batch: 12, training loss: 1.173826
Epoch: 44, batch: 13, training loss: 1.337894
Epoch: 44, batch: 14, training loss: 1.258287
Epoch: 44, batch: 15, training loss: 1.480337
Epoch: 44, batch: 16, training loss: 1.284629
Epoch: 44, batch: 17, training loss: 1.392835
Epoch: 44, batch: 18, training loss: 1.235230
Epoch: 44, batch: 19, training loss: 0.998117
Epoch: 44, batch: 20, training loss: 1.356150
Epoch: 44, batch: 21, training loss: 0.794556
Epoch: 44, batch: 22, training loss: 1.177530
Epoch: 44, batch: 23, training loss: 1.147355
Epoch: 44, batch: 24, training loss: 1.216471
Epoch: 44, batch: 25, training loss: 1.313244
Epoch: 44, batch: 26, training loss: 1.343977
Epoch: 44, batch: 27, training loss: 1.468970
Epoch: 44, batch: 28, training loss: 1.552756
Epoch: 44, batch: 29, training loss: 1.399034
Epoch: 44, batch: 30, training loss: 1.481677
Epoch: 44, batch: 31, training loss: 1.233296
Epoch: 44, batch: 32, training loss: 1.248972
Epoch: 44, batch: 33, training loss: 1.298005
Epoch: 44, batch: 34, training loss: 1.164080
Epoch: 44, batch: 35, training loss: 1.370320
Epoch: 44, batch: 36, training loss: 1.328032
Epoch: 44, batch: 37, training loss: 1.257267
Epoch: 44, batch: 38, training loss: 1.434752
Epoch: 44, batch: 39, training loss: 1.604565
Epoch: 44, batch: 40, training loss: 1.094054
Epoch: 44, batch: 41, training loss: 1.393561
Epoch: 44, batch: 42, training loss: 1.459650
Epoch: 44, batch: 43, training loss: 1.165218
Epoch: 44, batch: 44, training loss: 1.507978
Epoch: 44, batch: 45, training loss: 1.555845
Epoch: 44, batch: 46, training loss: 1.519515
Epoch: 44, batch: 47, training loss: 1.380521
Epoch: 44, batch: 48, training loss: 1.252881
Epoch: 44, batch: 49, training loss: 1.406937
Epoch: 45, batch: 0, training loss: 1.298715
Epoch: 45, batch: 1, training loss: 1.345108
Epoch: 45, batch: 2, training loss: 0.879938
Epoch: 45, batch: 3, training loss: 1.221472
Epoch: 45, batch: 4, training loss: 0.596812
Epoch: 45, batch: 5, training loss: 0.686096
Epoch: 45, batch: 6, training loss: 0.598093
Epoch: 45, batch: 7, training loss: 0.617579
Epoch: 45, batch: 8, training loss: 0.704606
Epoch: 45, batch: 9, training loss: 0.715511
Epoch: 45, batch: 10, training loss: 0.991341
Epoch: 45, batch: 11, training loss: 1.328886
Epoch: 45, batch: 12, training loss: 1.148765
Epoch: 45, batch: 13, training loss: 1.319058
Epoch: 45, batch: 14, training loss: 1.263116
Epoch: 45, batch: 15, training loss: 1.473517
Epoch: 45, batch: 16, training loss: 1.288536
Epoch: 45, batch: 17, training loss: 1.387839
Epoch: 45, batch: 18, training loss: 1.247812
Epoch: 45, batch: 19, training loss: 0.986231
Epoch: 45, batch: 20, training loss: 1.306473
Epoch: 45, batch: 21, training loss: 0.810675
Epoch: 45, batch: 22, training loss: 1.161054
Epoch: 45, batch: 23, training loss: 1.146276
Epoch: 45, batch: 24, training loss: 1.258306
Epoch: 45, batch: 25, training loss: 1.287568
Epoch: 45, batch: 26, training loss: 1.343897
Epoch: 45, batch: 27, training loss: 1.470414
Epoch: 45, batch: 28, training loss: 1.525419
Epoch: 45, batch: 29, training loss: 1.374719
Epoch: 45, batch: 30, training loss: 1.479522
Epoch: 45, batch: 31, training loss: 1.223682
Epoch: 45, batch: 32, training loss: 1.234296
Epoch: 45, batch: 33, training loss: 1.260472
Epoch: 45, batch: 34, training loss: 1.167460
Epoch: 45, batch: 35, training loss: 1.355028
Epoch: 45, batch: 36, training loss: 1.303737
Epoch: 45, batch: 37, training loss: 1.223289
Epoch: 45, batch: 38, training loss: 1.416714
Epoch: 45, batch: 39, training loss: 1.612908
Epoch: 45, batch: 40, training loss: 1.059660
Epoch: 45, batch: 41, training loss: 1.364192
Epoch: 45, batch: 42, training loss: 1.455151
Epoch: 45, batch: 43, training loss: 1.145392
Epoch: 45, batch: 44, training loss: 1.478779
Epoch: 45, batch: 45, training loss: 1.526612
Epoch: 45, batch: 46, training loss: 1.516619
Epoch: 45, batch: 47, training loss: 1.384512
Epoch: 45, batch: 48, training loss: 1.255947
Epoch: 45, batch: 49, training loss: 1.368354
Epoch: 46, batch: 0, training loss: 1.292691
Epoch: 46, batch: 1, training loss: 1.332808
Epoch: 46, batch: 2, training loss: 0.866268
Epoch: 46, batch: 3, training loss: 1.222898
Epoch: 46, batch: 4, training loss: 0.591479
Epoch: 46, batch: 5, training loss: 0.657904
Epoch: 46, batch: 6, training loss: 0.608844
Epoch: 46, batch: 7, training loss: 0.600464
Epoch: 46, batch: 8, training loss: 0.694897
Epoch: 46, batch: 9, training loss: 0.689542
Epoch: 46, batch: 10, training loss: 0.992734
Epoch: 46, batch: 11, training loss: 1.327343
Epoch: 46, batch: 12, training loss: 1.130577
Epoch: 46, batch: 13, training loss: 1.304692
Epoch: 46, batch: 14, training loss: 1.254263
Epoch: 46, batch: 15, training loss: 1.440326
Epoch: 46, batch: 16, training loss: 1.279134
Epoch: 46, batch: 17, training loss: 1.364066
Epoch: 46, batch: 18, training loss: 1.243303
Epoch: 46, batch: 19, training loss: 0.995223
Epoch: 46, batch: 20, training loss: 1.303729
Epoch: 46, batch: 21, training loss: 0.769705
Epoch: 46, batch: 22, training loss: 1.144804
Epoch: 46, batch: 23, training loss: 1.147134
Epoch: 46, batch: 24, training loss: 1.203960
Epoch: 46, batch: 25, training loss: 1.268467
Epoch: 46, batch: 26, training loss: 1.321177
Epoch: 46, batch: 27, training loss: 1.445145
Epoch: 46, batch: 28, training loss: 1.553858
Epoch: 46, batch: 29, training loss: 1.388149
Epoch: 46, batch: 30, training loss: 1.452969
Epoch: 46, batch: 31, training loss: 1.215342
Epoch: 46, batch: 32, training loss: 1.219978
Epoch: 46, batch: 33, training loss: 1.229909
Epoch: 46, batch: 34, training loss: 1.144160
Epoch: 46, batch: 35, training loss: 1.336103
Epoch: 46, batch: 36, training loss: 1.271725
Epoch: 46, batch: 37, training loss: 1.202385
Epoch: 46, batch: 38, training loss: 1.386260
Epoch: 46, batch: 39, training loss: 1.601655
Epoch: 46, batch: 40, training loss: 1.037363
Epoch: 46, batch: 41, training loss: 1.330856
Epoch: 46, batch: 42, training loss: 1.429967
Epoch: 46, batch: 43, training loss: 1.158028
Epoch: 46, batch: 44, training loss: 1.443498
Epoch: 46, batch: 45, training loss: 1.501505
Epoch: 46, batch: 46, training loss: 1.468681
Epoch: 46, batch: 47, training loss: 1.361331
Epoch: 46, batch: 48, training loss: 1.248133
Epoch: 46, batch: 49, training loss: 1.380380
Epoch: 47, batch: 0, training loss: 1.297735
Epoch: 47, batch: 1, training loss: 1.316219
Epoch: 47, batch: 2, training loss: 0.860601
Epoch: 47, batch: 3, training loss: 1.208103
Epoch: 47, batch: 4, training loss: 0.584220
Epoch: 47, batch: 5, training loss: 0.638531
Epoch: 47, batch: 6, training loss: 0.594845
Epoch: 47, batch: 7, training loss: 0.603564
Epoch: 47, batch: 8, training loss: 0.686979
Epoch: 47, batch: 9, training loss: 0.657016
Epoch: 47, batch: 10, training loss: 0.996470
Epoch: 47, batch: 11, training loss: 1.309274
Epoch: 47, batch: 12, training loss: 1.130224
Epoch: 47, batch: 13, training loss: 1.301955
Epoch: 47, batch: 14, training loss: 1.223897
Epoch: 47, batch: 15, training loss: 1.447030
Epoch: 47, batch: 16, training loss: 1.277842
Epoch: 47, batch: 17, training loss: 1.379910
Epoch: 47, batch: 18, training loss: 1.246602
Epoch: 47, batch: 19, training loss: 0.975191
Epoch: 47, batch: 20, training loss: 1.323260
Epoch: 47, batch: 21, training loss: 0.771073
Epoch: 47, batch: 22, training loss: 1.146947
Epoch: 47, batch: 23, training loss: 1.119850
Epoch: 47, batch: 24, training loss: 1.203370
Epoch: 47, batch: 25, training loss: 1.280169
Epoch: 47, batch: 26, training loss: 1.315891
Epoch: 47, batch: 27, training loss: 1.440528
Epoch: 47, batch: 28, training loss: 1.514694
Epoch: 47, batch: 29, training loss: 1.364467
Epoch: 47, batch: 30, training loss: 1.452931
Epoch: 47, batch: 31, training loss: 1.201929
Epoch: 47, batch: 32, training loss: 1.198880
Epoch: 47, batch: 33, training loss: 1.205769
Epoch: 47, batch: 34, training loss: 1.131978
Epoch: 47, batch: 35, training loss: 1.343000
Epoch: 47, batch: 36, training loss: 1.273198
Epoch: 47, batch: 37, training loss: 1.228766
Epoch: 47, batch: 38, training loss: 1.379927
Epoch: 47, batch: 39, training loss: 1.565409
Epoch: 47, batch: 40, training loss: 1.033355
Epoch: 47, batch: 41, training loss: 1.344161
Epoch: 47, batch: 42, training loss: 1.430887
Epoch: 47, batch: 43, training loss: 1.121883
Epoch: 47, batch: 44, training loss: 1.449886
Epoch: 47, batch: 45, training loss: 1.503771
Epoch: 47, batch: 46, training loss: 1.450043
Epoch: 47, batch: 47, training loss: 1.351525
Epoch: 47, batch: 48, training loss: 1.209694
Epoch: 47, batch: 49, training loss: 1.337371
Epoch: 48, batch: 0, training loss: 1.281290
Epoch: 48, batch: 1, training loss: 1.291656
Epoch: 48, batch: 2, training loss: 0.872274
Epoch: 48, batch: 3, training loss: 1.192051
Epoch: 48, batch: 4, training loss: 0.576387
Epoch: 48, batch: 5, training loss: 0.650413
Epoch: 48, batch: 6, training loss: 0.566411
Epoch: 48, batch: 7, training loss: 0.604342
Epoch: 48, batch: 8, training loss: 0.672098
Epoch: 48, batch: 9, training loss: 0.660235
Epoch: 48, batch: 10, training loss: 0.993061
Epoch: 48, batch: 11, training loss: 1.294385
Epoch: 48, batch: 12, training loss: 1.107793
Epoch: 48, batch: 13, training loss: 1.277683
Epoch: 48, batch: 14, training loss: 1.256173
Epoch: 48, batch: 15, training loss: 1.405603
Epoch: 48, batch: 16, training loss: 1.261557
Epoch: 48, batch: 17, training loss: 1.358613
Epoch: 48, batch: 18, training loss: 1.217293
Epoch: 48, batch: 19, training loss: 0.977217
Epoch: 48, batch: 20, training loss: 1.314229
Epoch: 48, batch: 21, training loss: 0.779794
Epoch: 48, batch: 22, training loss: 1.128439
Epoch: 48, batch: 23, training loss: 1.106027
Epoch: 48, batch: 24, training loss: 1.183930
Epoch: 48, batch: 25, training loss: 1.254032
Epoch: 48, batch: 26, training loss: 1.304667
Epoch: 48, batch: 27, training loss: 1.427564
Epoch: 48, batch: 28, training loss: 1.515398
Epoch: 48, batch: 29, training loss: 1.329803
Epoch: 48, batch: 30, training loss: 1.430008
Epoch: 48, batch: 31, training loss: 1.197748
Epoch: 48, batch: 32, training loss: 1.183749
Epoch: 48, batch: 33, training loss: 1.203237
Epoch: 48, batch: 34, training loss: 1.115589
Epoch: 48, batch: 35, training loss: 1.337150
Epoch: 48, batch: 36, training loss: 1.266856
Epoch: 48, batch: 37, training loss: 1.209404
Epoch: 48, batch: 38, training loss: 1.365685
Epoch: 48, batch: 39, training loss: 1.543064
Epoch: 48, batch: 40, training loss: 1.022755
Epoch: 48, batch: 41, training loss: 1.352918
Epoch: 48, batch: 42, training loss: 1.395867
Epoch: 48, batch: 43, training loss: 1.106106
Epoch: 48, batch: 44, training loss: 1.429484
Epoch: 48, batch: 45, training loss: 1.473122
Epoch: 48, batch: 46, training loss: 1.459928
Epoch: 48, batch: 47, training loss: 1.328775
Epoch: 48, batch: 48, training loss: 1.214161
Epoch: 48, batch: 49, training loss: 1.309662
Epoch: 49, batch: 0, training loss: 1.239146
Epoch: 49, batch: 1, training loss: 1.296893
Epoch: 49, batch: 2, training loss: 0.837935
Epoch: 49, batch: 3, training loss: 1.174566
Epoch: 49, batch: 4, training loss: 0.559837
Epoch: 49, batch: 5, training loss: 0.652446
Epoch: 49, batch: 6, training loss: 0.572614
Epoch: 49, batch: 7, training loss: 0.590271
Epoch: 49, batch: 8, training loss: 0.650083
Epoch: 49, batch: 9, training loss: 0.665261
Epoch: 49, batch: 10, training loss: 0.962993
Epoch: 49, batch: 11, training loss: 1.285495
Epoch: 49, batch: 12, training loss: 1.107086
Epoch: 49, batch: 13, training loss: 1.287705
Epoch: 49, batch: 14, training loss: 1.214968
Epoch: 49, batch: 15, training loss: 1.437628
Epoch: 49, batch: 16, training loss: 1.266073
Epoch: 49, batch: 17, training loss: 1.338289
Epoch: 49, batch: 18, training loss: 1.211430
Epoch: 49, batch: 19, training loss: 0.968718
Epoch: 49, batch: 20, training loss: 1.299971
Epoch: 49, batch: 21, training loss: 0.768026
Epoch: 49, batch: 22, training loss: 1.134853
Epoch: 49, batch: 23, training loss: 1.118811
Epoch: 49, batch: 24, training loss: 1.179664
Epoch: 49, batch: 25, training loss: 1.230426
Epoch: 49, batch: 26, training loss: 1.291231
Epoch: 49, batch: 27, training loss: 1.414742
Epoch: 49, batch: 28, training loss: 1.476086
Epoch: 49, batch: 29, training loss: 1.314925
Epoch: 49, batch: 30, training loss: 1.431369
Epoch: 49, batch: 31, training loss: 1.195218
Epoch: 49, batch: 32, training loss: 1.175087
Epoch: 49, batch: 33, training loss: 1.214903
Epoch: 49, batch: 34, training loss: 1.105926
Epoch: 49, batch: 35, training loss: 1.298753
Epoch: 49, batch: 36, training loss: 1.255806
Epoch: 49, batch: 37, training loss: 1.197956
Epoch: 49, batch: 38, training loss: 1.338146
Epoch: 49, batch: 39, training loss: 1.544269
Epoch: 49, batch: 40, training loss: 1.017278
Epoch: 49, batch: 41, training loss: 1.323356
Epoch: 49, batch: 42, training loss: 1.374414
Epoch: 49, batch: 43, training loss: 1.131471
Epoch: 49, batch: 44, training loss: 1.438740
Epoch: 49, batch: 45, training loss: 1.493655
Epoch: 49, batch: 46, training loss: 1.422051
Epoch: 49, batch: 47, training loss: 1.310199
Epoch: 49, batch: 48, training loss: 1.208476
Epoch: 49, batch: 49, training loss: 1.319425
Epoch: 0, batch: 0, training loss: 8.916346
Epoch: 0, batch: 1, training loss: 6.678258
Epoch: 0, batch: 2, training loss: 5.152746
Epoch: 0, batch: 3, training loss: 5.818608
Epoch: 0, batch: 4, training loss: 4.963741
Epoch: 0, batch: 5, training loss: 4.905105
Epoch: 0, batch: 6, training loss: 6.562762
Epoch: 0, batch: 7, training loss: 4.894782
Epoch: 0, batch: 8, training loss: 4.162481
Epoch: 0, batch: 9, training loss: 5.621347
Epoch: 0, batch: 10, training loss: 4.569247
Epoch: 0, batch: 11, training loss: 5.015949
Epoch: 0, batch: 12, training loss: 4.423619
Epoch: 0, batch: 13, training loss: 5.048013
Epoch: 0, batch: 14, training loss: 4.134705
Epoch: 0, batch: 15, training loss: 4.930650
Epoch: 0, batch: 16, training loss: 4.377088
Epoch: 0, batch: 17, training loss: 4.183740
Epoch: 0, batch: 18, training loss: 3.748383
Epoch: 0, batch: 19, training loss: 3.130714
Epoch: 0, batch: 20, training loss: 4.136881
Epoch: 0, batch: 21, training loss: 3.855053
Epoch: 0, batch: 22, training loss: 4.562070
Epoch: 0, batch: 23, training loss: 4.006790
Epoch: 0, batch: 24, training loss: 3.938640
Epoch: 0, batch: 25, training loss: 4.591513
Epoch: 0, batch: 26, training loss: 4.407770
Epoch: 0, batch: 27, training loss: 4.370998
Epoch: 0, batch: 28, training loss: 4.805436
Epoch: 0, batch: 29, training loss: 4.646917
Epoch: 0, batch: 30, training loss: 4.748664
Epoch: 0, batch: 31, training loss: 4.038754
Epoch: 0, batch: 32, training loss: 3.648718
Epoch: 0, batch: 33, training loss: 6.229321
Epoch: 0, batch: 34, training loss: 6.252075
Epoch: 0, batch: 35, training loss: 4.428892
Epoch: 0, batch: 36, training loss: 4.375710
Epoch: 0, batch: 37, training loss: 4.394251
Epoch: 0, batch: 38, training loss: 4.353286
Epoch: 0, batch: 39, training loss: 5.030621
Epoch: 0, batch: 40, training loss: 3.949427
Epoch: 0, batch: 41, training loss: 4.213121
Epoch: 0, batch: 42, training loss: 4.322630
Epoch: 0, batch: 43, training loss: 4.002194
Epoch: 0, batch: 44, training loss: 4.558324
Epoch: 0, batch: 45, training loss: 4.366086
Epoch: 0, batch: 46, training loss: 4.206447
Epoch: 0, batch: 47, training loss: 4.401137
Epoch: 0, batch: 48, training loss: 4.144826
Epoch: 0, batch: 49, training loss: 4.447555
Epoch: 1, batch: 0, training loss: 4.069503
Epoch: 1, batch: 1, training loss: 4.600109
Epoch: 1, batch: 2, training loss: 3.823952
Epoch: 1, batch: 3, training loss: 4.486935
Epoch: 1, batch: 4, training loss: 3.830950
Epoch: 1, batch: 5, training loss: 3.899027
Epoch: 1, batch: 6, training loss: 5.258312
Epoch: 1, batch: 7, training loss: 3.944104
Epoch: 1, batch: 8, training loss: 3.417299
Epoch: 1, batch: 9, training loss: 4.728378
Epoch: 1, batch: 10, training loss: 3.820656
Epoch: 1, batch: 11, training loss: 4.234314
Epoch: 1, batch: 12, training loss: 3.748612
Epoch: 1, batch: 13, training loss: 4.379282
Epoch: 1, batch: 14, training loss: 3.552313
Epoch: 1, batch: 15, training loss: 4.407654
Epoch: 1, batch: 16, training loss: 3.947222
Epoch: 1, batch: 17, training loss: 3.785346
Epoch: 1, batch: 18, training loss: 3.382078
Epoch: 1, batch: 19, training loss: 2.802820
Epoch: 1, batch: 20, training loss: 3.808021
Epoch: 1, batch: 21, training loss: 3.565569
Epoch: 1, batch: 22, training loss: 4.196496
Epoch: 1, batch: 23, training loss: 3.703142
Epoch: 1, batch: 24, training loss: 3.657499
Epoch: 1, batch: 25, training loss: 4.260829
Epoch: 1, batch: 26, training loss: 4.139172
Epoch: 1, batch: 27, training loss: 4.092525
Epoch: 1, batch: 28, training loss: 4.501412
Epoch: 1, batch: 29, training loss: 4.392303
Epoch: 1, batch: 30, training loss: 4.491961
Epoch: 1, batch: 31, training loss: 3.842031
Epoch: 1, batch: 32, training loss: 3.470442
Epoch: 1, batch: 33, training loss: 5.848236
Epoch: 1, batch: 34, training loss: 5.906481
Epoch: 1, batch: 35, training loss: 4.236708
Epoch: 1, batch: 36, training loss: 4.208145
Epoch: 1, batch: 37, training loss: 4.256265
Epoch: 1, batch: 38, training loss: 4.177141
Epoch: 1, batch: 39, training loss: 4.833857
Epoch: 1, batch: 40, training loss: 3.801962
Epoch: 1, batch: 41, training loss: 4.072326
Epoch: 1, batch: 42, training loss: 4.185043
Epoch: 1, batch: 43, training loss: 3.900798
Epoch: 1, batch: 44, training loss: 4.405353
Epoch: 1, batch: 45, training loss: 4.196352
Epoch: 1, batch: 46, training loss: 4.099833
Epoch: 1, batch: 47, training loss: 4.300725
Epoch: 1, batch: 48, training loss: 4.009034
Epoch: 1, batch: 49, training loss: 4.302631
Epoch: 2, batch: 0, training loss: 3.932666
Epoch: 2, batch: 1, training loss: 4.487871
Epoch: 2, batch: 2, training loss: 3.703750
Epoch: 2, batch: 3, training loss: 4.269543
Epoch: 2, batch: 4, training loss: 3.732751
Epoch: 2, batch: 5, training loss: 3.763979
Epoch: 2, batch: 6, training loss: 5.006793
Epoch: 2, batch: 7, training loss: 3.774848
Epoch: 2, batch: 8, training loss: 3.293099
Epoch: 2, batch: 9, training loss: 4.583778
Epoch: 2, batch: 10, training loss: 3.701594
Epoch: 2, batch: 11, training loss: 4.084303
Epoch: 2, batch: 12, training loss: 3.624127
Epoch: 2, batch: 13, training loss: 4.299015
Epoch: 2, batch: 14, training loss: 3.480143
Epoch: 2, batch: 15, training loss: 4.363801
Epoch: 2, batch: 16, training loss: 3.872669
Epoch: 2, batch: 17, training loss: 3.701589
Epoch: 2, batch: 18, training loss: 3.281905
Epoch: 2, batch: 19, training loss: 2.740424
Epoch: 2, batch: 20, training loss: 3.784490
Epoch: 2, batch: 21, training loss: 3.487301
Epoch: 2, batch: 22, training loss: 4.101618
Epoch: 2, batch: 23, training loss: 3.624281
Epoch: 2, batch: 24, training loss: 3.571737
Epoch: 2, batch: 25, training loss: 4.159310
Epoch: 2, batch: 26, training loss: 4.041346
Epoch: 2, batch: 27, training loss: 4.022061
Epoch: 2, batch: 28, training loss: 4.431036
Epoch: 2, batch: 29, training loss: 4.307769
Epoch: 2, batch: 30, training loss: 4.389353
Epoch: 2, batch: 31, training loss: 3.760560
Epoch: 2, batch: 32, training loss: 3.395206
Epoch: 2, batch: 33, training loss: 5.732962
Epoch: 2, batch: 34, training loss: 5.781012
Epoch: 2, batch: 35, training loss: 4.145247
Epoch: 2, batch: 36, training loss: 4.130424
Epoch: 2, batch: 37, training loss: 4.167811
Epoch: 2, batch: 38, training loss: 4.090154
Epoch: 2, batch: 39, training loss: 4.781694
Epoch: 2, batch: 40, training loss: 3.719743
Epoch: 2, batch: 41, training loss: 3.957070
Epoch: 2, batch: 42, training loss: 4.095099
Epoch: 2, batch: 43, training loss: 3.829299
Epoch: 2, batch: 44, training loss: 4.352763
Epoch: 2, batch: 45, training loss: 4.114769
Epoch: 2, batch: 46, training loss: 4.009958
Epoch: 2, batch: 47, training loss: 4.185009
Epoch: 2, batch: 48, training loss: 3.914068
Epoch: 2, batch: 49, training loss: 4.217967
Epoch: 3, batch: 0, training loss: 3.858111
Epoch: 3, batch: 1, training loss: 4.395483
Epoch: 3, batch: 2, training loss: 3.599296
Epoch: 3, batch: 3, training loss: 4.207002
Epoch: 3, batch: 4, training loss: 3.608707
Epoch: 3, batch: 5, training loss: 3.671673
Epoch: 3, batch: 6, training loss: 4.867150
Epoch: 3, batch: 7, training loss: 3.659309
Epoch: 3, batch: 8, training loss: 3.197916
Epoch: 3, batch: 9, training loss: 4.425648
Epoch: 3, batch: 10, training loss: 3.584298
Epoch: 3, batch: 11, training loss: 3.974515
Epoch: 3, batch: 12, training loss: 3.548559
Epoch: 3, batch: 13, training loss: 4.159755
Epoch: 3, batch: 14, training loss: 3.421659
Epoch: 3, batch: 15, training loss: 4.264819
Epoch: 3, batch: 16, training loss: 3.779488
Epoch: 3, batch: 17, training loss: 3.618368
Epoch: 3, batch: 18, training loss: 3.222286
Epoch: 3, batch: 19, training loss: 2.662796
Epoch: 3, batch: 20, training loss: 3.678266
Epoch: 3, batch: 21, training loss: 3.408281
Epoch: 3, batch: 22, training loss: 4.013462
Epoch: 3, batch: 23, training loss: 3.559246
Epoch: 3, batch: 24, training loss: 3.487286
