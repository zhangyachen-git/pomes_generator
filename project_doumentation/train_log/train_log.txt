Epoch: 0, batch: 0, training loss: 9.179437
Epoch: 0, batch: 1, training loss: 6.667832
Epoch: 0, batch: 2, training loss: 5.097968
Epoch: 0, batch: 3, training loss: 5.947628
Epoch: 0, batch: 4, training loss: 5.013000
Epoch: 0, batch: 5, training loss: 5.092757
Epoch: 0, batch: 6, training loss: 6.682013
Epoch: 0, batch: 7, training loss: 5.021339
Epoch: 0, batch: 8, training loss: 4.264718
Epoch: 0, batch: 9, training loss: 5.712297
Epoch: 0, batch: 10, training loss: 4.641275
Epoch: 0, batch: 11, training loss: 5.094928
Epoch: 0, batch: 12, training loss: 4.447609
Epoch: 0, batch: 13, training loss: 5.076501
Epoch: 0, batch: 14, training loss: 4.147414
Epoch: 0, batch: 15, training loss: 4.958369
Epoch: 0, batch: 16, training loss: 4.375253
Epoch: 0, batch: 17, training loss: 4.204450
Epoch: 0, batch: 18, training loss: 3.760474
Epoch: 0, batch: 19, training loss: 3.132317
Epoch: 0, batch: 20, training loss: 4.138125
Epoch: 0, batch: 21, training loss: 3.878255
Epoch: 0, batch: 22, training loss: 4.585502
Epoch: 0, batch: 23, training loss: 4.013425
Epoch: 0, batch: 24, training loss: 3.955024
Epoch: 0, batch: 25, training loss: 4.596204
Epoch: 0, batch: 26, training loss: 4.427351
Epoch: 0, batch: 27, training loss: 4.399773
Epoch: 0, batch: 28, training loss: 4.819815
Epoch: 0, batch: 29, training loss: 4.668254
Epoch: 0, batch: 30, training loss: 4.783199
Epoch: 0, batch: 31, training loss: 4.042268
Epoch: 0, batch: 32, training loss: 3.657958
Epoch: 0, batch: 33, training loss: 6.208767
Epoch: 0, batch: 34, training loss: 6.247618
Epoch: 0, batch: 35, training loss: 4.459776
Epoch: 0, batch: 36, training loss: 4.381458
Epoch: 0, batch: 37, training loss: 4.403495
Epoch: 0, batch: 38, training loss: 4.371823
Epoch: 0, batch: 39, training loss: 5.040662
Epoch: 0, batch: 40, training loss: 3.935981
Epoch: 0, batch: 41, training loss: 4.212861
Epoch: 0, batch: 42, training loss: 4.340208
Epoch: 0, batch: 43, training loss: 4.026345
Epoch: 0, batch: 44, training loss: 4.562144
Epoch: 0, batch: 45, training loss: 4.364848
Epoch: 0, batch: 46, training loss: 4.212824
Epoch: 0, batch: 47, training loss: 4.388793
Epoch: 0, batch: 48, training loss: 4.144102
Epoch: 0, batch: 49, training loss: 4.460621
Epoch: 1, batch: 0, training loss: 4.077067
Epoch: 1, batch: 1, training loss: 4.621207
Epoch: 1, batch: 2, training loss: 3.827538
Epoch: 1, batch: 3, training loss: 4.470389
Epoch: 1, batch: 4, training loss: 3.861730
Epoch: 1, batch: 5, training loss: 3.900419
Epoch: 1, batch: 6, training loss: 5.272670
Epoch: 1, batch: 7, training loss: 3.968151
Epoch: 1, batch: 8, training loss: 3.431294
Epoch: 1, batch: 9, training loss: 4.756869
Epoch: 1, batch: 10, training loss: 3.840354
Epoch: 1, batch: 11, training loss: 4.217216
Epoch: 1, batch: 12, training loss: 3.723995
Epoch: 1, batch: 13, training loss: 4.369138
Epoch: 1, batch: 14, training loss: 3.550441
Epoch: 1, batch: 15, training loss: 4.387502
Epoch: 1, batch: 16, training loss: 3.944804
Epoch: 1, batch: 17, training loss: 3.795605
Epoch: 1, batch: 18, training loss: 3.385134
Epoch: 1, batch: 19, training loss: 2.807823
Epoch: 1, batch: 20, training loss: 3.796024
Epoch: 1, batch: 21, training loss: 3.565600
Epoch: 1, batch: 22, training loss: 4.201769
Epoch: 1, batch: 23, training loss: 3.693489
Epoch: 1, batch: 24, training loss: 3.658105
Epoch: 1, batch: 25, training loss: 4.268663
Epoch: 1, batch: 26, training loss: 4.134605
Epoch: 1, batch: 27, training loss: 4.105364
Epoch: 1, batch: 28, training loss: 4.516326
Epoch: 1, batch: 29, training loss: 4.395786
Epoch: 1, batch: 30, training loss: 4.618254
Epoch: 1, batch: 31, training loss: 3.853970
Epoch: 1, batch: 32, training loss: 3.499741
Epoch: 1, batch: 33, training loss: 5.928493
Epoch: 1, batch: 34, training loss: 5.976640
Epoch: 1, batch: 35, training loss: 4.281168
Epoch: 1, batch: 36, training loss: 4.253882
Epoch: 1, batch: 37, training loss: 4.347159
Epoch: 1, batch: 38, training loss: 4.229986
Epoch: 1, batch: 39, training loss: 4.890968
Epoch: 1, batch: 40, training loss: 3.828812
Epoch: 1, batch: 41, training loss: 4.117656
Epoch: 1, batch: 42, training loss: 4.226888
Epoch: 1, batch: 43, training loss: 3.934778
Epoch: 1, batch: 44, training loss: 4.437395
Epoch: 1, batch: 45, training loss: 4.248775
Epoch: 1, batch: 46, training loss: 4.143114
Epoch: 1, batch: 47, training loss: 4.353940
Epoch: 1, batch: 48, training loss: 4.050517
Epoch: 1, batch: 49, training loss: 4.344856
Epoch: 2, batch: 0, training loss: 3.992228
Epoch: 2, batch: 1, training loss: 4.555786
Epoch: 2, batch: 2, training loss: 3.743723
Epoch: 2, batch: 3, training loss: 4.303559
Epoch: 2, batch: 4, training loss: 3.775674
Epoch: 2, batch: 5, training loss: 3.801450
Epoch: 2, batch: 6, training loss: 5.112886
Epoch: 2, batch: 7, training loss: 3.863609
Epoch: 2, batch: 8, training loss: 3.343668
Epoch: 2, batch: 9, training loss: 4.675561
Epoch: 2, batch: 10, training loss: 3.776724
Epoch: 2, batch: 11, training loss: 4.145419
Epoch: 2, batch: 12, training loss: 3.665263
Epoch: 2, batch: 13, training loss: 4.324481
Epoch: 2, batch: 14, training loss: 3.466207
Epoch: 2, batch: 15, training loss: 4.316653
Epoch: 2, batch: 16, training loss: 3.881660
Epoch: 2, batch: 17, training loss: 3.737870
Epoch: 2, batch: 18, training loss: 3.345353
Epoch: 2, batch: 19, training loss: 2.779392
Epoch: 2, batch: 20, training loss: 3.774168
Epoch: 2, batch: 21, training loss: 3.544644
Epoch: 2, batch: 22, training loss: 4.126196
Epoch: 2, batch: 23, training loss: 3.616430
Epoch: 2, batch: 24, training loss: 3.569990
Epoch: 2, batch: 25, training loss: 4.173615
Epoch: 2, batch: 26, training loss: 4.067800
Epoch: 2, batch: 27, training loss: 4.066030
Epoch: 2, batch: 28, training loss: 4.467981
Epoch: 2, batch: 29, training loss: 4.350339
Epoch: 2, batch: 30, training loss: 4.436525
Epoch: 2, batch: 31, training loss: 3.781483
Epoch: 2, batch: 32, training loss: 3.413456
Epoch: 2, batch: 33, training loss: 5.799255
Epoch: 2, batch: 34, training loss: 5.881804
Epoch: 2, batch: 35, training loss: 4.168277
Epoch: 2, batch: 36, training loss: 4.116474
Epoch: 2, batch: 37, training loss: 4.162748
Epoch: 2, batch: 38, training loss: 4.106712
Epoch: 2, batch: 39, training loss: 4.770033
Epoch: 2, batch: 40, training loss: 3.735684
Epoch: 2, batch: 41, training loss: 3.984485
Epoch: 2, batch: 42, training loss: 4.107432
Epoch: 2, batch: 43, training loss: 3.808122
Epoch: 2, batch: 44, training loss: 4.362069
Epoch: 2, batch: 45, training loss: 4.124002
Epoch: 2, batch: 46, training loss: 4.014499
Epoch: 2, batch: 47, training loss: 4.183703
Epoch: 2, batch: 48, training loss: 3.935985
Epoch: 2, batch: 49, training loss: 4.227473
Epoch: 3, batch: 0, training loss: 3.866589
Epoch: 3, batch: 1, training loss: 4.416009
Epoch: 3, batch: 2, training loss: 3.628162
Epoch: 3, batch: 3, training loss: 4.236936
Epoch: 3, batch: 4, training loss: 3.663049
Epoch: 3, batch: 5, training loss: 3.719155
Epoch: 3, batch: 6, training loss: 4.983876
Epoch: 3, batch: 7, training loss: 3.744001
Epoch: 3, batch: 8, training loss: 3.303417
Epoch: 3, batch: 9, training loss: 4.543630
Epoch: 3, batch: 10, training loss: 3.656998
Epoch: 3, batch: 11, training loss: 4.064498
Epoch: 3, batch: 12, training loss: 3.611998
Epoch: 3, batch: 13, training loss: 4.260515
Epoch: 3, batch: 14, training loss: 3.452750
Epoch: 3, batch: 15, training loss: 4.253121
Epoch: 3, batch: 16, training loss: 3.777997
Epoch: 3, batch: 17, training loss: 3.629941
Epoch: 3, batch: 18, training loss: 3.256394
Epoch: 3, batch: 19, training loss: 2.706138
Epoch: 3, batch: 20, training loss: 3.721098
Epoch: 3, batch: 21, training loss: 3.474561
Epoch: 3, batch: 22, training loss: 4.085732
Epoch: 3, batch: 23, training loss: 3.597529
Epoch: 3, batch: 24, training loss: 3.528783
Epoch: 3, batch: 25, training loss: 4.105186
Epoch: 3, batch: 26, training loss: 3.957233
Epoch: 3, batch: 27, training loss: 3.958951
Epoch: 3, batch: 28, training loss: 4.374709
Epoch: 3, batch: 29, training loss: 4.277672
Epoch: 3, batch: 30, training loss: 4.383717
Epoch: 3, batch: 31, training loss: 3.738996
Epoch: 3, batch: 32, training loss: 3.361808
Epoch: 3, batch: 33, training loss: 5.654369
Epoch: 3, batch: 34, training loss: 5.713537
Epoch: 3, batch: 35, training loss: 4.101380
Epoch: 3, batch: 36, training loss: 4.059516
Epoch: 3, batch: 37, training loss: 4.073428
Epoch: 3, batch: 38, training loss: 4.046796
Epoch: 3, batch: 39, training loss: 4.688942
Epoch: 3, batch: 40, training loss: 3.657839
Epoch: 3, batch: 41, training loss: 3.911030
Epoch: 3, batch: 42, training loss: 4.008451
Epoch: 3, batch: 43, training loss: 3.726086
Epoch: 3, batch: 44, training loss: 4.271885
Epoch: 3, batch: 45, training loss: 4.047531
Epoch: 3, batch: 46, training loss: 3.943977
Epoch: 3, batch: 47, training loss: 4.115470
Epoch: 3, batch: 48, training loss: 3.854310
Epoch: 3, batch: 49, training loss: 4.136403
Epoch: 4, batch: 0, training loss: 3.794854
Epoch: 4, batch: 1, training loss: 4.330729
Epoch: 4, batch: 2, training loss: 3.525292
Epoch: 4, batch: 3, training loss: 4.154298
Epoch: 4, batch: 4, training loss: 3.551362
Epoch: 4, batch: 5, training loss: 3.622115
Epoch: 4, batch: 6, training loss: 4.825026
Epoch: 4, batch: 7, training loss: 3.630081
Epoch: 4, batch: 8, training loss: 3.210746
Epoch: 4, batch: 9, training loss: 4.407314
Epoch: 4, batch: 10, training loss: 3.547176
Epoch: 4, batch: 11, training loss: 3.860418
Epoch: 4, batch: 12, training loss: 3.481624
Epoch: 4, batch: 13, training loss: 4.195748
Epoch: 4, batch: 14, training loss: 3.371515
Epoch: 4, batch: 15, training loss: 4.212358
Epoch: 4, batch: 16, training loss: 3.765749
Epoch: 4, batch: 17, training loss: 3.542880
Epoch: 4, batch: 18, training loss: 3.158338
Epoch: 4, batch: 19, training loss: 2.638725
Epoch: 4, batch: 20, training loss: 3.697406
Epoch: 4, batch: 21, training loss: 3.338017
Epoch: 4, batch: 22, training loss: 3.976527
Epoch: 4, batch: 23, training loss: 3.533201
Epoch: 4, batch: 24, training loss: 3.478199
Epoch: 4, batch: 25, training loss: 4.045503
Epoch: 4, batch: 26, training loss: 3.879674
Epoch: 4, batch: 27, training loss: 3.876146
Epoch: 4, batch: 28, training loss: 4.271629
Epoch: 4, batch: 29, training loss: 4.157870
Epoch: 4, batch: 30, training loss: 4.271827
Epoch: 4, batch: 31, training loss: 3.641695
Epoch: 4, batch: 32, training loss: 3.278347
Epoch: 4, batch: 33, training loss: 5.497192
Epoch: 4, batch: 34, training loss: 5.579465
Epoch: 4, batch: 35, training loss: 4.011032
Epoch: 4, batch: 36, training loss: 3.937888
Epoch: 4, batch: 37, training loss: 3.972714
Epoch: 4, batch: 38, training loss: 3.964224
Epoch: 4, batch: 39, training loss: 4.582906
Epoch: 4, batch: 40, training loss: 3.559609
Epoch: 4, batch: 41, training loss: 3.835606
Epoch: 4, batch: 42, training loss: 3.920649
Epoch: 4, batch: 43, training loss: 3.634942
Epoch: 4, batch: 44, training loss: 4.182010
Epoch: 4, batch: 45, training loss: 3.952305
Epoch: 4, batch: 46, training loss: 3.846937
Epoch: 4, batch: 47, training loss: 4.001695
Epoch: 4, batch: 48, training loss: 3.734326
Epoch: 4, batch: 49, training loss: 4.005456
Epoch: 5, batch: 0, training loss: 3.698874
Epoch: 5, batch: 1, training loss: 4.230628
Epoch: 5, batch: 2, training loss: 3.398581
Epoch: 5, batch: 3, training loss: 4.050043
Epoch: 5, batch: 4, training loss: 3.376214
Epoch: 5, batch: 5, training loss: 3.450177
Epoch: 5, batch: 6, training loss: 4.543783
Epoch: 5, batch: 7, training loss: 3.433305
Epoch: 5, batch: 8, training loss: 3.071434
Epoch: 5, batch: 9, training loss: 4.195395
Epoch: 5, batch: 10, training loss: 3.405980
Epoch: 5, batch: 11, training loss: 3.711486
Epoch: 5, batch: 12, training loss: 3.347545
Epoch: 5, batch: 13, training loss: 3.971587
Epoch: 5, batch: 14, training loss: 3.235518
Epoch: 5, batch: 15, training loss: 4.073760
Epoch: 5, batch: 16, training loss: 3.600280
Epoch: 5, batch: 17, training loss: 3.439808
Epoch: 5, batch: 18, training loss: 3.081018
Epoch: 5, batch: 19, training loss: 2.534314
Epoch: 5, batch: 20, training loss: 3.532393
Epoch: 5, batch: 21, training loss: 3.202199
Epoch: 5, batch: 22, training loss: 3.856755
Epoch: 5, batch: 23, training loss: 3.426177
Epoch: 5, batch: 24, training loss: 3.350407
Epoch: 5, batch: 25, training loss: 3.873246
Epoch: 5, batch: 26, training loss: 3.763653
Epoch: 5, batch: 27, training loss: 3.775499
Epoch: 5, batch: 28, training loss: 4.147553
Epoch: 5, batch: 29, training loss: 4.003659
Epoch: 5, batch: 30, training loss: 4.103265
Epoch: 5, batch: 31, training loss: 3.499140
Epoch: 5, batch: 32, training loss: 3.167283
Epoch: 5, batch: 33, training loss: 5.252877
Epoch: 5, batch: 34, training loss: 5.325003
Epoch: 5, batch: 35, training loss: 3.880593
Epoch: 5, batch: 36, training loss: 3.813247
Epoch: 5, batch: 37, training loss: 3.818858
Epoch: 5, batch: 38, training loss: 3.802829
Epoch: 5, batch: 39, training loss: 4.439040
Epoch: 5, batch: 40, training loss: 3.418124
Epoch: 5, batch: 41, training loss: 3.697751
Epoch: 5, batch: 42, training loss: 3.768967
Epoch: 5, batch: 43, training loss: 3.482999
Epoch: 5, batch: 44, training loss: 4.074586
Epoch: 5, batch: 45, training loss: 3.829407
Epoch: 5, batch: 46, training loss: 3.713925
Epoch: 5, batch: 47, training loss: 3.842490
Epoch: 5, batch: 48, training loss: 3.581882
Epoch: 5, batch: 49, training loss: 3.838068
Epoch: 6, batch: 0, training loss: 3.559320
Epoch: 6, batch: 1, training loss: 4.068893
Epoch: 6, batch: 2, training loss: 3.233403
Epoch: 6, batch: 3, training loss: 3.934031
Epoch: 6, batch: 4, training loss: 3.114137
Epoch: 6, batch: 5, training loss: 3.184233
Epoch: 6, batch: 6, training loss: 4.122019
Epoch: 6, batch: 7, training loss: 3.147293
Epoch: 6, batch: 8, training loss: 2.854047
Epoch: 6, batch: 9, training loss: 3.890474
Epoch: 6, batch: 10, training loss: 3.255172
Epoch: 6, batch: 11, training loss: 3.568499
Epoch: 6, batch: 12, training loss: 3.195294
Epoch: 6, batch: 13, training loss: 3.773074
Epoch: 6, batch: 14, training loss: 3.098308
Epoch: 6, batch: 15, training loss: 3.894480
Epoch: 6, batch: 16, training loss: 3.447236
Epoch: 6, batch: 17, training loss: 3.297623
Epoch: 6, batch: 18, training loss: 2.939467
Epoch: 6, batch: 19, training loss: 2.419549
Epoch: 6, batch: 20, training loss: 3.386222
Epoch: 6, batch: 21, training loss: 3.012083
Epoch: 6, batch: 22, training loss: 3.655916
Epoch: 6, batch: 23, training loss: 3.264889
Epoch: 6, batch: 24, training loss: 3.200240
Epoch: 6, batch: 25, training loss: 3.706402
Epoch: 6, batch: 26, training loss: 3.621188
Epoch: 6, batch: 27, training loss: 3.626932
Epoch: 6, batch: 28, training loss: 3.988499
Epoch: 6, batch: 29, training loss: 3.837697
Epoch: 6, batch: 30, training loss: 3.944955
Epoch: 6, batch: 31, training loss: 3.334693
Epoch: 6, batch: 32, training loss: 3.039122
Epoch: 6, batch: 33, training loss: 4.987565
Epoch: 6, batch: 34, training loss: 5.049490
Epoch: 6, batch: 35, training loss: 3.707184
Epoch: 6, batch: 36, training loss: 3.628524
Epoch: 6, batch: 37, training loss: 3.630233
Epoch: 6, batch: 38, training loss: 3.647231
Epoch: 6, batch: 39, training loss: 4.272954
Epoch: 6, batch: 40, training loss: 3.236043
Epoch: 6, batch: 41, training loss: 3.527982
Epoch: 6, batch: 42, training loss: 3.615452
Epoch: 6, batch: 43, training loss: 3.276783
Epoch: 6, batch: 44, training loss: 3.901918
Epoch: 6, batch: 45, training loss: 3.685143
Epoch: 6, batch: 46, training loss: 3.582224
Epoch: 6, batch: 47, training loss: 3.656066
Epoch: 6, batch: 48, training loss: 3.368677
Epoch: 6, batch: 49, training loss: 3.650005
Epoch: 7, batch: 0, training loss: 3.425781
Epoch: 7, batch: 1, training loss: 3.895536
Epoch: 7, batch: 2, training loss: 2.986646
Epoch: 7, batch: 3, training loss: 3.742912
Epoch: 7, batch: 4, training loss: 2.859347
Epoch: 7, batch: 5, training loss: 2.965598
Epoch: 7, batch: 6, training loss: 3.701204
Epoch: 7, batch: 7, training loss: 2.854782
Epoch: 7, batch: 8, training loss: 2.689058
Epoch: 7, batch: 9, training loss: 3.574677
Epoch: 7, batch: 10, training loss: 3.047244
Epoch: 7, batch: 11, training loss: 3.351964
Epoch: 7, batch: 12, training loss: 3.021894
Epoch: 7, batch: 13, training loss: 3.589187
Epoch: 7, batch: 14, training loss: 2.965354
Epoch: 7, batch: 15, training loss: 3.721282
Epoch: 7, batch: 16, training loss: 3.303213
Epoch: 7, batch: 17, training loss: 3.165697
Epoch: 7, batch: 18, training loss: 2.800446
Epoch: 7, batch: 19, training loss: 2.308979
Epoch: 7, batch: 20, training loss: 3.237437
Epoch: 7, batch: 21, training loss: 2.867760
Epoch: 7, batch: 22, training loss: 3.479909
Epoch: 7, batch: 23, training loss: 3.113468
Epoch: 7, batch: 24, training loss: 3.046833
Epoch: 7, batch: 25, training loss: 3.521827
Epoch: 7, batch: 26, training loss: 3.453748
Epoch: 7, batch: 27, training loss: 3.469573
Epoch: 7, batch: 28, training loss: 3.818418
Epoch: 7, batch: 29, training loss: 3.657603
Epoch: 7, batch: 30, training loss: 3.770994
Epoch: 7, batch: 31, training loss: 3.179143
Epoch: 7, batch: 32, training loss: 2.927571
Epoch: 7, batch: 33, training loss: 4.707299
Epoch: 7, batch: 34, training loss: 4.746582
Epoch: 7, batch: 35, training loss: 3.558397
Epoch: 7, batch: 36, training loss: 3.461766
Epoch: 7, batch: 37, training loss: 3.442573
Epoch: 7, batch: 38, training loss: 3.478635
Epoch: 7, batch: 39, training loss: 4.095709
Epoch: 7, batch: 40, training loss: 3.059299
Epoch: 7, batch: 41, training loss: 3.347771
Epoch: 7, batch: 42, training loss: 3.428301
Epoch: 7, batch: 43, training loss: 3.103364
Epoch: 7, batch: 44, training loss: 3.753116
Epoch: 7, batch: 45, training loss: 3.530163
Epoch: 7, batch: 46, training loss: 3.410625
Epoch: 7, batch: 47, training loss: 3.459200
Epoch: 7, batch: 48, training loss: 3.191376
Epoch: 7, batch: 49, training loss: 3.447952
Epoch: 8, batch: 0, training loss: 3.273040
Epoch: 8, batch: 1, training loss: 3.692212
Epoch: 8, batch: 2, training loss: 2.798448
Epoch: 8, batch: 3, training loss: 3.608254
Epoch: 8, batch: 4, training loss: 2.588881
Epoch: 8, batch: 5, training loss: 2.712701
Epoch: 8, batch: 6, training loss: 3.346290
Epoch: 8, batch: 7, training loss: 2.613461
Epoch: 8, batch: 8, training loss: 2.485942
Epoch: 8, batch: 9, training loss: 3.257688
Epoch: 8, batch: 10, training loss: 2.930241
Epoch: 8, batch: 11, training loss: 3.202852
Epoch: 8, batch: 12, training loss: 2.878419
Epoch: 8, batch: 13, training loss: 3.413387
Epoch: 8, batch: 14, training loss: 2.847708
Epoch: 8, batch: 15, training loss: 3.558199
Epoch: 8, batch: 16, training loss: 3.156260
Epoch: 8, batch: 17, training loss: 3.047839
Epoch: 8, batch: 18, training loss: 2.698105
Epoch: 8, batch: 19, training loss: 2.211511
Epoch: 8, batch: 20, training loss: 3.098520
Epoch: 8, batch: 21, training loss: 2.622186
Epoch: 8, batch: 22, training loss: 3.266343
Epoch: 8, batch: 23, training loss: 2.963058
Epoch: 8, batch: 24, training loss: 2.913371
Epoch: 8, batch: 25, training loss: 3.366783
Epoch: 8, batch: 26, training loss: 3.299614
Epoch: 8, batch: 27, training loss: 3.323641
Epoch: 8, batch: 28, training loss: 3.651141
Epoch: 8, batch: 29, training loss: 3.473978
Epoch: 8, batch: 30, training loss: 3.600038
Epoch: 8, batch: 31, training loss: 3.027115
Epoch: 8, batch: 32, training loss: 2.809456
Epoch: 8, batch: 33, training loss: 4.424200
Epoch: 8, batch: 34, training loss: 4.433309
Epoch: 8, batch: 35, training loss: 3.394058
Epoch: 8, batch: 36, training loss: 3.304195
Epoch: 8, batch: 37, training loss: 3.248793
Epoch: 8, batch: 38, training loss: 3.316978
Epoch: 8, batch: 39, training loss: 3.917180
Epoch: 8, batch: 40, training loss: 2.883845
Epoch: 8, batch: 41, training loss: 3.193157
Epoch: 8, batch: 42, training loss: 3.274591
Epoch: 8, batch: 43, training loss: 2.899333
Epoch: 8, batch: 44, training loss: 3.565428
Epoch: 8, batch: 45, training loss: 3.389179
Epoch: 8, batch: 46, training loss: 3.270467
Epoch: 8, batch: 47, training loss: 3.279612
Epoch: 8, batch: 48, training loss: 2.983012
Epoch: 8, batch: 49, training loss: 3.251566
Epoch: 9, batch: 0, training loss: 3.152117
Epoch: 9, batch: 1, training loss: 3.507490
Epoch: 9, batch: 2, training loss: 2.564190
Epoch: 9, batch: 3, training loss: 3.362723
Epoch: 9, batch: 4, training loss: 2.325355
Epoch: 9, batch: 5, training loss: 2.470570
Epoch: 9, batch: 6, training loss: 2.876504
Epoch: 9, batch: 7, training loss: 2.359734
Epoch: 9, batch: 8, training loss: 2.357131
Epoch: 9, batch: 9, training loss: 2.968215
Epoch: 9, batch: 10, training loss: 2.716914
Epoch: 9, batch: 11, training loss: 3.040564
Epoch: 9, batch: 12, training loss: 2.697831
Epoch: 9, batch: 13, training loss: 3.190841
Epoch: 9, batch: 14, training loss: 2.705121
Epoch: 9, batch: 15, training loss: 3.410682
Epoch: 9, batch: 16, training loss: 3.028059
Epoch: 9, batch: 17, training loss: 2.930651
Epoch: 9, batch: 18, training loss: 2.613661
Epoch: 9, batch: 19, training loss: 2.138236
Epoch: 9, batch: 20, training loss: 2.996538
Epoch: 9, batch: 21, training loss: 2.448879
Epoch: 9, batch: 22, training loss: 3.109766
Epoch: 9, batch: 23, training loss: 2.828221
Epoch: 9, batch: 24, training loss: 2.797952
Epoch: 9, batch: 25, training loss: 3.202359
Epoch: 9, batch: 26, training loss: 3.161707
Epoch: 9, batch: 27, training loss: 3.207471
Epoch: 9, batch: 28, training loss: 3.501368
Epoch: 9, batch: 29, training loss: 3.308453
Epoch: 9, batch: 30, training loss: 3.432470
Epoch: 9, batch: 31, training loss: 2.889936
Epoch: 9, batch: 32, training loss: 2.692027
Epoch: 9, batch: 33, training loss: 4.161576
Epoch: 9, batch: 34, training loss: 4.133129
Epoch: 9, batch: 35, training loss: 3.222059
Epoch: 9, batch: 36, training loss: 3.133592
Epoch: 9, batch: 37, training loss: 3.067818
Epoch: 9, batch: 38, training loss: 3.201112
Epoch: 9, batch: 39, training loss: 3.741084
Epoch: 9, batch: 40, training loss: 2.705631
Epoch: 9, batch: 41, training loss: 3.050781
Epoch: 9, batch: 42, training loss: 3.151351
Epoch: 9, batch: 43, training loss: 2.751086
Epoch: 9, batch: 44, training loss: 3.401629
Epoch: 9, batch: 45, training loss: 3.230248
Epoch: 9, batch: 46, training loss: 3.111311
Epoch: 9, batch: 47, training loss: 3.122918
Epoch: 9, batch: 48, training loss: 2.866602
Epoch: 9, batch: 49, training loss: 3.092726
Epoch: 10, batch: 0, training loss: 2.997168
Epoch: 10, batch: 1, training loss: 3.329041
Epoch: 10, batch: 2, training loss: 2.403744
Epoch: 10, batch: 3, training loss: 3.203852
Epoch: 10, batch: 4, training loss: 2.121992
Epoch: 10, batch: 5, training loss: 2.262091
Epoch: 10, batch: 6, training loss: 2.528223
Epoch: 10, batch: 7, training loss: 2.118809
Epoch: 10, batch: 8, training loss: 2.152020
Epoch: 10, batch: 9, training loss: 2.614963
Epoch: 10, batch: 10, training loss: 2.542703
Epoch: 10, batch: 11, training loss: 2.839131
Epoch: 10, batch: 12, training loss: 2.550789
Epoch: 10, batch: 13, training loss: 3.021810
Epoch: 10, batch: 14, training loss: 2.577868
Epoch: 10, batch: 15, training loss: 3.230338
Epoch: 10, batch: 16, training loss: 2.845621
Epoch: 10, batch: 17, training loss: 2.779498
Epoch: 10, batch: 18, training loss: 2.482327
Epoch: 10, batch: 19, training loss: 2.041238
Epoch: 10, batch: 20, training loss: 2.833031
Epoch: 10, batch: 21, training loss: 2.279757
Epoch: 10, batch: 22, training loss: 2.897690
Epoch: 10, batch: 23, training loss: 2.683637
Epoch: 10, batch: 24, training loss: 2.683825
Epoch: 10, batch: 25, training loss: 3.059896
Epoch: 10, batch: 26, training loss: 3.016839
Epoch: 10, batch: 27, training loss: 3.077985
Epoch: 10, batch: 28, training loss: 3.352955
Epoch: 10, batch: 29, training loss: 3.135875
Epoch: 10, batch: 30, training loss: 3.292463
Epoch: 10, batch: 31, training loss: 2.773585
Epoch: 10, batch: 32, training loss: 2.601917
Epoch: 10, batch: 33, training loss: 3.886313
Epoch: 10, batch: 34, training loss: 3.833766
Epoch: 10, batch: 35, training loss: 3.067175
Epoch: 10, batch: 36, training loss: 2.965708
Epoch: 10, batch: 37, training loss: 2.884135
Epoch: 10, batch: 38, training loss: 3.044974
Epoch: 10, batch: 39, training loss: 3.556345
Epoch: 10, batch: 40, training loss: 2.537119
Epoch: 10, batch: 41, training loss: 2.890052
Epoch: 10, batch: 42, training loss: 2.986995
Epoch: 10, batch: 43, training loss: 2.592970
Epoch: 10, batch: 44, training loss: 3.271672
Epoch: 10, batch: 45, training loss: 3.117763
Epoch: 10, batch: 46, training loss: 2.974890
Epoch: 10, batch: 47, training loss: 2.932930
Epoch: 10, batch: 48, training loss: 2.679477
Epoch: 10, batch: 49, training loss: 2.955933
Epoch: 11, batch: 0, training loss: 2.896825
Epoch: 11, batch: 1, training loss: 3.193588
Epoch: 11, batch: 2, training loss: 2.216369
Epoch: 11, batch: 3, training loss: 2.991911
Epoch: 11, batch: 4, training loss: 1.933386
Epoch: 11, batch: 5, training loss: 2.117580
Epoch: 11, batch: 6, training loss: 2.257001
Epoch: 11, batch: 7, training loss: 1.945318
Epoch: 11, batch: 8, training loss: 1.979224
Epoch: 11, batch: 9, training loss: 2.312185
Epoch: 11, batch: 10, training loss: 2.391818
Epoch: 11, batch: 11, training loss: 2.646950
Epoch: 11, batch: 12, training loss: 2.384454
Epoch: 11, batch: 13, training loss: 2.858105
Epoch: 11, batch: 14, training loss: 2.477636
Epoch: 11, batch: 15, training loss: 3.101570
Epoch: 11, batch: 16, training loss: 2.702073
Epoch: 11, batch: 17, training loss: 2.655390
Epoch: 11, batch: 18, training loss: 2.371509
Epoch: 11, batch: 19, training loss: 1.968759
Epoch: 11, batch: 20, training loss: 2.725019
Epoch: 11, batch: 21, training loss: 2.032897
Epoch: 11, batch: 22, training loss: 2.722247
Epoch: 11, batch: 23, training loss: 2.518464
Epoch: 11, batch: 24, training loss: 2.556078
Epoch: 11, batch: 25, training loss: 2.881370
Epoch: 11, batch: 26, training loss: 2.891786
Epoch: 11, batch: 27, training loss: 2.984272
Epoch: 11, batch: 28, training loss: 3.262743
Epoch: 11, batch: 29, training loss: 2.986856
Epoch: 11, batch: 30, training loss: 3.124405
Epoch: 11, batch: 31, training loss: 2.616591
Epoch: 11, batch: 32, training loss: 2.488094
Epoch: 11, batch: 33, training loss: 3.635227
Epoch: 11, batch: 34, training loss: 3.585178
Epoch: 11, batch: 35, training loss: 2.955776
Epoch: 11, batch: 36, training loss: 2.834413
Epoch: 11, batch: 37, training loss: 2.728949
Epoch: 11, batch: 38, training loss: 2.907053
Epoch: 11, batch: 39, training loss: 3.398505
Epoch: 11, batch: 40, training loss: 2.402612
Epoch: 11, batch: 41, training loss: 2.786495
Epoch: 11, batch: 42, training loss: 2.875141
Epoch: 11, batch: 43, training loss: 2.438799
Epoch: 11, batch: 44, training loss: 3.084362
Epoch: 11, batch: 45, training loss: 2.978319
Epoch: 11, batch: 46, training loss: 2.905205
Epoch: 11, batch: 47, training loss: 2.846603
Epoch: 11, batch: 48, training loss: 2.574265
Epoch: 11, batch: 49, training loss: 2.773543
Epoch: 12, batch: 0, training loss: 2.717175
Epoch: 12, batch: 1, training loss: 3.012899
Epoch: 12, batch: 2, training loss: 2.117589
Epoch: 12, batch: 3, training loss: 2.905540
Epoch: 12, batch: 4, training loss: 1.783165
Epoch: 12, batch: 5, training loss: 1.937288
Epoch: 12, batch: 6, training loss: 2.000037
Epoch: 12, batch: 7, training loss: 1.786837
Epoch: 12, batch: 8, training loss: 1.860220
Epoch: 12, batch: 9, training loss: 2.156808
Epoch: 12, batch: 10, training loss: 2.243003
Epoch: 12, batch: 11, training loss: 2.523087
Epoch: 12, batch: 12, training loss: 2.250979
Epoch: 12, batch: 13, training loss: 2.693258
Epoch: 12, batch: 14, training loss: 2.344830
Epoch: 12, batch: 15, training loss: 2.957466
Epoch: 12, batch: 16, training loss: 2.625790
Epoch: 12, batch: 17, training loss: 2.579576
Epoch: 12, batch: 18, training loss: 2.293844
Epoch: 12, batch: 19, training loss: 1.892671
Epoch: 12, batch: 20, training loss: 2.608791
Epoch: 12, batch: 21, training loss: 2.001024
Epoch: 12, batch: 22, training loss: 2.667354
Epoch: 12, batch: 23, training loss: 2.440070
Epoch: 12, batch: 24, training loss: 2.485561
Epoch: 12, batch: 25, training loss: 2.763817
Epoch: 12, batch: 26, training loss: 2.765468
Epoch: 12, batch: 27, training loss: 2.847054
Epoch: 12, batch: 28, training loss: 3.137963
Epoch: 12, batch: 29, training loss: 2.906007
Epoch: 12, batch: 30, training loss: 3.047867
Epoch: 12, batch: 31, training loss: 2.537423
Epoch: 12, batch: 32, training loss: 2.394871
Epoch: 12, batch: 33, training loss: 3.376339
Epoch: 12, batch: 34, training loss: 3.332495
Epoch: 12, batch: 35, training loss: 2.848150
Epoch: 12, batch: 36, training loss: 2.767495
Epoch: 12, batch: 37, training loss: 2.687036
Epoch: 12, batch: 38, training loss: 2.846267
Epoch: 12, batch: 39, training loss: 3.271896
Epoch: 12, batch: 40, training loss: 2.275016
Epoch: 12, batch: 41, training loss: 2.658337
Epoch: 12, batch: 42, training loss: 2.799103
Epoch: 12, batch: 43, training loss: 2.398389
Epoch: 12, batch: 44, training loss: 2.992863
Epoch: 12, batch: 45, training loss: 2.888484
Epoch: 12, batch: 46, training loss: 2.756487
Epoch: 12, batch: 47, training loss: 2.691894
Epoch: 12, batch: 48, training loss: 2.480399
Epoch: 12, batch: 49, training loss: 2.691500
Epoch: 13, batch: 0, training loss: 2.676725
Epoch: 13, batch: 1, training loss: 2.919822
Epoch: 13, batch: 2, training loss: 1.991997
Epoch: 13, batch: 3, training loss: 2.694546
Epoch: 13, batch: 4, training loss: 1.624952
Epoch: 13, batch: 5, training loss: 1.829618
Epoch: 13, batch: 6, training loss: 1.863052
Epoch: 13, batch: 7, training loss: 1.669940
Epoch: 13, batch: 8, training loss: 1.726070
Epoch: 13, batch: 9, training loss: 1.943382
Epoch: 13, batch: 10, training loss: 2.154310
Epoch: 13, batch: 11, training loss: 2.450971
Epoch: 13, batch: 12, training loss: 2.176787
Epoch: 13, batch: 13, training loss: 2.587433
Epoch: 13, batch: 14, training loss: 2.272954
Epoch: 13, batch: 15, training loss: 2.854521
Epoch: 13, batch: 16, training loss: 2.493499
Epoch: 13, batch: 17, training loss: 2.488987
Epoch: 13, batch: 18, training loss: 2.210174
Epoch: 13, batch: 19, training loss: 1.817575
Epoch: 13, batch: 20, training loss: 2.487903
Epoch: 13, batch: 21, training loss: 1.912949
Epoch: 13, batch: 22, training loss: 2.531734
Epoch: 13, batch: 23, training loss: 2.327197
Epoch: 13, batch: 24, training loss: 2.420045
Epoch: 13, batch: 25, training loss: 2.711800
Epoch: 13, batch: 26, training loss: 2.675033
Epoch: 13, batch: 27, training loss: 2.742847
Epoch: 13, batch: 28, training loss: 2.995867
Epoch: 13, batch: 29, training loss: 2.750398
Epoch: 13, batch: 30, training loss: 2.934529
Epoch: 13, batch: 31, training loss: 2.482096
Epoch: 13, batch: 32, training loss: 2.352644
Epoch: 13, batch: 33, training loss: 3.232076
Epoch: 13, batch: 34, training loss: 3.149560
Epoch: 13, batch: 35, training loss: 2.718372
Epoch: 13, batch: 36, training loss: 2.608455
Epoch: 13, batch: 37, training loss: 2.553464
Epoch: 13, batch: 38, training loss: 2.733417
Epoch: 13, batch: 39, training loss: 3.161227
Epoch: 13, batch: 40, training loss: 2.203976
Epoch: 13, batch: 41, training loss: 2.578795
Epoch: 13, batch: 42, training loss: 2.676633
Epoch: 13, batch: 43, training loss: 2.242466
Epoch: 13, batch: 44, training loss: 2.838615
Epoch: 13, batch: 45, training loss: 2.786807
Epoch: 13, batch: 46, training loss: 2.672960
Epoch: 13, batch: 47, training loss: 2.603968
Epoch: 13, batch: 48, training loss: 2.375903
Epoch: 13, batch: 49, training loss: 2.524532
Epoch: 14, batch: 0, training loss: 2.515792
Epoch: 14, batch: 1, training loss: 2.769939
Epoch: 14, batch: 2, training loss: 1.903907
Epoch: 14, batch: 3, training loss: 2.591451
Epoch: 14, batch: 4, training loss: 1.513166
Epoch: 14, batch: 5, training loss: 1.684729
Epoch: 14, batch: 6, training loss: 1.625268
Epoch: 14, batch: 7, training loss: 1.531210
Epoch: 14, batch: 8, training loss: 1.622824
Epoch: 14, batch: 9, training loss: 1.750534
Epoch: 14, batch: 10, training loss: 2.025664
Epoch: 14, batch: 11, training loss: 2.361351
Epoch: 14, batch: 12, training loss: 2.081353
Epoch: 14, batch: 13, training loss: 2.451519
Epoch: 14, batch: 14, training loss: 2.160814
Epoch: 14, batch: 15, training loss: 2.666363
Epoch: 14, batch: 16, training loss: 2.353554
Epoch: 14, batch: 17, training loss: 2.386536
Epoch: 14, batch: 18, training loss: 2.119535
Epoch: 14, batch: 19, training loss: 1.739360
Epoch: 14, batch: 20, training loss: 2.391947
Epoch: 14, batch: 21, training loss: 1.759063
Epoch: 14, batch: 22, training loss: 2.393096
Epoch: 14, batch: 23, training loss: 2.194112
Epoch: 14, batch: 24, training loss: 2.287013
Epoch: 14, batch: 25, training loss: 2.560643
Epoch: 14, batch: 26, training loss: 2.555791
Epoch: 14, batch: 27, training loss: 2.665068
Epoch: 14, batch: 28, training loss: 2.885471
Epoch: 14, batch: 29, training loss: 2.617392
Epoch: 14, batch: 30, training loss: 2.778854
Epoch: 14, batch: 31, training loss: 2.327108
Epoch: 14, batch: 32, training loss: 2.255510
Epoch: 14, batch: 33, training loss: 3.047152
Epoch: 14, batch: 34, training loss: 2.955209
Epoch: 14, batch: 35, training loss: 2.638228
Epoch: 14, batch: 36, training loss: 2.485400
Epoch: 14, batch: 37, training loss: 2.409085
Epoch: 14, batch: 38, training loss: 2.577048
Epoch: 14, batch: 39, training loss: 3.017646
Epoch: 14, batch: 40, training loss: 2.108844
Epoch: 14, batch: 41, training loss: 2.488645
Epoch: 14, batch: 42, training loss: 2.572738
Epoch: 14, batch: 43, training loss: 2.146937
Epoch: 14, batch: 44, training loss: 2.703450
Epoch: 14, batch: 45, training loss: 2.649861
Epoch: 14, batch: 46, training loss: 2.556395
Epoch: 14, batch: 47, training loss: 2.485901
Epoch: 14, batch: 48, training loss: 2.259094
Epoch: 14, batch: 49, training loss: 2.420161
Epoch: 15, batch: 0, training loss: 2.437797
Epoch: 15, batch: 1, training loss: 2.651143
Epoch: 15, batch: 2, training loss: 1.782180
Epoch: 15, batch: 3, training loss: 2.433429
Epoch: 15, batch: 4, training loss: 1.407847
Epoch: 15, batch: 5, training loss: 1.615768
Epoch: 15, batch: 6, training loss: 1.475636
Epoch: 15, batch: 7, training loss: 1.408970
Epoch: 15, batch: 8, training loss: 1.484516
Epoch: 15, batch: 9, training loss: 1.620989
Epoch: 15, batch: 10, training loss: 1.941227
Epoch: 15, batch: 11, training loss: 2.281493
Epoch: 15, batch: 12, training loss: 1.987752
Epoch: 15, batch: 13, training loss: 2.368562
Epoch: 15, batch: 14, training loss: 2.100216
Epoch: 15, batch: 15, training loss: 2.590920
Epoch: 15, batch: 16, training loss: 2.287518
Epoch: 15, batch: 17, training loss: 2.313005
Epoch: 15, batch: 18, training loss: 2.046859
Epoch: 15, batch: 19, training loss: 1.680564
Epoch: 15, batch: 20, training loss: 2.293287
Epoch: 15, batch: 21, training loss: 1.608021
Epoch: 15, batch: 22, training loss: 2.274362
Epoch: 15, batch: 23, training loss: 2.117829
Epoch: 15, batch: 24, training loss: 2.205007
Epoch: 15, batch: 25, training loss: 2.442679
Epoch: 15, batch: 26, training loss: 2.455991
Epoch: 15, batch: 27, training loss: 2.565246
Epoch: 15, batch: 28, training loss: 2.781898
Epoch: 15, batch: 29, training loss: 2.537359
Epoch: 15, batch: 30, training loss: 2.682863
Epoch: 15, batch: 31, training loss: 2.220579
Epoch: 15, batch: 32, training loss: 2.172075
Epoch: 15, batch: 33, training loss: 2.855428
Epoch: 15, batch: 34, training loss: 2.764993
Epoch: 15, batch: 35, training loss: 2.557101
Epoch: 15, batch: 36, training loss: 2.404892
Epoch: 15, batch: 37, training loss: 2.318105
Epoch: 15, batch: 38, training loss: 2.487475
Epoch: 15, batch: 39, training loss: 2.884573
Epoch: 15, batch: 40, training loss: 1.986218
Epoch: 15, batch: 41, training loss: 2.375561
Epoch: 15, batch: 42, training loss: 2.490508
Epoch: 15, batch: 43, training loss: 2.044641
Epoch: 15, batch: 44, training loss: 2.593068
Epoch: 15, batch: 45, training loss: 2.563423
Epoch: 15, batch: 46, training loss: 2.461942
Epoch: 15, batch: 47, training loss: 2.387385
Epoch: 15, batch: 48, training loss: 2.159885
Epoch: 15, batch: 49, training loss: 2.327331
Epoch: 16, batch: 0, training loss: 2.342595
Epoch: 16, batch: 1, training loss: 2.537926
Epoch: 16, batch: 2, training loss: 1.672694
Epoch: 16, batch: 3, training loss: 2.329302
Epoch: 16, batch: 4, training loss: 1.307462
Epoch: 16, batch: 5, training loss: 1.506892
Epoch: 16, batch: 6, training loss: 1.351905
Epoch: 16, batch: 7, training loss: 1.313446
Epoch: 16, batch: 8, training loss: 1.425115
Epoch: 16, batch: 9, training loss: 1.492552
Epoch: 16, batch: 10, training loss: 1.858389
Epoch: 16, batch: 11, training loss: 2.209070
Epoch: 16, batch: 12, training loss: 1.913525
Epoch: 16, batch: 13, training loss: 2.286351
Epoch: 16, batch: 14, training loss: 2.041035
Epoch: 16, batch: 15, training loss: 2.500238
Epoch: 16, batch: 16, training loss: 2.216146
Epoch: 16, batch: 17, training loss: 2.254839
Epoch: 16, batch: 18, training loss: 2.007224
Epoch: 16, batch: 19, training loss: 1.640885
Epoch: 16, batch: 20, training loss: 2.251762
Epoch: 16, batch: 21, training loss: 1.569348
Epoch: 16, batch: 22, training loss: 2.196547
Epoch: 16, batch: 23, training loss: 2.050154
Epoch: 16, batch: 24, training loss: 2.136403
Epoch: 16, batch: 25, training loss: 2.391290
Epoch: 16, batch: 26, training loss: 2.380141
Epoch: 16, batch: 27, training loss: 2.505847
Epoch: 16, batch: 28, training loss: 2.709884
Epoch: 16, batch: 29, training loss: 2.458398
Epoch: 16, batch: 30, training loss: 2.598301
Epoch: 16, batch: 31, training loss: 2.148637
Epoch: 16, batch: 32, training loss: 2.108735
Epoch: 16, batch: 33, training loss: 2.714008
Epoch: 16, batch: 34, training loss: 2.624733
Epoch: 16, batch: 35, training loss: 2.450479
Epoch: 16, batch: 36, training loss: 2.338530
Epoch: 16, batch: 37, training loss: 2.232486
Epoch: 16, batch: 38, training loss: 2.415043
Epoch: 16, batch: 39, training loss: 2.798913
Epoch: 16, batch: 40, training loss: 1.916961
Epoch: 16, batch: 41, training loss: 2.306516
Epoch: 16, batch: 42, training loss: 2.413555
Epoch: 16, batch: 43, training loss: 1.976235
Epoch: 16, batch: 44, training loss: 2.504034
Epoch: 16, batch: 45, training loss: 2.474689
Epoch: 16, batch: 46, training loss: 2.363761
Epoch: 16, batch: 47, training loss: 2.284325
Epoch: 16, batch: 48, training loss: 2.080290
Epoch: 16, batch: 49, training loss: 2.244211
Epoch: 17, batch: 0, training loss: 2.270585
Epoch: 17, batch: 1, training loss: 2.452440
Epoch: 17, batch: 2, training loss: 1.601065
Epoch: 17, batch: 3, training loss: 2.237543
Epoch: 17, batch: 4, training loss: 1.239199
Epoch: 17, batch: 5, training loss: 1.412475
Epoch: 17, batch: 6, training loss: 1.253026
Epoch: 17, batch: 7, training loss: 1.244532
Epoch: 17, batch: 8, training loss: 1.346507
Epoch: 17, batch: 9, training loss: 1.418409
Epoch: 17, batch: 10, training loss: 1.784225
Epoch: 17, batch: 11, training loss: 2.138829
Epoch: 17, batch: 12, training loss: 1.861264
Epoch: 17, batch: 13, training loss: 2.188337
Epoch: 17, batch: 14, training loss: 1.981448
Epoch: 17, batch: 15, training loss: 2.414664
Epoch: 17, batch: 16, training loss: 2.138921
Epoch: 17, batch: 17, training loss: 2.207540
Epoch: 17, batch: 18, training loss: 1.994177
Epoch: 17, batch: 19, training loss: 1.597413
Epoch: 17, batch: 20, training loss: 2.207857
Epoch: 17, batch: 21, training loss: 1.483928
Epoch: 17, batch: 22, training loss: 2.151272
Epoch: 17, batch: 23, training loss: 2.000061
Epoch: 17, batch: 24, training loss: 2.115969
Epoch: 17, batch: 25, training loss: 2.357099
Epoch: 17, batch: 26, training loss: 2.335920
Epoch: 17, batch: 27, training loss: 2.437587
Epoch: 17, batch: 28, training loss: 2.647622
Epoch: 17, batch: 29, training loss: 2.395975
Epoch: 17, batch: 30, training loss: 2.534344
Epoch: 17, batch: 31, training loss: 2.109710
Epoch: 17, batch: 32, training loss: 2.067606
Epoch: 17, batch: 33, training loss: 2.617628
Epoch: 17, batch: 34, training loss: 2.546668
Epoch: 17, batch: 35, training loss: 2.392082
Epoch: 17, batch: 36, training loss: 2.289093
Epoch: 17, batch: 37, training loss: 2.196127
Epoch: 17, batch: 38, training loss: 2.350012
Epoch: 17, batch: 39, training loss: 2.731715
Epoch: 17, batch: 40, training loss: 1.847130
Epoch: 17, batch: 41, training loss: 2.246849
Epoch: 17, batch: 42, training loss: 2.365313
Epoch: 17, batch: 43, training loss: 1.958858
Epoch: 17, batch: 44, training loss: 2.479950
Epoch: 17, batch: 45, training loss: 2.441808
Epoch: 17, batch: 46, training loss: 2.326780
Epoch: 17, batch: 47, training loss: 2.228861
Epoch: 17, batch: 48, training loss: 2.037772
Epoch: 17, batch: 49, training loss: 2.164819
Epoch: 18, batch: 0, training loss: 2.194899
Epoch: 18, batch: 1, training loss: 2.374589
Epoch: 18, batch: 2, training loss: 1.570190
Epoch: 18, batch: 3, training loss: 2.218872
Epoch: 18, batch: 4, training loss: 1.201676
Epoch: 18, batch: 5, training loss: 1.373590
Epoch: 18, batch: 6, training loss: 1.207152
Epoch: 18, batch: 7, training loss: 1.190829
Epoch: 18, batch: 8, training loss: 1.309494
Epoch: 18, batch: 9, training loss: 1.368786
Epoch: 18, batch: 10, training loss: 1.731138
Epoch: 18, batch: 11, training loss: 2.085361
Epoch: 18, batch: 12, training loss: 1.790825
Epoch: 18, batch: 13, training loss: 2.184085
Epoch: 18, batch: 14, training loss: 1.963250
Epoch: 18, batch: 15, training loss: 2.339702
Epoch: 18, batch: 16, training loss: 2.078913
Epoch: 18, batch: 17, training loss: 2.135588
Epoch: 18, batch: 18, training loss: 1.944576
Epoch: 18, batch: 19, training loss: 1.556666
Epoch: 18, batch: 20, training loss: 2.175996
Epoch: 18, batch: 21, training loss: 1.512011
Epoch: 18, batch: 22, training loss: 2.097802
Epoch: 18, batch: 23, training loss: 1.961105
Epoch: 18, batch: 24, training loss: 2.035411
Epoch: 18, batch: 25, training loss: 2.285445
Epoch: 18, batch: 26, training loss: 2.306790
Epoch: 18, batch: 27, training loss: 2.420551
Epoch: 18, batch: 28, training loss: 2.627110
Epoch: 18, batch: 29, training loss: 2.389977
Epoch: 18, batch: 30, training loss: 2.513117
Epoch: 18, batch: 31, training loss: 2.069473
Epoch: 18, batch: 32, training loss: 2.023415
Epoch: 18, batch: 33, training loss: 2.536745
Epoch: 18, batch: 34, training loss: 2.433990
Epoch: 18, batch: 35, training loss: 2.337635
Epoch: 18, batch: 36, training loss: 2.249610
Epoch: 18, batch: 37, training loss: 2.137689
Epoch: 18, batch: 38, training loss: 2.323257
Epoch: 18, batch: 39, training loss: 2.695550
Epoch: 18, batch: 40, training loss: 1.822647
Epoch: 18, batch: 41, training loss: 2.205685
Epoch: 18, batch: 42, training loss: 2.314721
Epoch: 18, batch: 43, training loss: 1.906159
Epoch: 18, batch: 44, training loss: 2.421402
Epoch: 18, batch: 45, training loss: 2.382426
Epoch: 18, batch: 46, training loss: 2.304083
Epoch: 18, batch: 47, training loss: 2.210157
Epoch: 18, batch: 48, training loss: 1.991806
Epoch: 18, batch: 49, training loss: 2.109327
Epoch: 19, batch: 0, training loss: 2.136348
Epoch: 19, batch: 1, training loss: 2.322155
Epoch: 19, batch: 2, training loss: 1.517707
Epoch: 19, batch: 3, training loss: 2.143698
Epoch: 19, batch: 4, training loss: 1.166343
Epoch: 19, batch: 5, training loss: 1.341800
Epoch: 19, batch: 6, training loss: 1.166671
Epoch: 19, batch: 7, training loss: 1.190680
Epoch: 19, batch: 8, training loss: 1.285998
Epoch: 19, batch: 9, training loss: 1.297802
Epoch: 19, batch: 10, training loss: 1.712875
Epoch: 19, batch: 11, training loss: 2.052095
Epoch: 19, batch: 12, training loss: 1.763771
Epoch: 19, batch: 13, training loss: 2.102911
Epoch: 19, batch: 14, training loss: 1.893158
Epoch: 19, batch: 15, training loss: 2.323054
Epoch: 19, batch: 16, training loss: 2.053019
Epoch: 19, batch: 17, training loss: 2.131397
Epoch: 19, batch: 18, training loss: 1.904005
Epoch: 19, batch: 19, training loss: 1.518755
Epoch: 19, batch: 20, training loss: 2.090332
Epoch: 19, batch: 21, training loss: 1.474018
Epoch: 19, batch: 22, training loss: 2.053752
Epoch: 19, batch: 23, training loss: 1.934740
Epoch: 19, batch: 24, training loss: 2.014076
Epoch: 19, batch: 25, training loss: 2.261368
Epoch: 19, batch: 26, training loss: 2.269581
Epoch: 19, batch: 27, training loss: 2.384068
Epoch: 19, batch: 28, training loss: 2.579190
Epoch: 19, batch: 29, training loss: 2.333136
Epoch: 19, batch: 30, training loss: 2.464965
Epoch: 19, batch: 31, training loss: 2.052218
Epoch: 19, batch: 32, training loss: 2.006013
Epoch: 19, batch: 33, training loss: 2.513648
Epoch: 19, batch: 34, training loss: 2.398821
Epoch: 19, batch: 35, training loss: 2.310554
Epoch: 19, batch: 36, training loss: 2.179002
Epoch: 19, batch: 37, training loss: 2.078149
Epoch: 19, batch: 38, training loss: 2.251667
Epoch: 19, batch: 39, training loss: 2.622403
Epoch: 19, batch: 40, training loss: 1.766758
Epoch: 19, batch: 41, training loss: 2.183937
Epoch: 19, batch: 42, training loss: 2.280139
Epoch: 19, batch: 43, training loss: 1.847955
Epoch: 19, batch: 44, training loss: 2.338451
Epoch: 19, batch: 45, training loss: 2.318156
Epoch: 19, batch: 46, training loss: 2.235868
Epoch: 19, batch: 47, training loss: 2.130334
Epoch: 19, batch: 48, training loss: 1.951859
Epoch: 19, batch: 49, training loss: 2.053405
Epoch: 20, batch: 0, training loss: 2.124064
Epoch: 20, batch: 1, training loss: 2.273793
Epoch: 20, batch: 2, training loss: 1.455307
Epoch: 20, batch: 3, training loss: 2.061871
Epoch: 20, batch: 4, training loss: 1.115925
Epoch: 20, batch: 5, training loss: 1.288826
Epoch: 20, batch: 6, training loss: 1.117771
Epoch: 20, batch: 7, training loss: 1.142457
Epoch: 20, batch: 8, training loss: 1.262381
Epoch: 20, batch: 9, training loss: 1.298808
Epoch: 20, batch: 10, training loss: 1.670714
Epoch: 20, batch: 11, training loss: 2.033023
Epoch: 20, batch: 12, training loss: 1.737416
Epoch: 20, batch: 13, training loss: 2.057651
Epoch: 20, batch: 14, training loss: 1.840629
Epoch: 20, batch: 15, training loss: 2.242640
Epoch: 20, batch: 16, training loss: 1.976386
Epoch: 20, batch: 17, training loss: 2.081075
Epoch: 20, batch: 18, training loss: 1.886723
Epoch: 20, batch: 19, training loss: 1.517357
Epoch: 20, batch: 20, training loss: 2.080943
Epoch: 20, batch: 21, training loss: 1.401280
Epoch: 20, batch: 22, training loss: 1.981115
Epoch: 20, batch: 23, training loss: 1.856800
Epoch: 20, batch: 24, training loss: 1.956422
Epoch: 20, batch: 25, training loss: 2.163213
Epoch: 20, batch: 26, training loss: 2.206447
Epoch: 20, batch: 27, training loss: 2.335749
Epoch: 20, batch: 28, training loss: 2.539852
Epoch: 20, batch: 29, training loss: 2.290082
Epoch: 20, batch: 30, training loss: 2.408697
Epoch: 20, batch: 31, training loss: 2.008501
Epoch: 20, batch: 32, training loss: 1.960851
Epoch: 20, batch: 33, training loss: 2.393832
Epoch: 20, batch: 34, training loss: 2.342941
Epoch: 20, batch: 35, training loss: 2.259633
Epoch: 20, batch: 36, training loss: 2.146794
Epoch: 20, batch: 37, training loss: 2.036751
Epoch: 20, batch: 38, training loss: 2.222585
Epoch: 20, batch: 39, training loss: 2.539212
Epoch: 20, batch: 40, training loss: 1.707841
Epoch: 20, batch: 41, training loss: 2.124670
Epoch: 20, batch: 42, training loss: 2.229777
Epoch: 20, batch: 43, training loss: 1.818295
Epoch: 20, batch: 44, training loss: 2.305242
Epoch: 20, batch: 45, training loss: 2.279989
Epoch: 20, batch: 46, training loss: 2.209578
Epoch: 20, batch: 47, training loss: 2.084154
Epoch: 20, batch: 48, training loss: 1.898148
Epoch: 20, batch: 49, training loss: 2.014140
Epoch: 21, batch: 0, training loss: 2.038398
Epoch: 21, batch: 1, training loss: 2.194334
Epoch: 21, batch: 2, training loss: 1.446286
Epoch: 21, batch: 3, training loss: 2.013217
Epoch: 21, batch: 4, training loss: 1.095601
Epoch: 21, batch: 5, training loss: 1.251459
Epoch: 21, batch: 6, training loss: 1.042170
Epoch: 21, batch: 7, training loss: 1.065646
Epoch: 21, batch: 8, training loss: 1.178879
Epoch: 21, batch: 9, training loss: 1.183688
Epoch: 21, batch: 10, training loss: 1.584882
Epoch: 21, batch: 11, training loss: 1.920742
Epoch: 21, batch: 12, training loss: 1.661015
Epoch: 21, batch: 13, training loss: 1.993948
Epoch: 21, batch: 14, training loss: 1.815456
Epoch: 21, batch: 15, training loss: 2.192739
Epoch: 21, batch: 16, training loss: 1.918920
Epoch: 21, batch: 17, training loss: 2.004226
Epoch: 21, batch: 18, training loss: 1.816908
Epoch: 21, batch: 19, training loss: 1.472294
Epoch: 21, batch: 20, training loss: 1.989301
Epoch: 21, batch: 21, training loss: 1.316204
Epoch: 21, batch: 22, training loss: 1.906476
Epoch: 21, batch: 23, training loss: 1.808213
Epoch: 21, batch: 24, training loss: 1.898880
Epoch: 21, batch: 25, training loss: 2.080953
Epoch: 21, batch: 26, training loss: 2.113841
Epoch: 21, batch: 27, training loss: 2.248359
Epoch: 21, batch: 28, training loss: 2.420359
Epoch: 21, batch: 29, training loss: 2.196162
Epoch: 21, batch: 30, training loss: 2.341485
Epoch: 21, batch: 31, training loss: 1.934296
Epoch: 21, batch: 32, training loss: 1.913632
Epoch: 21, batch: 33, training loss: 2.288088
Epoch: 21, batch: 34, training loss: 2.210681
Epoch: 21, batch: 35, training loss: 2.187169
Epoch: 21, batch: 36, training loss: 2.086169
Epoch: 21, batch: 37, training loss: 1.971698
Epoch: 21, batch: 38, training loss: 2.164374
Epoch: 21, batch: 39, training loss: 2.456009
Epoch: 21, batch: 40, training loss: 1.665102
Epoch: 21, batch: 41, training loss: 2.072869
Epoch: 21, batch: 42, training loss: 2.160207
Epoch: 21, batch: 43, training loss: 1.751140
Epoch: 21, batch: 44, training loss: 2.230405
Epoch: 21, batch: 45, training loss: 2.212680
Epoch: 21, batch: 46, training loss: 2.128952
Epoch: 21, batch: 47, training loss: 2.030870
Epoch: 21, batch: 48, training loss: 1.849714
Epoch: 21, batch: 49, training loss: 1.958883
Epoch: 22, batch: 0, training loss: 1.990605
Epoch: 22, batch: 1, training loss: 2.148500
Epoch: 22, batch: 2, training loss: 1.370275
Epoch: 22, batch: 3, training loss: 1.920306
Epoch: 22, batch: 4, training loss: 1.032227
Epoch: 22, batch: 5, training loss: 1.184923
Epoch: 22, batch: 6, training loss: 1.007209
Epoch: 22, batch: 7, training loss: 1.015756
Epoch: 22, batch: 8, training loss: 1.155671
Epoch: 22, batch: 9, training loss: 1.123776
Epoch: 22, batch: 10, training loss: 1.524055
Epoch: 22, batch: 11, training loss: 1.846712
Epoch: 22, batch: 12, training loss: 1.588134
Epoch: 22, batch: 13, training loss: 1.905221
Epoch: 22, batch: 14, training loss: 1.750568
Epoch: 22, batch: 15, training loss: 2.122435
Epoch: 22, batch: 16, training loss: 1.880436
Epoch: 22, batch: 17, training loss: 1.963439
Epoch: 22, batch: 18, training loss: 1.763102
Epoch: 22, batch: 19, training loss: 1.412166
Epoch: 22, batch: 20, training loss: 1.915383
Epoch: 22, batch: 21, training loss: 1.231510
Epoch: 22, batch: 22, training loss: 1.849125
Epoch: 22, batch: 23, training loss: 1.742335
Epoch: 22, batch: 24, training loss: 1.866967
Epoch: 22, batch: 25, training loss: 2.070615
Epoch: 22, batch: 26, training loss: 2.060794
Epoch: 22, batch: 27, training loss: 2.179043
Epoch: 22, batch: 28, training loss: 2.356976
Epoch: 22, batch: 29, training loss: 2.115806
Epoch: 22, batch: 30, training loss: 2.244426
Epoch: 22, batch: 31, training loss: 1.875356
Epoch: 22, batch: 32, training loss: 1.860510
Epoch: 22, batch: 33, training loss: 2.236653
Epoch: 22, batch: 34, training loss: 2.151395
Epoch: 22, batch: 35, training loss: 2.149188
Epoch: 22, batch: 36, training loss: 2.014280
Epoch: 22, batch: 37, training loss: 1.904698
Epoch: 22, batch: 38, training loss: 2.100246
Epoch: 22, batch: 39, training loss: 2.385765
Epoch: 22, batch: 40, training loss: 1.616489
Epoch: 22, batch: 41, training loss: 2.018201
Epoch: 22, batch: 42, training loss: 2.123428
Epoch: 22, batch: 43, training loss: 1.727348
Epoch: 22, batch: 44, training loss: 2.179125
Epoch: 22, batch: 45, training loss: 2.153832
Epoch: 22, batch: 46, training loss: 2.088258
Epoch: 22, batch: 47, training loss: 1.940084
Epoch: 22, batch: 48, training loss: 1.793704
Epoch: 22, batch: 49, training loss: 1.900005
Epoch: 23, batch: 0, training loss: 1.949111
Epoch: 23, batch: 1, training loss: 2.101750
Epoch: 23, batch: 2, training loss: 1.333829
Epoch: 23, batch: 3, training loss: 1.880866
Epoch: 23, batch: 4, training loss: 0.991780
Epoch: 23, batch: 5, training loss: 1.125299
Epoch: 23, batch: 6, training loss: 0.967281
Epoch: 23, batch: 7, training loss: 0.996471
Epoch: 23, batch: 8, training loss: 1.116724
Epoch: 23, batch: 9, training loss: 1.075268
Epoch: 23, batch: 10, training loss: 1.480008
Epoch: 23, batch: 11, training loss: 1.786701
Epoch: 23, batch: 12, training loss: 1.568273
Epoch: 23, batch: 13, training loss: 1.846457
Epoch: 23, batch: 14, training loss: 1.702607
Epoch: 23, batch: 15, training loss: 2.048405
Epoch: 23, batch: 16, training loss: 1.819949
Epoch: 23, batch: 17, training loss: 1.906798
Epoch: 23, batch: 18, training loss: 1.756909
Epoch: 23, batch: 19, training loss: 1.390235
Epoch: 23, batch: 20, training loss: 1.908777
Epoch: 23, batch: 21, training loss: 1.221299
Epoch: 23, batch: 22, training loss: 1.793081
Epoch: 23, batch: 23, training loss: 1.695307
Epoch: 23, batch: 24, training loss: 1.800376
Epoch: 23, batch: 25, training loss: 2.024561
Epoch: 23, batch: 26, training loss: 2.023030
Epoch: 23, batch: 27, training loss: 2.155469
Epoch: 23, batch: 28, training loss: 2.337230
Epoch: 23, batch: 29, training loss: 2.079407
Epoch: 23, batch: 30, training loss: 2.190517
Epoch: 23, batch: 31, training loss: 1.830251
Epoch: 23, batch: 32, training loss: 1.818617
Epoch: 23, batch: 33, training loss: 2.108364
Epoch: 23, batch: 34, training loss: 2.040401
Epoch: 23, batch: 35, training loss: 2.110105
Epoch: 23, batch: 36, training loss: 1.989304
Epoch: 23, batch: 37, training loss: 1.884546
Epoch: 23, batch: 38, training loss: 2.067959
Epoch: 23, batch: 39, training loss: 2.336587
Epoch: 23, batch: 40, training loss: 1.570738
Epoch: 23, batch: 41, training loss: 1.981412
Epoch: 23, batch: 42, training loss: 2.065920
Epoch: 23, batch: 43, training loss: 1.670008
Epoch: 23, batch: 44, training loss: 2.120315
Epoch: 23, batch: 45, training loss: 2.113742
Epoch: 23, batch: 46, training loss: 2.051870
Epoch: 23, batch: 47, training loss: 1.922447
Epoch: 23, batch: 48, training loss: 1.764656
Epoch: 23, batch: 49, training loss: 1.873067
Epoch: 24, batch: 0, training loss: 1.907852
Epoch: 24, batch: 1, training loss: 2.029933
Epoch: 24, batch: 2, training loss: 1.311055
Epoch: 24, batch: 3, training loss: 1.824047
Epoch: 24, batch: 4, training loss: 0.965010
Epoch: 24, batch: 5, training loss: 1.103412
Epoch: 24, batch: 6, training loss: 0.945694
Epoch: 24, batch: 7, training loss: 0.954059
Epoch: 24, batch: 8, training loss: 1.085624
Epoch: 24, batch: 9, training loss: 1.072188
Epoch: 24, batch: 10, training loss: 1.444819
Epoch: 24, batch: 11, training loss: 1.732611
Epoch: 24, batch: 12, training loss: 1.519373
Epoch: 24, batch: 13, training loss: 1.806100
Epoch: 24, batch: 14, training loss: 1.655997
Epoch: 24, batch: 15, training loss: 1.985376
Epoch: 24, batch: 16, training loss: 1.781809
Epoch: 24, batch: 17, training loss: 1.861115
Epoch: 24, batch: 18, training loss: 1.696391
Epoch: 24, batch: 19, training loss: 1.369521
Epoch: 24, batch: 20, training loss: 1.866716
Epoch: 24, batch: 21, training loss: 1.198686
Epoch: 24, batch: 22, training loss: 1.775117
Epoch: 24, batch: 23, training loss: 1.656047
Epoch: 24, batch: 24, training loss: 1.759518
Epoch: 24, batch: 25, training loss: 1.937997
Epoch: 24, batch: 26, training loss: 1.948211
Epoch: 24, batch: 27, training loss: 2.102567
Epoch: 24, batch: 28, training loss: 2.292252
Epoch: 24, batch: 29, training loss: 2.035243
Epoch: 24, batch: 30, training loss: 2.181696
Epoch: 24, batch: 31, training loss: 1.804197
Epoch: 24, batch: 32, training loss: 1.812730
Epoch: 24, batch: 33, training loss: 2.053727
Epoch: 24, batch: 34, training loss: 2.002633
Epoch: 24, batch: 35, training loss: 2.048750
Epoch: 24, batch: 36, training loss: 1.939225
Epoch: 24, batch: 37, training loss: 1.867859
Epoch: 24, batch: 38, training loss: 2.036772
Epoch: 24, batch: 39, training loss: 2.326028
Epoch: 24, batch: 40, training loss: 1.558990
Epoch: 24, batch: 41, training loss: 1.932703
Epoch: 24, batch: 42, training loss: 2.029266
Epoch: 24, batch: 43, training loss: 1.638586
Epoch: 24, batch: 44, training loss: 2.055908
Epoch: 24, batch: 45, training loss: 2.062425
Epoch: 24, batch: 46, training loss: 2.017846
Epoch: 24, batch: 47, training loss: 1.883226
Epoch: 24, batch: 48, training loss: 1.725026
Epoch: 24, batch: 49, training loss: 1.847508
Epoch: 25, batch: 0, training loss: 1.900776
Epoch: 25, batch: 1, training loss: 2.009098
Epoch: 25, batch: 2, training loss: 1.300216
Epoch: 25, batch: 3, training loss: 1.789733
Epoch: 25, batch: 4, training loss: 0.940384
Epoch: 25, batch: 5, training loss: 1.078041
Epoch: 25, batch: 6, training loss: 0.914892
Epoch: 25, batch: 7, training loss: 0.938451
Epoch: 25, batch: 8, training loss: 1.067343
Epoch: 25, batch: 9, training loss: 1.044354
Epoch: 25, batch: 10, training loss: 1.432457
Epoch: 25, batch: 11, training loss: 1.745844
Epoch: 25, batch: 12, training loss: 1.516123
Epoch: 25, batch: 13, training loss: 1.801006
Epoch: 25, batch: 14, training loss: 1.638502
Epoch: 25, batch: 15, training loss: 1.944318
Epoch: 25, batch: 16, training loss: 1.740457
Epoch: 25, batch: 17, training loss: 1.820962
Epoch: 25, batch: 18, training loss: 1.670306
Epoch: 25, batch: 19, training loss: 1.340469
Epoch: 25, batch: 20, training loss: 1.800928
Epoch: 25, batch: 21, training loss: 1.119622
Epoch: 25, batch: 22, training loss: 1.723975
Epoch: 25, batch: 23, training loss: 1.619751
Epoch: 25, batch: 24, training loss: 1.742839
Epoch: 25, batch: 25, training loss: 1.909474
Epoch: 25, batch: 26, training loss: 1.912935
Epoch: 25, batch: 27, training loss: 2.076495
Epoch: 25, batch: 28, training loss: 2.245267
Epoch: 25, batch: 29, training loss: 2.025192
Epoch: 25, batch: 30, training loss: 2.135411
Epoch: 25, batch: 31, training loss: 1.778939
Epoch: 25, batch: 32, training loss: 1.767588
Epoch: 25, batch: 33, training loss: 2.019145
Epoch: 25, batch: 34, training loss: 1.939705
Epoch: 25, batch: 35, training loss: 2.014257
Epoch: 25, batch: 36, training loss: 1.885863
Epoch: 25, batch: 37, training loss: 1.794846
Epoch: 25, batch: 38, training loss: 1.992794
Epoch: 25, batch: 39, training loss: 2.279543
Epoch: 25, batch: 40, training loss: 1.510763
Epoch: 25, batch: 41, training loss: 1.916419
Epoch: 25, batch: 42, training loss: 2.043787
Epoch: 25, batch: 43, training loss: 1.636731
Epoch: 25, batch: 44, training loss: 2.042963
Epoch: 25, batch: 45, training loss: 2.030308
Epoch: 25, batch: 46, training loss: 1.958233
Epoch: 25, batch: 47, training loss: 1.821708
Epoch: 25, batch: 48, training loss: 1.688526
Epoch: 25, batch: 49, training loss: 1.817316
Epoch: 26, batch: 0, training loss: 1.871327
Epoch: 26, batch: 1, training loss: 1.984095
Epoch: 26, batch: 2, training loss: 1.297120
Epoch: 26, batch: 3, training loss: 1.775678
Epoch: 26, batch: 4, training loss: 0.922971
Epoch: 26, batch: 5, training loss: 1.042365
Epoch: 26, batch: 6, training loss: 0.880393
Epoch: 26, batch: 7, training loss: 0.921330
Epoch: 26, batch: 8, training loss: 1.030051
Epoch: 26, batch: 9, training loss: 1.011866
Epoch: 26, batch: 10, training loss: 1.384919
Epoch: 26, batch: 11, training loss: 1.717153
Epoch: 26, batch: 12, training loss: 1.514678
Epoch: 26, batch: 13, training loss: 1.776479
Epoch: 26, batch: 14, training loss: 1.629136
Epoch: 26, batch: 15, training loss: 1.945193
Epoch: 26, batch: 16, training loss: 1.731180
Epoch: 26, batch: 17, training loss: 1.787287
Epoch: 26, batch: 18, training loss: 1.649336
Epoch: 26, batch: 19, training loss: 1.315021
Epoch: 26, batch: 20, training loss: 1.790186
Epoch: 26, batch: 21, training loss: 1.105718
Epoch: 26, batch: 22, training loss: 1.671145
Epoch: 26, batch: 23, training loss: 1.580873
Epoch: 26, batch: 24, training loss: 1.695378
Epoch: 26, batch: 25, training loss: 1.849986
Epoch: 26, batch: 26, training loss: 1.879410
Epoch: 26, batch: 27, training loss: 2.027567
Epoch: 26, batch: 28, training loss: 2.209433
Epoch: 26, batch: 29, training loss: 1.974163
Epoch: 26, batch: 30, training loss: 2.115417
Epoch: 26, batch: 31, training loss: 1.738732
Epoch: 26, batch: 32, training loss: 1.746351
Epoch: 26, batch: 33, training loss: 2.010311
Epoch: 26, batch: 34, training loss: 1.881727
Epoch: 26, batch: 35, training loss: 1.960317
Epoch: 26, batch: 36, training loss: 1.849469
Epoch: 26, batch: 37, training loss: 1.766241
Epoch: 26, batch: 38, training loss: 1.945575
Epoch: 26, batch: 39, training loss: 2.222775
Epoch: 26, batch: 40, training loss: 1.469612
Epoch: 26, batch: 41, training loss: 1.870874
Epoch: 26, batch: 42, training loss: 1.976816
Epoch: 26, batch: 43, training loss: 1.578527
Epoch: 26, batch: 44, training loss: 1.978152
Epoch: 26, batch: 45, training loss: 2.008642
Epoch: 26, batch: 46, training loss: 1.943433
Epoch: 26, batch: 47, training loss: 1.803708
Epoch: 26, batch: 48, training loss: 1.627241
Epoch: 26, batch: 49, training loss: 1.770295
Epoch: 27, batch: 0, training loss: 1.823478
Epoch: 27, batch: 1, training loss: 1.935977
Epoch: 27, batch: 2, training loss: 1.237334
Epoch: 27, batch: 3, training loss: 1.720472
Epoch: 27, batch: 4, training loss: 0.903349
Epoch: 27, batch: 5, training loss: 1.017779
Epoch: 27, batch: 6, training loss: 0.862876
Epoch: 27, batch: 7, training loss: 0.899074
Epoch: 27, batch: 8, training loss: 1.001738
Epoch: 27, batch: 9, training loss: 0.983386
Epoch: 27, batch: 10, training loss: 1.350222
Epoch: 27, batch: 11, training loss: 1.659276
Epoch: 27, batch: 12, training loss: 1.459749
Epoch: 27, batch: 13, training loss: 1.724584
Epoch: 27, batch: 14, training loss: 1.602392
Epoch: 27, batch: 15, training loss: 1.911916
Epoch: 27, batch: 16, training loss: 1.678313
Epoch: 27, batch: 17, training loss: 1.754831
Epoch: 27, batch: 18, training loss: 1.587552
Epoch: 27, batch: 19, training loss: 1.295491
Epoch: 27, batch: 20, training loss: 1.758737
Epoch: 27, batch: 21, training loss: 1.076929
Epoch: 27, batch: 22, training loss: 1.640749
Epoch: 27, batch: 23, training loss: 1.533033
Epoch: 27, batch: 24, training loss: 1.669038
Epoch: 27, batch: 25, training loss: 1.815862
Epoch: 27, batch: 26, training loss: 1.841411
Epoch: 27, batch: 27, training loss: 1.980130
Epoch: 27, batch: 28, training loss: 2.153485
Epoch: 27, batch: 29, training loss: 1.904020
Epoch: 27, batch: 30, training loss: 2.041627
Epoch: 27, batch: 31, training loss: 1.669838
Epoch: 27, batch: 32, training loss: 1.705058
Epoch: 27, batch: 33, training loss: 1.928521
Epoch: 27, batch: 34, training loss: 1.814427
Epoch: 27, batch: 35, training loss: 1.933531
Epoch: 27, batch: 36, training loss: 1.836709
Epoch: 27, batch: 37, training loss: 1.730805
Epoch: 27, batch: 38, training loss: 1.905447
Epoch: 27, batch: 39, training loss: 2.171231
Epoch: 27, batch: 40, training loss: 1.420469
Epoch: 27, batch: 41, training loss: 1.837842
Epoch: 27, batch: 42, training loss: 1.931966
Epoch: 27, batch: 43, training loss: 1.547438
Epoch: 27, batch: 44, training loss: 1.971063
Epoch: 27, batch: 45, training loss: 1.949161
Epoch: 27, batch: 46, training loss: 1.898014
Epoch: 27, batch: 47, training loss: 1.782095
Epoch: 27, batch: 48, training loss: 1.605267
Epoch: 27, batch: 49, training loss: 1.747063
Epoch: 28, batch: 0, training loss: 1.779690
Epoch: 28, batch: 1, training loss: 1.886041
Epoch: 28, batch: 2, training loss: 1.211091
Epoch: 28, batch: 3, training loss: 1.674095
Epoch: 28, batch: 4, training loss: 0.875875
Epoch: 28, batch: 5, training loss: 1.003828
Epoch: 28, batch: 6, training loss: 0.857966
Epoch: 28, batch: 7, training loss: 0.880624
Epoch: 28, batch: 8, training loss: 0.988283
Epoch: 28, batch: 9, training loss: 0.953611
Epoch: 28, batch: 10, training loss: 1.336835
Epoch: 28, batch: 11, training loss: 1.646542
Epoch: 28, batch: 12, training loss: 1.445675
Epoch: 28, batch: 13, training loss: 1.667130
Epoch: 28, batch: 14, training loss: 1.555633
Epoch: 28, batch: 15, training loss: 1.859254
Epoch: 28, batch: 16, training loss: 1.646209
Epoch: 28, batch: 17, training loss: 1.745608
Epoch: 28, batch: 18, training loss: 1.603692
Epoch: 28, batch: 19, training loss: 1.275630
Epoch: 28, batch: 20, training loss: 1.729008
Epoch: 28, batch: 21, training loss: 1.064200
Epoch: 28, batch: 22, training loss: 1.593881
Epoch: 28, batch: 23, training loss: 1.499177
Epoch: 28, batch: 24, training loss: 1.638225
Epoch: 28, batch: 25, training loss: 1.791840
Epoch: 28, batch: 26, training loss: 1.812464
Epoch: 28, batch: 27, training loss: 1.954333
Epoch: 28, batch: 28, training loss: 2.103193
Epoch: 28, batch: 29, training loss: 1.895017
Epoch: 28, batch: 30, training loss: 2.018373
Epoch: 28, batch: 31, training loss: 1.658385
Epoch: 28, batch: 32, training loss: 1.663407
Epoch: 28, batch: 33, training loss: 1.880454
Epoch: 28, batch: 34, training loss: 1.777361
Epoch: 28, batch: 35, training loss: 1.885918
Epoch: 28, batch: 36, training loss: 1.779624
Epoch: 28, batch: 37, training loss: 1.707565
Epoch: 28, batch: 38, training loss: 1.872212
Epoch: 28, batch: 39, training loss: 2.149027
Epoch: 28, batch: 40, training loss: 1.394210
Epoch: 28, batch: 41, training loss: 1.811105
Epoch: 28, batch: 42, training loss: 1.918309
Epoch: 28, batch: 43, training loss: 1.542413
Epoch: 28, batch: 44, training loss: 1.933307
Epoch: 28, batch: 45, training loss: 1.922634
Epoch: 28, batch: 46, training loss: 1.873722
Epoch: 28, batch: 47, training loss: 1.752328
Epoch: 28, batch: 48, training loss: 1.569194
Epoch: 28, batch: 49, training loss: 1.718435
Epoch: 29, batch: 0, training loss: 1.761863
Epoch: 29, batch: 1, training loss: 1.867440
Epoch: 29, batch: 2, training loss: 1.191217
Epoch: 29, batch: 3, training loss: 1.672153
Epoch: 29, batch: 4, training loss: 0.884147
Epoch: 29, batch: 5, training loss: 0.979362
Epoch: 29, batch: 6, training loss: 0.866542
Epoch: 29, batch: 7, training loss: 0.840938
Epoch: 29, batch: 8, training loss: 0.980981
Epoch: 29, batch: 9, training loss: 0.928786
Epoch: 29, batch: 10, training loss: 1.321319
Epoch: 29, batch: 11, training loss: 1.632288
Epoch: 29, batch: 12, training loss: 1.408809
Epoch: 29, batch: 13, training loss: 1.644022
Epoch: 29, batch: 14, training loss: 1.529601
Epoch: 29, batch: 15, training loss: 1.848085
Epoch: 29, batch: 16, training loss: 1.636188
Epoch: 29, batch: 17, training loss: 1.715873
Epoch: 29, batch: 18, training loss: 1.558508
Epoch: 29, batch: 19, training loss: 1.261775
Epoch: 29, batch: 20, training loss: 1.732414
Epoch: 29, batch: 21, training loss: 1.078878
Epoch: 29, batch: 22, training loss: 1.616935
Epoch: 29, batch: 23, training loss: 1.496631
Epoch: 29, batch: 24, training loss: 1.596201
Epoch: 29, batch: 25, training loss: 1.738958
Epoch: 29, batch: 26, training loss: 1.791336
Epoch: 29, batch: 27, training loss: 1.914982
Epoch: 29, batch: 28, training loss: 2.086932
Epoch: 29, batch: 29, training loss: 1.860330
Epoch: 29, batch: 30, training loss: 2.010885
Epoch: 29, batch: 31, training loss: 1.634835
Epoch: 29, batch: 32, training loss: 1.661500
Epoch: 29, batch: 33, training loss: 1.884417
Epoch: 29, batch: 34, training loss: 1.786803
Epoch: 29, batch: 35, training loss: 1.848049
Epoch: 29, batch: 36, training loss: 1.766580
Epoch: 29, batch: 37, training loss: 1.680099
Epoch: 29, batch: 38, training loss: 1.850798
Epoch: 29, batch: 39, training loss: 2.125179
Epoch: 29, batch: 40, training loss: 1.392990
Epoch: 29, batch: 41, training loss: 1.790267
Epoch: 29, batch: 42, training loss: 1.918503
Epoch: 29, batch: 43, training loss: 1.527545
Epoch: 29, batch: 44, training loss: 1.914193
Epoch: 29, batch: 45, training loss: 1.908275
Epoch: 29, batch: 46, training loss: 1.862248
Epoch: 29, batch: 47, training loss: 1.732141
Epoch: 29, batch: 48, training loss: 1.573699
Epoch: 29, batch: 49, training loss: 1.694304
Epoch: 30, batch: 0, training loss: 1.768223
Epoch: 30, batch: 1, training loss: 1.851138
Epoch: 30, batch: 2, training loss: 1.172780
Epoch: 30, batch: 3, training loss: 1.659329
Epoch: 30, batch: 4, training loss: 0.863788
Epoch: 30, batch: 5, training loss: 0.982286
Epoch: 30, batch: 6, training loss: 0.818841
Epoch: 30, batch: 7, training loss: 0.854920
Epoch: 30, batch: 8, training loss: 0.955806
Epoch: 30, batch: 9, training loss: 0.942423
Epoch: 30, batch: 10, training loss: 1.323161
Epoch: 30, batch: 11, training loss: 1.613653
Epoch: 30, batch: 12, training loss: 1.395581
Epoch: 30, batch: 13, training loss: 1.634994
Epoch: 30, batch: 14, training loss: 1.509615
Epoch: 30, batch: 15, training loss: 1.827415
Epoch: 30, batch: 16, training loss: 1.598065
Epoch: 30, batch: 17, training loss: 1.662922
Epoch: 30, batch: 18, training loss: 1.518463
Epoch: 30, batch: 19, training loss: 1.250985
Epoch: 30, batch: 20, training loss: 1.672363
Epoch: 30, batch: 21, training loss: 1.021117
Epoch: 30, batch: 22, training loss: 1.556142
Epoch: 30, batch: 23, training loss: 1.475642
Epoch: 30, batch: 24, training loss: 1.581929
Epoch: 30, batch: 25, training loss: 1.703402
Epoch: 30, batch: 26, training loss: 1.771662
Epoch: 30, batch: 27, training loss: 1.893116
Epoch: 30, batch: 28, training loss: 2.041393
Epoch: 30, batch: 29, training loss: 1.817128
Epoch: 30, batch: 30, training loss: 1.946281
Epoch: 30, batch: 31, training loss: 1.628656
Epoch: 30, batch: 32, training loss: 1.622933
Epoch: 30, batch: 33, training loss: 1.817557
Epoch: 30, batch: 34, training loss: 1.732276
Epoch: 30, batch: 35, training loss: 1.842562
Epoch: 30, batch: 36, training loss: 1.738172
Epoch: 30, batch: 37, training loss: 1.633406
Epoch: 30, batch: 38, training loss: 1.803337
Epoch: 30, batch: 39, training loss: 2.046695
Epoch: 30, batch: 40, training loss: 1.380606
Epoch: 30, batch: 41, training loss: 1.747314
Epoch: 30, batch: 42, training loss: 1.858745
Epoch: 30, batch: 43, training loss: 1.489795
Epoch: 30, batch: 44, training loss: 1.887778
Epoch: 30, batch: 45, training loss: 1.888449
Epoch: 30, batch: 46, training loss: 1.842528
Epoch: 30, batch: 47, training loss: 1.713896
Epoch: 30, batch: 48, training loss: 1.546922
Epoch: 30, batch: 49, training loss: 1.673028
Epoch: 31, batch: 0, training loss: 1.728382
Epoch: 31, batch: 1, training loss: 1.793104
Epoch: 31, batch: 2, training loss: 1.151161
Epoch: 31, batch: 3, training loss: 1.610449
Epoch: 31, batch: 4, training loss: 0.841994
Epoch: 31, batch: 5, training loss: 0.931755
Epoch: 31, batch: 6, training loss: 0.796169
Epoch: 31, batch: 7, training loss: 0.842477
Epoch: 31, batch: 8, training loss: 0.953663
Epoch: 31, batch: 9, training loss: 0.922757
Epoch: 31, batch: 10, training loss: 1.311086
Epoch: 31, batch: 11, training loss: 1.625470
Epoch: 31, batch: 12, training loss: 1.388475
Epoch: 31, batch: 13, training loss: 1.624013
Epoch: 31, batch: 14, training loss: 1.486155
Epoch: 31, batch: 15, training loss: 1.796813
Epoch: 31, batch: 16, training loss: 1.552672
Epoch: 31, batch: 17, training loss: 1.619097
Epoch: 31, batch: 18, training loss: 1.513121
Epoch: 31, batch: 19, training loss: 1.215910
Epoch: 31, batch: 20, training loss: 1.652834
Epoch: 31, batch: 21, training loss: 0.984715
Epoch: 31, batch: 22, training loss: 1.537401
Epoch: 31, batch: 23, training loss: 1.451554
Epoch: 31, batch: 24, training loss: 1.555292
Epoch: 31, batch: 25, training loss: 1.685183
Epoch: 31, batch: 26, training loss: 1.732869
Epoch: 31, batch: 27, training loss: 1.832769
Epoch: 31, batch: 28, training loss: 1.993507
Epoch: 31, batch: 29, training loss: 1.795674
Epoch: 31, batch: 30, training loss: 1.920299
Epoch: 31, batch: 31, training loss: 1.605757
Epoch: 31, batch: 32, training loss: 1.622973
Epoch: 31, batch: 33, training loss: 1.802779
Epoch: 31, batch: 34, training loss: 1.666158
Epoch: 31, batch: 35, training loss: 1.815924
Epoch: 31, batch: 36, training loss: 1.712134
Epoch: 31, batch: 37, training loss: 1.601679
Epoch: 31, batch: 38, training loss: 1.785483
Epoch: 31, batch: 39, training loss: 2.039132
Epoch: 31, batch: 40, training loss: 1.343886
Epoch: 31, batch: 41, training loss: 1.732878
Epoch: 31, batch: 42, training loss: 1.819428
Epoch: 31, batch: 43, training loss: 1.470136
Epoch: 31, batch: 44, training loss: 1.829319
Epoch: 31, batch: 45, training loss: 1.840789
Epoch: 31, batch: 46, training loss: 1.824425
Epoch: 31, batch: 47, training loss: 1.689384
Epoch: 31, batch: 48, training loss: 1.530930
Epoch: 31, batch: 49, training loss: 1.643482
Epoch: 32, batch: 0, training loss: 1.698664
Epoch: 32, batch: 1, training loss: 1.750080
Epoch: 32, batch: 2, training loss: 1.128494
Epoch: 32, batch: 3, training loss: 1.586491
Epoch: 32, batch: 4, training loss: 0.820419
Epoch: 32, batch: 5, training loss: 0.935416
Epoch: 32, batch: 6, training loss: 0.767076
Epoch: 32, batch: 7, training loss: 0.836652
Epoch: 32, batch: 8, training loss: 0.936551
Epoch: 32, batch: 9, training loss: 0.917158
Epoch: 32, batch: 10, training loss: 1.286175
Epoch: 32, batch: 11, training loss: 1.584134
Epoch: 32, batch: 12, training loss: 1.377895
Epoch: 32, batch: 13, training loss: 1.586417
Epoch: 32, batch: 14, training loss: 1.465774
Epoch: 32, batch: 15, training loss: 1.731028
Epoch: 32, batch: 16, training loss: 1.546248
Epoch: 32, batch: 17, training loss: 1.601565
Epoch: 32, batch: 18, training loss: 1.499145
Epoch: 32, batch: 19, training loss: 1.187818
Epoch: 32, batch: 20, training loss: 1.627092
Epoch: 32, batch: 21, training loss: 0.947100
Epoch: 32, batch: 22, training loss: 1.470903
Epoch: 32, batch: 23, training loss: 1.389485
Epoch: 32, batch: 24, training loss: 1.533756
Epoch: 32, batch: 25, training loss: 1.684140
Epoch: 32, batch: 26, training loss: 1.715254
Epoch: 32, batch: 27, training loss: 1.833372
Epoch: 32, batch: 28, training loss: 1.973375
Epoch: 32, batch: 29, training loss: 1.750574
Epoch: 32, batch: 30, training loss: 1.898286
Epoch: 32, batch: 31, training loss: 1.560704
Epoch: 32, batch: 32, training loss: 1.595452
Epoch: 32, batch: 33, training loss: 1.750547
Epoch: 32, batch: 34, training loss: 1.672800
Epoch: 32, batch: 35, training loss: 1.769927
Epoch: 32, batch: 36, training loss: 1.715564
Epoch: 32, batch: 37, training loss: 1.584453
Epoch: 32, batch: 38, training loss: 1.758588
Epoch: 32, batch: 39, training loss: 1.996796
Epoch: 32, batch: 40, training loss: 1.331448
Epoch: 32, batch: 41, training loss: 1.692726
Epoch: 32, batch: 42, training loss: 1.794992
Epoch: 32, batch: 43, training loss: 1.435314
Epoch: 32, batch: 44, training loss: 1.820094
Epoch: 32, batch: 45, training loss: 1.814398
Epoch: 32, batch: 46, training loss: 1.771456
Epoch: 32, batch: 47, training loss: 1.641260
Epoch: 32, batch: 48, training loss: 1.498556
Epoch: 32, batch: 49, training loss: 1.610134
Epoch: 33, batch: 0, training loss: 1.664814
Epoch: 33, batch: 1, training loss: 1.737665
Epoch: 33, batch: 2, training loss: 1.115386
Epoch: 33, batch: 3, training loss: 1.561327
Epoch: 33, batch: 4, training loss: 0.796659
Epoch: 33, batch: 5, training loss: 0.907552
Epoch: 33, batch: 6, training loss: 0.763116
Epoch: 33, batch: 7, training loss: 0.787502
Epoch: 33, batch: 8, training loss: 0.905824
Epoch: 33, batch: 9, training loss: 0.869102
Epoch: 33, batch: 10, training loss: 1.271151
Epoch: 33, batch: 11, training loss: 1.597260
Epoch: 33, batch: 12, training loss: 1.365158
Epoch: 33, batch: 13, training loss: 1.595082
Epoch: 33, batch: 14, training loss: 1.489089
Epoch: 33, batch: 15, training loss: 1.728568
Epoch: 33, batch: 16, training loss: 1.545852
Epoch: 33, batch: 17, training loss: 1.608239
Epoch: 33, batch: 18, training loss: 1.466000
Epoch: 33, batch: 19, training loss: 1.170313
Epoch: 33, batch: 20, training loss: 1.597684
Epoch: 33, batch: 21, training loss: 0.935926
Epoch: 33, batch: 22, training loss: 1.437423
Epoch: 33, batch: 23, training loss: 1.366991
Epoch: 33, batch: 24, training loss: 1.489604
Epoch: 33, batch: 25, training loss: 1.612502
Epoch: 33, batch: 26, training loss: 1.685693
Epoch: 33, batch: 27, training loss: 1.783227
Epoch: 33, batch: 28, training loss: 1.947537
Epoch: 33, batch: 29, training loss: 1.735165
Epoch: 33, batch: 30, training loss: 1.856665
Epoch: 33, batch: 31, training loss: 1.539371
Epoch: 33, batch: 32, training loss: 1.567381
Epoch: 33, batch: 33, training loss: 1.703058
Epoch: 33, batch: 34, training loss: 1.605162
Epoch: 33, batch: 35, training loss: 1.734142
Epoch: 33, batch: 36, training loss: 1.667937
Epoch: 33, batch: 37, training loss: 1.580654
Epoch: 33, batch: 38, training loss: 1.733677
Epoch: 33, batch: 39, training loss: 1.988301
Epoch: 33, batch: 40, training loss: 1.317310
Epoch: 33, batch: 41, training loss: 1.693925
Epoch: 33, batch: 42, training loss: 1.771107
Epoch: 33, batch: 43, training loss: 1.387880
Epoch: 33, batch: 44, training loss: 1.783444
Epoch: 33, batch: 45, training loss: 1.782594
Epoch: 33, batch: 46, training loss: 1.748378
Epoch: 33, batch: 47, training loss: 1.636464
Epoch: 33, batch: 48, training loss: 1.483282
Epoch: 33, batch: 49, training loss: 1.604357
Epoch: 34, batch: 0, training loss: 1.633395
Epoch: 34, batch: 1, training loss: 1.687623
Epoch: 34, batch: 2, training loss: 1.104027
Epoch: 34, batch: 3, training loss: 1.551223
Epoch: 34, batch: 4, training loss: 0.802536
Epoch: 34, batch: 5, training loss: 0.890564
Epoch: 34, batch: 6, training loss: 0.756864
Epoch: 34, batch: 7, training loss: 0.789919
Epoch: 34, batch: 8, training loss: 0.910985
Epoch: 34, batch: 9, training loss: 0.861250
Epoch: 34, batch: 10, training loss: 1.245676
Epoch: 34, batch: 11, training loss: 1.563854
Epoch: 34, batch: 12, training loss: 1.364509
Epoch: 34, batch: 13, training loss: 1.582003
Epoch: 34, batch: 14, training loss: 1.451185
Epoch: 34, batch: 15, training loss: 1.720011
Epoch: 34, batch: 16, training loss: 1.536403
Epoch: 34, batch: 17, training loss: 1.592845
Epoch: 34, batch: 18, training loss: 1.457112
Epoch: 34, batch: 19, training loss: 1.168210
Epoch: 34, batch: 20, training loss: 1.594766
Epoch: 34, batch: 21, training loss: 0.937278
Epoch: 34, batch: 22, training loss: 1.457052
Epoch: 34, batch: 23, training loss: 1.350268
Epoch: 34, batch: 24, training loss: 1.480542
Epoch: 34, batch: 25, training loss: 1.611796
Epoch: 34, batch: 26, training loss: 1.675704
Epoch: 34, batch: 27, training loss: 1.748348
Epoch: 34, batch: 28, training loss: 1.917968
Epoch: 34, batch: 29, training loss: 1.709763
Epoch: 34, batch: 30, training loss: 1.819400
Epoch: 34, batch: 31, training loss: 1.525863
Epoch: 34, batch: 32, training loss: 1.543964
Epoch: 34, batch: 33, training loss: 1.711842
Epoch: 34, batch: 34, training loss: 1.581347
Epoch: 34, batch: 35, training loss: 1.719323
Epoch: 34, batch: 36, training loss: 1.653773
Epoch: 34, batch: 37, training loss: 1.556486
Epoch: 34, batch: 38, training loss: 1.706082
Epoch: 34, batch: 39, training loss: 1.959639
Epoch: 34, batch: 40, training loss: 1.311960
Epoch: 34, batch: 41, training loss: 1.662639
Epoch: 34, batch: 42, training loss: 1.771247
Epoch: 34, batch: 43, training loss: 1.398193
Epoch: 34, batch: 44, training loss: 1.750128
Epoch: 34, batch: 45, training loss: 1.769807
Epoch: 34, batch: 46, training loss: 1.737429
Epoch: 34, batch: 47, training loss: 1.596754
Epoch: 34, batch: 48, training loss: 1.465021
Epoch: 34, batch: 49, training loss: 1.585952
Epoch: 35, batch: 0, training loss: 1.638148
Epoch: 35, batch: 1, training loss: 1.702105
Epoch: 35, batch: 2, training loss: 1.096884
Epoch: 35, batch: 3, training loss: 1.527090
Epoch: 35, batch: 4, training loss: 0.790013
Epoch: 35, batch: 5, training loss: 0.891842
Epoch: 35, batch: 6, training loss: 0.751641
Epoch: 35, batch: 7, training loss: 0.757859
Epoch: 35, batch: 8, training loss: 0.887761
Epoch: 35, batch: 9, training loss: 0.855525
Epoch: 35, batch: 10, training loss: 1.247983
Epoch: 35, batch: 11, training loss: 1.534479
Epoch: 35, batch: 12, training loss: 1.328012
Epoch: 35, batch: 13, training loss: 1.530411
Epoch: 35, batch: 14, training loss: 1.441033
Epoch: 35, batch: 15, training loss: 1.674838
Epoch: 35, batch: 16, training loss: 1.501879
Epoch: 35, batch: 17, training loss: 1.567977
Epoch: 35, batch: 18, training loss: 1.445083
Epoch: 35, batch: 19, training loss: 1.145827
Epoch: 35, batch: 20, training loss: 1.544630
Epoch: 35, batch: 21, training loss: 0.934098
Epoch: 35, batch: 22, training loss: 1.434423
Epoch: 35, batch: 23, training loss: 1.314724
Epoch: 35, batch: 24, training loss: 1.467443
Epoch: 35, batch: 25, training loss: 1.565155
Epoch: 35, batch: 26, training loss: 1.627987
Epoch: 35, batch: 27, training loss: 1.735286
Epoch: 35, batch: 28, training loss: 1.882255
Epoch: 35, batch: 29, training loss: 1.685936
Epoch: 35, batch: 30, training loss: 1.794115
Epoch: 35, batch: 31, training loss: 1.503813
Epoch: 35, batch: 32, training loss: 1.542742
Epoch: 35, batch: 33, training loss: 1.640507
Epoch: 35, batch: 34, training loss: 1.534617
Epoch: 35, batch: 35, training loss: 1.686413
Epoch: 35, batch: 36, training loss: 1.625430
Epoch: 35, batch: 37, training loss: 1.538092
Epoch: 35, batch: 38, training loss: 1.692712
Epoch: 35, batch: 39, training loss: 1.940141
Epoch: 35, batch: 40, training loss: 1.306581
Epoch: 35, batch: 41, training loss: 1.658500
Epoch: 35, batch: 42, training loss: 1.727529
Epoch: 35, batch: 43, training loss: 1.381296
Epoch: 35, batch: 44, training loss: 1.761484
Epoch: 35, batch: 45, training loss: 1.756672
Epoch: 35, batch: 46, training loss: 1.712065
Epoch: 35, batch: 47, training loss: 1.605108
Epoch: 35, batch: 48, training loss: 1.444317
Epoch: 35, batch: 49, training loss: 1.547794
Epoch: 36, batch: 0, training loss: 1.583874
Epoch: 36, batch: 1, training loss: 1.652500
Epoch: 36, batch: 2, training loss: 1.080282
Epoch: 36, batch: 3, training loss: 1.499893
Epoch: 36, batch: 4, training loss: 0.765954
Epoch: 36, batch: 5, training loss: 0.882890
Epoch: 36, batch: 6, training loss: 0.739055
Epoch: 36, batch: 7, training loss: 0.761679
Epoch: 36, batch: 8, training loss: 0.870681
Epoch: 36, batch: 9, training loss: 0.823579
Epoch: 36, batch: 10, training loss: 1.243814
Epoch: 36, batch: 11, training loss: 1.519250
Epoch: 36, batch: 12, training loss: 1.339908
Epoch: 36, batch: 13, training loss: 1.535322
Epoch: 36, batch: 14, training loss: 1.421014
Epoch: 36, batch: 15, training loss: 1.692988
Epoch: 36, batch: 16, training loss: 1.474310
Epoch: 36, batch: 17, training loss: 1.560778
Epoch: 36, batch: 18, training loss: 1.419481
Epoch: 36, batch: 19, training loss: 1.154414
Epoch: 36, batch: 20, training loss: 1.543319
Epoch: 36, batch: 21, training loss: 0.902656
Epoch: 36, batch: 22, training loss: 1.385300
Epoch: 36, batch: 23, training loss: 1.310834
Epoch: 36, batch: 24, training loss: 1.440241
Epoch: 36, batch: 25, training loss: 1.548804
Epoch: 36, batch: 26, training loss: 1.630095
Epoch: 36, batch: 27, training loss: 1.704916
Epoch: 36, batch: 28, training loss: 1.839784
Epoch: 36, batch: 29, training loss: 1.670598
Epoch: 36, batch: 30, training loss: 1.785219
Epoch: 36, batch: 31, training loss: 1.461951
Epoch: 36, batch: 32, training loss: 1.506464
Epoch: 36, batch: 33, training loss: 1.613887
Epoch: 36, batch: 34, training loss: 1.478811
Epoch: 36, batch: 35, training loss: 1.678196
Epoch: 36, batch: 36, training loss: 1.593076
Epoch: 36, batch: 37, training loss: 1.495105
Epoch: 36, batch: 38, training loss: 1.668812
Epoch: 36, batch: 39, training loss: 1.897806
Epoch: 36, batch: 40, training loss: 1.246324
Epoch: 36, batch: 41, training loss: 1.606046
Epoch: 36, batch: 42, training loss: 1.692755
Epoch: 36, batch: 43, training loss: 1.354512
Epoch: 36, batch: 44, training loss: 1.699849
Epoch: 36, batch: 45, training loss: 1.735993
Epoch: 36, batch: 46, training loss: 1.694876
Epoch: 36, batch: 47, training loss: 1.571207
Epoch: 36, batch: 48, training loss: 1.431121
Epoch: 36, batch: 49, training loss: 1.527354
Epoch: 37, batch: 0, training loss: 1.553403
Epoch: 37, batch: 1, training loss: 1.610536
Epoch: 37, batch: 2, training loss: 1.044662
Epoch: 37, batch: 3, training loss: 1.493815
Epoch: 37, batch: 4, training loss: 0.758078
Epoch: 37, batch: 5, training loss: 0.846872
Epoch: 37, batch: 6, training loss: 0.716057
Epoch: 37, batch: 7, training loss: 0.744110
Epoch: 37, batch: 8, training loss: 0.862155
Epoch: 37, batch: 9, training loss: 0.807869
Epoch: 37, batch: 10, training loss: 1.219231
Epoch: 37, batch: 11, training loss: 1.503111
Epoch: 37, batch: 12, training loss: 1.312367
Epoch: 37, batch: 13, training loss: 1.531439
Epoch: 37, batch: 14, training loss: 1.399541
Epoch: 37, batch: 15, training loss: 1.646240
Epoch: 37, batch: 16, training loss: 1.463076
Epoch: 37, batch: 17, training loss: 1.519362
Epoch: 37, batch: 18, training loss: 1.407941
Epoch: 37, batch: 19, training loss: 1.109047
Epoch: 37, batch: 20, training loss: 1.503288
Epoch: 37, batch: 21, training loss: 0.861440
Epoch: 37, batch: 22, training loss: 1.353872
Epoch: 37, batch: 23, training loss: 1.277761
Epoch: 37, batch: 24, training loss: 1.438341
Epoch: 37, batch: 25, training loss: 1.523748
Epoch: 37, batch: 26, training loss: 1.573028
Epoch: 37, batch: 27, training loss: 1.674963
Epoch: 37, batch: 28, training loss: 1.798318
Epoch: 37, batch: 29, training loss: 1.603098
Epoch: 37, batch: 30, training loss: 1.736591
Epoch: 37, batch: 31, training loss: 1.451526
Epoch: 37, batch: 32, training loss: 1.481892
Epoch: 37, batch: 33, training loss: 1.578626
Epoch: 37, batch: 34, training loss: 1.489440
Epoch: 37, batch: 35, training loss: 1.632533
Epoch: 37, batch: 36, training loss: 1.565511
Epoch: 37, batch: 37, training loss: 1.502812
Epoch: 37, batch: 38, training loss: 1.647336
Epoch: 37, batch: 39, training loss: 1.856001
Epoch: 37, batch: 40, training loss: 1.230390
Epoch: 37, batch: 41, training loss: 1.597348
Epoch: 37, batch: 42, training loss: 1.670968
Epoch: 37, batch: 43, training loss: 1.317711
Epoch: 37, batch: 44, training loss: 1.688860
Epoch: 37, batch: 45, training loss: 1.701334
Epoch: 37, batch: 46, training loss: 1.671844
Epoch: 37, batch: 47, training loss: 1.537312
Epoch: 37, batch: 48, training loss: 1.383662
Epoch: 37, batch: 49, training loss: 1.501118
Epoch: 38, batch: 0, training loss: 1.530313
Epoch: 38, batch: 1, training loss: 1.614568
Epoch: 38, batch: 2, training loss: 1.055613
Epoch: 38, batch: 3, training loss: 1.481282
Epoch: 38, batch: 4, training loss: 0.720387
Epoch: 38, batch: 5, training loss: 0.824365
Epoch: 38, batch: 6, training loss: 0.700554
Epoch: 38, batch: 7, training loss: 0.722144
Epoch: 38, batch: 8, training loss: 0.825269
Epoch: 38, batch: 9, training loss: 0.819661
Epoch: 38, batch: 10, training loss: 1.197680
Epoch: 38, batch: 11, training loss: 1.501751
Epoch: 38, batch: 12, training loss: 1.290467
Epoch: 38, batch: 13, training loss: 1.517739
Epoch: 38, batch: 14, training loss: 1.386246
Epoch: 38, batch: 15, training loss: 1.645800
Epoch: 38, batch: 16, training loss: 1.432778
Epoch: 38, batch: 17, training loss: 1.505736
Epoch: 38, batch: 18, training loss: 1.377991
Epoch: 38, batch: 19, training loss: 1.095410
Epoch: 38, batch: 20, training loss: 1.467619
Epoch: 38, batch: 21, training loss: 0.849270
Epoch: 38, batch: 22, training loss: 1.334576
Epoch: 38, batch: 23, training loss: 1.272897
Epoch: 38, batch: 24, training loss: 1.401495
Epoch: 38, batch: 25, training loss: 1.531118
Epoch: 38, batch: 26, training loss: 1.569871
Epoch: 38, batch: 27, training loss: 1.657983
Epoch: 38, batch: 28, training loss: 1.780562
Epoch: 38, batch: 29, training loss: 1.592487
Epoch: 38, batch: 30, training loss: 1.721654
Epoch: 38, batch: 31, training loss: 1.410447
Epoch: 38, batch: 32, training loss: 1.466578
Epoch: 38, batch: 33, training loss: 1.544267
Epoch: 38, batch: 34, training loss: 1.432754
Epoch: 38, batch: 35, training loss: 1.614079
Epoch: 38, batch: 36, training loss: 1.539874
Epoch: 38, batch: 37, training loss: 1.464889
Epoch: 38, batch: 38, training loss: 1.643468
Epoch: 38, batch: 39, training loss: 1.843476
Epoch: 38, batch: 40, training loss: 1.210717
Epoch: 38, batch: 41, training loss: 1.584681
Epoch: 38, batch: 42, training loss: 1.665272
Epoch: 38, batch: 43, training loss: 1.329644
Epoch: 38, batch: 44, training loss: 1.664254
Epoch: 38, batch: 45, training loss: 1.694786
Epoch: 38, batch: 46, training loss: 1.651507
Epoch: 38, batch: 47, training loss: 1.498185
Epoch: 38, batch: 48, training loss: 1.376312
Epoch: 38, batch: 49, training loss: 1.480016
Epoch: 39, batch: 0, training loss: 1.530428
Epoch: 39, batch: 1, training loss: 1.601994
Epoch: 39, batch: 2, training loss: 1.033943
Epoch: 39, batch: 3, training loss: 1.460565
Epoch: 39, batch: 4, training loss: 0.721009
Epoch: 39, batch: 5, training loss: 0.805270
Epoch: 39, batch: 6, training loss: 0.688222
Epoch: 39, batch: 7, training loss: 0.725310
Epoch: 39, batch: 8, training loss: 0.839559
Epoch: 39, batch: 9, training loss: 0.808894
Epoch: 39, batch: 10, training loss: 1.175483
Epoch: 39, batch: 11, training loss: 1.459995
Epoch: 39, batch: 12, training loss: 1.280178
Epoch: 39, batch: 13, training loss: 1.489239
Epoch: 39, batch: 14, training loss: 1.380058
Epoch: 39, batch: 15, training loss: 1.611569
Epoch: 39, batch: 16, training loss: 1.406240
Epoch: 39, batch: 17, training loss: 1.507095
Epoch: 39, batch: 18, training loss: 1.364331
Epoch: 39, batch: 19, training loss: 1.095386
Epoch: 39, batch: 20, training loss: 1.456446
Epoch: 39, batch: 21, training loss: 0.837305
Epoch: 39, batch: 22, training loss: 1.310636
Epoch: 39, batch: 23, training loss: 1.248190
Epoch: 39, batch: 24, training loss: 1.386341
Epoch: 39, batch: 25, training loss: 1.467742
Epoch: 39, batch: 26, training loss: 1.545037
Epoch: 39, batch: 27, training loss: 1.615265
Epoch: 39, batch: 28, training loss: 1.771312
Epoch: 39, batch: 29, training loss: 1.578578
Epoch: 39, batch: 30, training loss: 1.711957
Epoch: 39, batch: 31, training loss: 1.401125
Epoch: 39, batch: 32, training loss: 1.456466
Epoch: 39, batch: 33, training loss: 1.476241
Epoch: 39, batch: 34, training loss: 1.418670
Epoch: 39, batch: 35, training loss: 1.593874
Epoch: 39, batch: 36, training loss: 1.515946
Epoch: 39, batch: 37, training loss: 1.453306
Epoch: 39, batch: 38, training loss: 1.616234
Epoch: 39, batch: 39, training loss: 1.788988
Epoch: 39, batch: 40, training loss: 1.197922
Epoch: 39, batch: 41, training loss: 1.573977
Epoch: 39, batch: 42, training loss: 1.642250
Epoch: 39, batch: 43, training loss: 1.285217
Epoch: 39, batch: 44, training loss: 1.660587
Epoch: 39, batch: 45, training loss: 1.671517
Epoch: 39, batch: 46, training loss: 1.644410
Epoch: 39, batch: 47, training loss: 1.485586
Epoch: 39, batch: 48, training loss: 1.368942
Epoch: 39, batch: 49, training loss: 1.468316
Epoch: 40, batch: 0, training loss: 1.501350
Epoch: 40, batch: 1, training loss: 1.556809
Epoch: 40, batch: 2, training loss: 1.001682
Epoch: 40, batch: 3, training loss: 1.416043
Epoch: 40, batch: 4, training loss: 0.694462
Epoch: 40, batch: 5, training loss: 0.794463
Epoch: 40, batch: 6, training loss: 0.692017
Epoch: 40, batch: 7, training loss: 0.701239
Epoch: 40, batch: 8, training loss: 0.807902
Epoch: 40, batch: 9, training loss: 0.785517
Epoch: 40, batch: 10, training loss: 1.151894
Epoch: 40, batch: 11, training loss: 1.432273
Epoch: 40, batch: 12, training loss: 1.259096
Epoch: 40, batch: 13, training loss: 1.447783
Epoch: 40, batch: 14, training loss: 1.355567
Epoch: 40, batch: 15, training loss: 1.600114
Epoch: 40, batch: 16, training loss: 1.409885
Epoch: 40, batch: 17, training loss: 1.458499
Epoch: 40, batch: 18, training loss: 1.353170
Epoch: 40, batch: 19, training loss: 1.077844
Epoch: 40, batch: 20, training loss: 1.439351
Epoch: 40, batch: 21, training loss: 0.840487
Epoch: 40, batch: 22, training loss: 1.334084
Epoch: 40, batch: 23, training loss: 1.210070
Epoch: 40, batch: 24, training loss: 1.373395
Epoch: 40, batch: 25, training loss: 1.461434
Epoch: 40, batch: 26, training loss: 1.547179
Epoch: 40, batch: 27, training loss: 1.620556
Epoch: 40, batch: 28, training loss: 1.737375
Epoch: 40, batch: 29, training loss: 1.558476
Epoch: 40, batch: 30, training loss: 1.675132
Epoch: 40, batch: 31, training loss: 1.379668
Epoch: 40, batch: 32, training loss: 1.413302
Epoch: 40, batch: 33, training loss: 1.476117
Epoch: 40, batch: 34, training loss: 1.407650
Epoch: 40, batch: 35, training loss: 1.570469
Epoch: 40, batch: 36, training loss: 1.513137
Epoch: 40, batch: 37, training loss: 1.441597
Epoch: 40, batch: 38, training loss: 1.582936
Epoch: 40, batch: 39, training loss: 1.797050
Epoch: 40, batch: 40, training loss: 1.218266
Epoch: 40, batch: 41, training loss: 1.551537
Epoch: 40, batch: 42, training loss: 1.632167
Epoch: 40, batch: 43, training loss: 1.268262
Epoch: 40, batch: 44, training loss: 1.643974
Epoch: 40, batch: 45, training loss: 1.642537
Epoch: 40, batch: 46, training loss: 1.611367
Epoch: 40, batch: 47, training loss: 1.486714
Epoch: 40, batch: 48, training loss: 1.341578
Epoch: 40, batch: 49, training loss: 1.453054
Epoch: 41, batch: 0, training loss: 1.465867
Epoch: 41, batch: 1, training loss: 1.558093
Epoch: 41, batch: 2, training loss: 0.999151
Epoch: 41, batch: 3, training loss: 1.416448
Epoch: 41, batch: 4, training loss: 0.689800
Epoch: 41, batch: 5, training loss: 0.802816
Epoch: 41, batch: 6, training loss: 0.668590
Epoch: 41, batch: 7, training loss: 0.725809
Epoch: 41, batch: 8, training loss: 0.790081
Epoch: 41, batch: 9, training loss: 0.784809
Epoch: 41, batch: 10, training loss: 1.147594
Epoch: 41, batch: 11, training loss: 1.417872
Epoch: 41, batch: 12, training loss: 1.268309
Epoch: 41, batch: 13, training loss: 1.440747
Epoch: 41, batch: 14, training loss: 1.354113
Epoch: 41, batch: 15, training loss: 1.580169
Epoch: 41, batch: 16, training loss: 1.381696
Epoch: 41, batch: 17, training loss: 1.474054
Epoch: 41, batch: 18, training loss: 1.368038
Epoch: 41, batch: 19, training loss: 1.081001
Epoch: 41, batch: 20, training loss: 1.425377
Epoch: 41, batch: 21, training loss: 0.835571
Epoch: 41, batch: 22, training loss: 1.291753
Epoch: 41, batch: 23, training loss: 1.217711
Epoch: 41, batch: 24, training loss: 1.354645
Epoch: 41, batch: 25, training loss: 1.444845
Epoch: 41, batch: 26, training loss: 1.503953
Epoch: 41, batch: 27, training loss: 1.585932
Epoch: 41, batch: 28, training loss: 1.711851
Epoch: 41, batch: 29, training loss: 1.528442
Epoch: 41, batch: 30, training loss: 1.673240
Epoch: 41, batch: 31, training loss: 1.356775
Epoch: 41, batch: 32, training loss: 1.407244
Epoch: 41, batch: 33, training loss: 1.461902
Epoch: 41, batch: 34, training loss: 1.378783
Epoch: 41, batch: 35, training loss: 1.534368
Epoch: 41, batch: 36, training loss: 1.484323
Epoch: 41, batch: 37, training loss: 1.422590
Epoch: 41, batch: 38, training loss: 1.574645
Epoch: 41, batch: 39, training loss: 1.775104
Epoch: 41, batch: 40, training loss: 1.188010
Epoch: 41, batch: 41, training loss: 1.561119
Epoch: 41, batch: 42, training loss: 1.615179
Epoch: 41, batch: 43, training loss: 1.278930
Epoch: 41, batch: 44, training loss: 1.644763
Epoch: 41, batch: 45, training loss: 1.651098
Epoch: 41, batch: 46, training loss: 1.607323
Epoch: 41, batch: 47, training loss: 1.449716
Epoch: 41, batch: 48, training loss: 1.332228
Epoch: 41, batch: 49, training loss: 1.430764
Epoch: 42, batch: 0, training loss: 1.486277
Epoch: 42, batch: 1, training loss: 1.549849
Epoch: 42, batch: 2, training loss: 0.990606
Epoch: 42, batch: 3, training loss: 1.416415
Epoch: 42, batch: 4, training loss: 0.697427
Epoch: 42, batch: 5, training loss: 0.767287
Epoch: 42, batch: 6, training loss: 0.694568
Epoch: 42, batch: 7, training loss: 0.679873
Epoch: 42, batch: 8, training loss: 0.811819
Epoch: 42, batch: 9, training loss: 0.767668
Epoch: 42, batch: 10, training loss: 1.156074
Epoch: 42, batch: 11, training loss: 1.425643
Epoch: 42, batch: 12, training loss: 1.239052
Epoch: 42, batch: 13, training loss: 1.447938
Epoch: 42, batch: 14, training loss: 1.339199
Epoch: 42, batch: 15, training loss: 1.557158
Epoch: 42, batch: 16, training loss: 1.402995
Epoch: 42, batch: 17, training loss: 1.461647
Epoch: 42, batch: 18, training loss: 1.353256
Epoch: 42, batch: 19, training loss: 1.079165
Epoch: 42, batch: 20, training loss: 1.443949
Epoch: 42, batch: 21, training loss: 0.820915
Epoch: 42, batch: 22, training loss: 1.283753
Epoch: 42, batch: 23, training loss: 1.203715
Epoch: 42, batch: 24, training loss: 1.343716
Epoch: 42, batch: 25, training loss: 1.414048
Epoch: 42, batch: 26, training loss: 1.475731
Epoch: 42, batch: 27, training loss: 1.557005
Epoch: 42, batch: 28, training loss: 1.676381
Epoch: 42, batch: 29, training loss: 1.514446
Epoch: 42, batch: 30, training loss: 1.642107
Epoch: 42, batch: 31, training loss: 1.356198
Epoch: 42, batch: 32, training loss: 1.404484
Epoch: 42, batch: 33, training loss: 1.405713
Epoch: 42, batch: 34, training loss: 1.361961
Epoch: 42, batch: 35, training loss: 1.544112
Epoch: 42, batch: 36, training loss: 1.444084
Epoch: 42, batch: 37, training loss: 1.387461
Epoch: 42, batch: 38, training loss: 1.576771
Epoch: 42, batch: 39, training loss: 1.750262
Epoch: 42, batch: 40, training loss: 1.190965
Epoch: 42, batch: 41, training loss: 1.517727
Epoch: 42, batch: 42, training loss: 1.616312
Epoch: 42, batch: 43, training loss: 1.249463
Epoch: 42, batch: 44, training loss: 1.610682
Epoch: 42, batch: 45, training loss: 1.625144
Epoch: 42, batch: 46, training loss: 1.591643
Epoch: 42, batch: 47, training loss: 1.426471
Epoch: 42, batch: 48, training loss: 1.328079
Epoch: 42, batch: 49, training loss: 1.434498
Epoch: 43, batch: 0, training loss: 1.454026
Epoch: 43, batch: 1, training loss: 1.515175
Epoch: 43, batch: 2, training loss: 0.981280
Epoch: 43, batch: 3, training loss: 1.392506
Epoch: 43, batch: 4, training loss: 0.693278
Epoch: 43, batch: 5, training loss: 0.783038
Epoch: 43, batch: 6, training loss: 0.688238
Epoch: 43, batch: 7, training loss: 0.675168
Epoch: 43, batch: 8, training loss: 0.781399
Epoch: 43, batch: 9, training loss: 0.755477
Epoch: 43, batch: 10, training loss: 1.135029
Epoch: 43, batch: 11, training loss: 1.384496
Epoch: 43, batch: 12, training loss: 1.209662
Epoch: 43, batch: 13, training loss: 1.401814
Epoch: 43, batch: 14, training loss: 1.305299
Epoch: 43, batch: 15, training loss: 1.539490
Epoch: 43, batch: 16, training loss: 1.369497
Epoch: 43, batch: 17, training loss: 1.441007
Epoch: 43, batch: 18, training loss: 1.331786
Epoch: 43, batch: 19, training loss: 1.057108
Epoch: 43, batch: 20, training loss: 1.403636
Epoch: 43, batch: 21, training loss: 0.805535
Epoch: 43, batch: 22, training loss: 1.270113
Epoch: 43, batch: 23, training loss: 1.185046
Epoch: 43, batch: 24, training loss: 1.319679
Epoch: 43, batch: 25, training loss: 1.405842
Epoch: 43, batch: 26, training loss: 1.476499
Epoch: 43, batch: 27, training loss: 1.525640
Epoch: 43, batch: 28, training loss: 1.652808
Epoch: 43, batch: 29, training loss: 1.494996
Epoch: 43, batch: 30, training loss: 1.603605
Epoch: 43, batch: 31, training loss: 1.320430
Epoch: 43, batch: 32, training loss: 1.370970
Epoch: 43, batch: 33, training loss: 1.416531
Epoch: 43, batch: 34, training loss: 1.335684
Epoch: 43, batch: 35, training loss: 1.522522
Epoch: 43, batch: 36, training loss: 1.421341
Epoch: 43, batch: 37, training loss: 1.389550
Epoch: 43, batch: 38, training loss: 1.536057
Epoch: 43, batch: 39, training loss: 1.722502
Epoch: 43, batch: 40, training loss: 1.162580
Epoch: 43, batch: 41, training loss: 1.516042
Epoch: 43, batch: 42, training loss: 1.590180
Epoch: 43, batch: 43, training loss: 1.254523
Epoch: 43, batch: 44, training loss: 1.607066
Epoch: 43, batch: 45, training loss: 1.625000
Epoch: 43, batch: 46, training loss: 1.588607
Epoch: 43, batch: 47, training loss: 1.453270
Epoch: 43, batch: 48, training loss: 1.328520
Epoch: 43, batch: 49, training loss: 1.434251
Epoch: 44, batch: 0, training loss: 1.451316
Epoch: 44, batch: 1, training loss: 1.504080
Epoch: 44, batch: 2, training loss: 0.994745
Epoch: 44, batch: 3, training loss: 1.351279
Epoch: 44, batch: 4, training loss: 0.696852
Epoch: 44, batch: 5, training loss: 0.792918
Epoch: 44, batch: 6, training loss: 0.666371
Epoch: 44, batch: 7, training loss: 0.671252
Epoch: 44, batch: 8, training loss: 0.784215
Epoch: 44, batch: 9, training loss: 0.747316
Epoch: 44, batch: 10, training loss: 1.132911
Epoch: 44, batch: 11, training loss: 1.386604
Epoch: 44, batch: 12, training loss: 1.208554
Epoch: 44, batch: 13, training loss: 1.424283
Epoch: 44, batch: 14, training loss: 1.304531
Epoch: 44, batch: 15, training loss: 1.534784
Epoch: 44, batch: 16, training loss: 1.375611
Epoch: 44, batch: 17, training loss: 1.433684
Epoch: 44, batch: 18, training loss: 1.335745
Epoch: 44, batch: 19, training loss: 1.076045
Epoch: 44, batch: 20, training loss: 1.379785
Epoch: 44, batch: 21, training loss: 0.804594
Epoch: 44, batch: 22, training loss: 1.277307
Epoch: 44, batch: 23, training loss: 1.173696
Epoch: 44, batch: 24, training loss: 1.302186
Epoch: 44, batch: 25, training loss: 1.415259
Epoch: 44, batch: 26, training loss: 1.466014
Epoch: 44, batch: 27, training loss: 1.509269
Epoch: 44, batch: 28, training loss: 1.662215
Epoch: 44, batch: 29, training loss: 1.496776
Epoch: 44, batch: 30, training loss: 1.629253
Epoch: 44, batch: 31, training loss: 1.309307
Epoch: 44, batch: 32, training loss: 1.356479
Epoch: 44, batch: 33, training loss: 1.424685
Epoch: 44, batch: 34, training loss: 1.327398
Epoch: 44, batch: 35, training loss: 1.497882
Epoch: 44, batch: 36, training loss: 1.456874
Epoch: 44, batch: 37, training loss: 1.356864
Epoch: 44, batch: 38, training loss: 1.536959
Epoch: 44, batch: 39, training loss: 1.696608
Epoch: 44, batch: 40, training loss: 1.178706
Epoch: 44, batch: 41, training loss: 1.499833
Epoch: 44, batch: 42, training loss: 1.565380
Epoch: 44, batch: 43, training loss: 1.228921
Epoch: 44, batch: 44, training loss: 1.584054
Epoch: 44, batch: 45, training loss: 1.590699
Epoch: 44, batch: 46, training loss: 1.570376
Epoch: 44, batch: 47, training loss: 1.419116
Epoch: 44, batch: 48, training loss: 1.304008
Epoch: 44, batch: 49, training loss: 1.403641
Epoch: 45, batch: 0, training loss: 1.454616
Epoch: 45, batch: 1, training loss: 1.508310
Epoch: 45, batch: 2, training loss: 0.971182
Epoch: 45, batch: 3, training loss: 1.370789
Epoch: 45, batch: 4, training loss: 0.684384
Epoch: 45, batch: 5, training loss: 0.768098
Epoch: 45, batch: 6, training loss: 0.649370
Epoch: 45, batch: 7, training loss: 0.677375
Epoch: 45, batch: 8, training loss: 0.781931
Epoch: 45, batch: 9, training loss: 0.757262
Epoch: 45, batch: 10, training loss: 1.097973
Epoch: 45, batch: 11, training loss: 1.351837
Epoch: 45, batch: 12, training loss: 1.204412
Epoch: 45, batch: 13, training loss: 1.382713
Epoch: 45, batch: 14, training loss: 1.306302
Epoch: 45, batch: 15, training loss: 1.540840
Epoch: 45, batch: 16, training loss: 1.351453
Epoch: 45, batch: 17, training loss: 1.412207
Epoch: 45, batch: 18, training loss: 1.308906
Epoch: 45, batch: 19, training loss: 1.020412
Epoch: 45, batch: 20, training loss: 1.397609
Epoch: 45, batch: 21, training loss: 0.803730
Epoch: 45, batch: 22, training loss: 1.240531
Epoch: 45, batch: 23, training loss: 1.169098
Epoch: 45, batch: 24, training loss: 1.291484
Epoch: 45, batch: 25, training loss: 1.389984
Epoch: 45, batch: 26, training loss: 1.442678
Epoch: 45, batch: 27, training loss: 1.523376
Epoch: 45, batch: 28, training loss: 1.654004
Epoch: 45, batch: 29, training loss: 1.437776
Epoch: 45, batch: 30, training loss: 1.571564
Epoch: 45, batch: 31, training loss: 1.290116
Epoch: 45, batch: 32, training loss: 1.339684
Epoch: 45, batch: 33, training loss: 1.353246
Epoch: 45, batch: 34, training loss: 1.292650
Epoch: 45, batch: 35, training loss: 1.472643
Epoch: 45, batch: 36, training loss: 1.403454
Epoch: 45, batch: 37, training loss: 1.349524
Epoch: 45, batch: 38, training loss: 1.532141
Epoch: 45, batch: 39, training loss: 1.678076
Epoch: 45, batch: 40, training loss: 1.112629
Epoch: 45, batch: 41, training loss: 1.458193
Epoch: 45, batch: 42, training loss: 1.569144
Epoch: 45, batch: 43, training loss: 1.237414
Epoch: 45, batch: 44, training loss: 1.574934
Epoch: 45, batch: 45, training loss: 1.583289
Epoch: 45, batch: 46, training loss: 1.553830
Epoch: 45, batch: 47, training loss: 1.406094
Epoch: 45, batch: 48, training loss: 1.270701
Epoch: 45, batch: 49, training loss: 1.378426
Epoch: 46, batch: 0, training loss: 1.413069
Epoch: 46, batch: 1, training loss: 1.483110
Epoch: 46, batch: 2, training loss: 0.939041
Epoch: 46, batch: 3, training loss: 1.353728
Epoch: 46, batch: 4, training loss: 0.683902
Epoch: 46, batch: 5, training loss: 0.770247
Epoch: 46, batch: 6, training loss: 0.663000
Epoch: 46, batch: 7, training loss: 0.682476
Epoch: 46, batch: 8, training loss: 0.756036
Epoch: 46, batch: 9, training loss: 0.761690
Epoch: 46, batch: 10, training loss: 1.103423
Epoch: 46, batch: 11, training loss: 1.362983
Epoch: 46, batch: 12, training loss: 1.192239
Epoch: 46, batch: 13, training loss: 1.373896
Epoch: 46, batch: 14, training loss: 1.295860
Epoch: 46, batch: 15, training loss: 1.523111
Epoch: 46, batch: 16, training loss: 1.344424
Epoch: 46, batch: 17, training loss: 1.417217
Epoch: 46, batch: 18, training loss: 1.304325
Epoch: 46, batch: 19, training loss: 1.026673
Epoch: 46, batch: 20, training loss: 1.386495
Epoch: 46, batch: 21, training loss: 0.821209
Epoch: 46, batch: 22, training loss: 1.233895
Epoch: 46, batch: 23, training loss: 1.166027
Epoch: 46, batch: 24, training loss: 1.292771
Epoch: 46, batch: 25, training loss: 1.357004
Epoch: 46, batch: 26, training loss: 1.442374
Epoch: 46, batch: 27, training loss: 1.494593
Epoch: 46, batch: 28, training loss: 1.629641
Epoch: 46, batch: 29, training loss: 1.446461
Epoch: 46, batch: 30, training loss: 1.543949
Epoch: 46, batch: 31, training loss: 1.286834
Epoch: 46, batch: 32, training loss: 1.340313
Epoch: 46, batch: 33, training loss: 1.318790
Epoch: 46, batch: 34, training loss: 1.265283
Epoch: 46, batch: 35, training loss: 1.451151
Epoch: 46, batch: 36, training loss: 1.373710
Epoch: 46, batch: 37, training loss: 1.335724
Epoch: 46, batch: 38, training loss: 1.484141
Epoch: 46, batch: 39, training loss: 1.653164
Epoch: 46, batch: 40, training loss: 1.109809
Epoch: 46, batch: 41, training loss: 1.444430
Epoch: 46, batch: 42, training loss: 1.505087
Epoch: 46, batch: 43, training loss: 1.196387
Epoch: 46, batch: 44, training loss: 1.561856
Epoch: 46, batch: 45, training loss: 1.557630
Epoch: 46, batch: 46, training loss: 1.527029
Epoch: 46, batch: 47, training loss: 1.378059
Epoch: 46, batch: 48, training loss: 1.262173
Epoch: 46, batch: 49, training loss: 1.349344
Epoch: 47, batch: 0, training loss: 1.397778
Epoch: 47, batch: 1, training loss: 1.478912
Epoch: 47, batch: 2, training loss: 0.942010
Epoch: 47, batch: 3, training loss: 1.312141
Epoch: 47, batch: 4, training loss: 0.660254
Epoch: 47, batch: 5, training loss: 0.737144
Epoch: 47, batch: 6, training loss: 0.633544
Epoch: 47, batch: 7, training loss: 0.670768
Epoch: 47, batch: 8, training loss: 0.754826
Epoch: 47, batch: 9, training loss: 0.735370
Epoch: 47, batch: 10, training loss: 1.073688
Epoch: 47, batch: 11, training loss: 1.320636
Epoch: 47, batch: 12, training loss: 1.156044
Epoch: 47, batch: 13, training loss: 1.343202
Epoch: 47, batch: 14, training loss: 1.267122
Epoch: 47, batch: 15, training loss: 1.486173
Epoch: 47, batch: 16, training loss: 1.317194
Epoch: 47, batch: 17, training loss: 1.385668
Epoch: 47, batch: 18, training loss: 1.276045
Epoch: 47, batch: 19, training loss: 1.018504
Epoch: 47, batch: 20, training loss: 1.356897
Epoch: 47, batch: 21, training loss: 0.792943
Epoch: 47, batch: 22, training loss: 1.206334
Epoch: 47, batch: 23, training loss: 1.142011
Epoch: 47, batch: 24, training loss: 1.257242
Epoch: 47, batch: 25, training loss: 1.363510
Epoch: 47, batch: 26, training loss: 1.399950
Epoch: 47, batch: 27, training loss: 1.474528
Epoch: 47, batch: 28, training loss: 1.610454
Epoch: 47, batch: 29, training loss: 1.423080
Epoch: 47, batch: 30, training loss: 1.520625
Epoch: 47, batch: 31, training loss: 1.230900
Epoch: 47, batch: 32, training loss: 1.318894
Epoch: 47, batch: 33, training loss: 1.314662
Epoch: 47, batch: 34, training loss: 1.240450
Epoch: 47, batch: 35, training loss: 1.430660
Epoch: 47, batch: 36, training loss: 1.365273
Epoch: 47, batch: 37, training loss: 1.297522
Epoch: 47, batch: 38, training loss: 1.483248
Epoch: 47, batch: 39, training loss: 1.611408
Epoch: 47, batch: 40, training loss: 1.093922
Epoch: 47, batch: 41, training loss: 1.399802
Epoch: 47, batch: 42, training loss: 1.500045
Epoch: 47, batch: 43, training loss: 1.185108
Epoch: 47, batch: 44, training loss: 1.526926
Epoch: 47, batch: 45, training loss: 1.557155
Epoch: 47, batch: 46, training loss: 1.507290
Epoch: 47, batch: 47, training loss: 1.325039
Epoch: 47, batch: 48, training loss: 1.254801
Epoch: 47, batch: 49, training loss: 1.345441
Epoch: 48, batch: 0, training loss: 1.351007
Epoch: 48, batch: 1, training loss: 1.418536
Epoch: 48, batch: 2, training loss: 0.915809
Epoch: 48, batch: 3, training loss: 1.292471
Epoch: 48, batch: 4, training loss: 0.652138
Epoch: 48, batch: 5, training loss: 0.707824
Epoch: 48, batch: 6, training loss: 0.626985
Epoch: 48, batch: 7, training loss: 0.661212
Epoch: 48, batch: 8, training loss: 0.729811
Epoch: 48, batch: 9, training loss: 0.719264
Epoch: 48, batch: 10, training loss: 1.067409
Epoch: 48, batch: 11, training loss: 1.310475
Epoch: 48, batch: 12, training loss: 1.148365
Epoch: 48, batch: 13, training loss: 1.346402
Epoch: 48, batch: 14, training loss: 1.257776
Epoch: 48, batch: 15, training loss: 1.478868
Epoch: 48, batch: 16, training loss: 1.306091
Epoch: 48, batch: 17, training loss: 1.363291
Epoch: 48, batch: 18, training loss: 1.251329
Epoch: 48, batch: 19, training loss: 1.018984
Epoch: 48, batch: 20, training loss: 1.327007
Epoch: 48, batch: 21, training loss: 0.752302
Epoch: 48, batch: 22, training loss: 1.216061
Epoch: 48, batch: 23, training loss: 1.130791
Epoch: 48, batch: 24, training loss: 1.252845
Epoch: 48, batch: 25, training loss: 1.344319
Epoch: 48, batch: 26, training loss: 1.400695
Epoch: 48, batch: 27, training loss: 1.458613
Epoch: 48, batch: 28, training loss: 1.600518
Epoch: 48, batch: 29, training loss: 1.409860
Epoch: 48, batch: 30, training loss: 1.496818
Epoch: 48, batch: 31, training loss: 1.240335
Epoch: 48, batch: 32, training loss: 1.285921
Epoch: 48, batch: 33, training loss: 1.259155
Epoch: 48, batch: 34, training loss: 1.256995
Epoch: 48, batch: 35, training loss: 1.400595
Epoch: 48, batch: 36, training loss: 1.337746
Epoch: 48, batch: 37, training loss: 1.282683
Epoch: 48, batch: 38, training loss: 1.452994
Epoch: 48, batch: 39, training loss: 1.613797
Epoch: 48, batch: 40, training loss: 1.077843
Epoch: 48, batch: 41, training loss: 1.407773
Epoch: 48, batch: 42, training loss: 1.482121
Epoch: 48, batch: 43, training loss: 1.172480
Epoch: 48, batch: 44, training loss: 1.489902
Epoch: 48, batch: 45, training loss: 1.510499
Epoch: 48, batch: 46, training loss: 1.492301
Epoch: 48, batch: 47, training loss: 1.334535
Epoch: 48, batch: 48, training loss: 1.254280
Epoch: 48, batch: 49, training loss: 1.347217
Epoch: 49, batch: 0, training loss: 1.373160
Epoch: 49, batch: 1, training loss: 1.428972
Epoch: 49, batch: 2, training loss: 0.898921
Epoch: 49, batch: 3, training loss: 1.258058
Epoch: 49, batch: 4, training loss: 0.630858
Epoch: 49, batch: 5, training loss: 0.705566
Epoch: 49, batch: 6, training loss: 0.626565
Epoch: 49, batch: 7, training loss: 0.646792
Epoch: 49, batch: 8, training loss: 0.734926
Epoch: 49, batch: 9, training loss: 0.717630
Epoch: 49, batch: 10, training loss: 1.049018
Epoch: 49, batch: 11, training loss: 1.327667
Epoch: 49, batch: 12, training loss: 1.128579
Epoch: 49, batch: 13, training loss: 1.322916
Epoch: 49, batch: 14, training loss: 1.276688
Epoch: 49, batch: 15, training loss: 1.450733
Epoch: 49, batch: 16, training loss: 1.285886
Epoch: 49, batch: 17, training loss: 1.355077
Epoch: 49, batch: 18, training loss: 1.254279
Epoch: 49, batch: 19, training loss: 0.997495
Epoch: 49, batch: 20, training loss: 1.333934
Epoch: 49, batch: 21, training loss: 0.766842
Epoch: 49, batch: 22, training loss: 1.210289
Epoch: 49, batch: 23, training loss: 1.103648
Epoch: 49, batch: 24, training loss: 1.240252
Epoch: 49, batch: 25, training loss: 1.322817
Epoch: 49, batch: 26, training loss: 1.374673
Epoch: 49, batch: 27, training loss: 1.445804
Epoch: 49, batch: 28, training loss: 1.580282
Epoch: 49, batch: 29, training loss: 1.400888
Epoch: 49, batch: 30, training loss: 1.514764
Epoch: 49, batch: 31, training loss: 1.242885
Epoch: 49, batch: 32, training loss: 1.272133
Epoch: 49, batch: 33, training loss: 1.277909
Epoch: 49, batch: 34, training loss: 1.222662
Epoch: 49, batch: 35, training loss: 1.392292
Epoch: 49, batch: 36, training loss: 1.337317
Epoch: 49, batch: 37, training loss: 1.292041
Epoch: 49, batch: 38, training loss: 1.432842
Epoch: 49, batch: 39, training loss: 1.591808
Epoch: 49, batch: 40, training loss: 1.070178
Epoch: 49, batch: 41, training loss: 1.413640
Epoch: 49, batch: 42, training loss: 1.480436
Epoch: 49, batch: 43, training loss: 1.178115
Epoch: 49, batch: 44, training loss: 1.490987
Epoch: 49, batch: 45, training loss: 1.516955
Epoch: 49, batch: 46, training loss: 1.492390
Epoch: 49, batch: 47, training loss: 1.338777
Epoch: 49, batch: 48, training loss: 1.219364
Epoch: 49, batch: 49, training loss: 1.323165
Epoch: 48, batch: 0, training loss: 1.373160
Epoch: 48, batch: 1, training loss: 1.428972
Epoch: 48, batch: 2, training loss: 0.898921
Epoch: 48, batch: 3, training loss: 1.258058
Epoch: 48, batch: 4, training loss: 0.630858
Epoch: 48, batch: 5, training loss: 0.705566
Epoch: 48, batch: 6, training loss: 0.626565
Epoch: 48, batch: 7, training loss: 0.646792
Epoch: 48, batch: 8, training loss: 0.734926
Epoch: 48, batch: 9, training loss: 0.717630
Epoch: 48, batch: 10, training loss: 1.049018
Epoch: 48, batch: 11, training loss: 1.327667
Epoch: 48, batch: 12, training loss: 1.128579
Epoch: 48, batch: 13, training loss: 1.322916
Epoch: 48, batch: 14, training loss: 1.276688
Epoch: 48, batch: 15, training loss: 1.450733
Epoch: 48, batch: 16, training loss: 1.285886
Epoch: 48, batch: 17, training loss: 1.355077
Epoch: 48, batch: 18, training loss: 1.254279
Epoch: 48, batch: 19, training loss: 0.997495
Epoch: 48, batch: 20, training loss: 1.333934
Epoch: 48, batch: 21, training loss: 0.766842
Epoch: 48, batch: 22, training loss: 1.210289
Epoch: 48, batch: 23, training loss: 1.103648
Epoch: 48, batch: 24, training loss: 1.240252
Epoch: 48, batch: 25, training loss: 1.322817
Epoch: 48, batch: 26, training loss: 1.374673
Epoch: 48, batch: 27, training loss: 1.445804
Epoch: 48, batch: 28, training loss: 1.580282
Epoch: 48, batch: 29, training loss: 1.400888
Epoch: 48, batch: 30, training loss: 1.514764
Epoch: 48, batch: 31, training loss: 1.242885
Epoch: 48, batch: 32, training loss: 1.272133
Epoch: 48, batch: 33, training loss: 1.277909
Epoch: 48, batch: 34, training loss: 1.222662
Epoch: 48, batch: 35, training loss: 1.392292
Epoch: 48, batch: 36, training loss: 1.337317
Epoch: 48, batch: 37, training loss: 1.292041
Epoch: 48, batch: 38, training loss: 1.432842
Epoch: 48, batch: 39, training loss: 1.591808
Epoch: 48, batch: 40, training loss: 1.070178
Epoch: 48, batch: 41, training loss: 1.413640
Epoch: 48, batch: 42, training loss: 1.480436
Epoch: 48, batch: 43, training loss: 1.178115
Epoch: 48, batch: 44, training loss: 1.490987
Epoch: 48, batch: 45, training loss: 1.516955
Epoch: 48, batch: 46, training loss: 1.492390
Epoch: 48, batch: 47, training loss: 1.338777
Epoch: 48, batch: 48, training loss: 1.219364
Epoch: 48, batch: 49, training loss: 1.323165
Epoch: 49, batch: 0, training loss: 1.346328
Epoch: 49, batch: 1, training loss: 1.434020
Epoch: 49, batch: 2, training loss: 0.907520
Epoch: 49, batch: 3, training loss: 1.276174
Epoch: 49, batch: 4, training loss: 0.649986
Epoch: 49, batch: 5, training loss: 0.721029
Epoch: 49, batch: 6, training loss: 0.644131
Epoch: 49, batch: 7, training loss: 0.643404
Epoch: 49, batch: 8, training loss: 0.716621
Epoch: 49, batch: 9, training loss: 0.699262
Epoch: 49, batch: 10, training loss: 1.052616
Epoch: 49, batch: 11, training loss: 1.312234
Epoch: 49, batch: 12, training loss: 1.142705
Epoch: 49, batch: 13, training loss: 1.331524
Epoch: 49, batch: 14, training loss: 1.243310
Epoch: 49, batch: 15, training loss: 1.443129
Epoch: 49, batch: 16, training loss: 1.281785
Epoch: 49, batch: 17, training loss: 1.354544
Epoch: 49, batch: 18, training loss: 1.256520
Epoch: 49, batch: 19, training loss: 1.007311
Epoch: 49, batch: 20, training loss: 1.322164
Epoch: 49, batch: 21, training loss: 0.759657
Epoch: 49, batch: 22, training loss: 1.176995
Epoch: 49, batch: 23, training loss: 1.117778
Epoch: 49, batch: 24, training loss: 1.232663
Epoch: 49, batch: 25, training loss: 1.317800
Epoch: 49, batch: 26, training loss: 1.355891
Epoch: 49, batch: 27, training loss: 1.426740
Epoch: 49, batch: 28, training loss: 1.571272
Epoch: 49, batch: 29, training loss: 1.370188
Epoch: 49, batch: 30, training loss: 1.482551
Epoch: 49, batch: 31, training loss: 1.225705
Epoch: 49, batch: 32, training loss: 1.278922
Epoch: 49, batch: 33, training loss: 1.241485
Epoch: 49, batch: 34, training loss: 1.199968
Epoch: 49, batch: 35, training loss: 1.387891
Epoch: 49, batch: 36, training loss: 1.352963
Epoch: 49, batch: 37, training loss: 1.291317
Epoch: 49, batch: 38, training loss: 1.449337
Epoch: 49, batch: 39, training loss: 1.584774
Epoch: 49, batch: 40, training loss: 1.055388
Epoch: 49, batch: 41, training loss: 1.392006
Epoch: 49, batch: 42, training loss: 1.451729
Epoch: 49, batch: 43, training loss: 1.165060
Epoch: 49, batch: 44, training loss: 1.472736
Epoch: 49, batch: 45, training loss: 1.514873
Epoch: 49, batch: 46, training loss: 1.474691
Epoch: 49, batch: 47, training loss: 1.344127
Epoch: 49, batch: 48, training loss: 1.247406
Epoch: 49, batch: 49, training loss: 1.313120
