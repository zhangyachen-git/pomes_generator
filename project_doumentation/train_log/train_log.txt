Epoch: 14, batch: 0, training loss: 1.930131
Epoch: 14, batch: 1, training loss: 2.077824
Epoch: 14, batch: 2, training loss: 1.279802
Epoch: 14, batch: 3, training loss: 1.864902
Epoch: 14, batch: 4, training loss: 0.915340
Epoch: 14, batch: 5, training loss: 1.023350
Epoch: 14, batch: 6, training loss: 0.882979
Epoch: 14, batch: 7, training loss: 1.251360
Epoch: 14, batch: 8, training loss: 1.360496
Epoch: 14, batch: 9, training loss: 1.520576
Epoch: 14, batch: 10, training loss: 1.791427
Epoch: 14, batch: 11, training loss: 2.149421
Epoch: 14, batch: 12, training loss: 1.868161
Epoch: 14, batch: 13, training loss: 2.232265
Epoch: 14, batch: 14, training loss: 2.018370
Epoch: 14, batch: 15, training loss: 2.444708
Epoch: 14, batch: 16, training loss: 2.144704
Epoch: 14, batch: 17, training loss: 2.172683
Epoch: 14, batch: 18, training loss: 1.954907
Epoch: 14, batch: 19, training loss: 1.531316
Epoch: 14, batch: 20, training loss: 2.165844
Epoch: 14, batch: 21, training loss: 1.391247
Epoch: 14, batch: 22, training loss: 2.007111
Epoch: 14, batch: 23, training loss: 1.970367
Epoch: 14, batch: 24, training loss: 2.020684
Epoch: 14, batch: 25, training loss: 2.220515
Epoch: 14, batch: 26, training loss: 2.289808
Epoch: 14, batch: 27, training loss: 2.403732
Epoch: 14, batch: 28, training loss: 2.623917
Epoch: 14, batch: 29, training loss: 2.406757
Epoch: 14, batch: 30, training loss: 2.492640
Epoch: 14, batch: 31, training loss: 2.059985
Epoch: 14, batch: 32, training loss: 1.985653
Epoch: 14, batch: 33, training loss: 2.488719
Epoch: 14, batch: 34, training loss: 2.399257
Epoch: 14, batch: 35, training loss: 2.293513
Epoch: 14, batch: 36, training loss: 2.230926
Epoch: 14, batch: 37, training loss: 2.152888
Epoch: 14, batch: 38, training loss: 2.327703
Epoch: 14, batch: 39, training loss: 2.716771
Epoch: 14, batch: 40, training loss: 1.827235
Epoch: 14, batch: 41, training loss: 2.206690
Epoch: 14, batch: 42, training loss: 2.319341
Epoch: 14, batch: 43, training loss: 1.897871
Epoch: 14, batch: 44, training loss: 2.513644
Epoch: 14, batch: 45, training loss: 2.505348
Epoch: 14, batch: 46, training loss: 2.412877
Epoch: 14, batch: 47, training loss: 2.346514
Epoch: 14, batch: 48, training loss: 2.075790
Epoch: 14, batch: 49, training loss: 2.286806
Epoch: 15, batch: 0, training loss: 2.051405
Epoch: 15, batch: 1, training loss: 2.183816
Epoch: 15, batch: 2, training loss: 1.403343
Epoch: 15, batch: 3, training loss: 2.012464
Epoch: 15, batch: 4, training loss: 1.039914
Epoch: 15, batch: 5, training loss: 1.155290
Epoch: 15, batch: 6, training loss: 0.979289
Epoch: 15, batch: 7, training loss: 1.155183
Epoch: 15, batch: 8, training loss: 1.268082
Epoch: 15, batch: 9, training loss: 1.371928
Epoch: 15, batch: 10, training loss: 1.699769
Epoch: 15, batch: 11, training loss: 2.082436
Epoch: 15, batch: 12, training loss: 1.817829
Epoch: 15, batch: 13, training loss: 2.171305
Epoch: 15, batch: 14, training loss: 1.952185
Epoch: 15, batch: 15, training loss: 2.325888
